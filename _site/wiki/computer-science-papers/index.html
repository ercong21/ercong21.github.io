<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>Computer Science Papers &mdash; Peiqin Lin's Homepage</title>
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/primer-css/css/primer.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/collection.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/repo-card.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/repo-list.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/mini-repo-list.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/profile.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/contact.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/news.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/boxed-group.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/common.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/share.js/dist/css/share.min.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/responsive.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/posts/index.css">
    <!-- Latest compiled and minified CSS -->
    

    
    <link rel="canonical" href="http://localhost:4000/wiki/computer-science-papers/">
    <link rel="alternate" type="application/atom+xml" title="Peiqin Lin's Homepage" href="/feed.xml">
    <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
    
    <meta property="og:title" content="Computer Science Papers">
      
    <meta name="keywords" content="omputer Science Papers, 计算机科学论文, 深度学习论文, 自然语言处理论文">
    <meta name="og:keywords" content="omputer Science Papers, 计算机科学论文, 深度学习论文, 自然语言处理论文">
      
    <meta name="description" content="Paper List">
    <meta name="og:description" content="Paper List">
      
    
    
        
    
    <meta property="og:url" content="http://localhost:4000/wiki/computer-science-papers/">
    <meta property="og:site_name" content="Peiqin Lin's Homepage">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh_CN" />
    
    <meta property="article:published_time" content="2025-04-01">
    
    <script src="http://localhost:4000/assets/vendor/jquery/dist/jquery.min.js"></script>
    <script src="http://localhost:4000/assets/js/jquery-ui.js"></script>
    <script type="text/javascript">
    function toggleMenu() {
        var nav = document.getElementsByClassName("site-header-nav")[0];
        if (nav.style.display == "inline-flex") {
          nav.style.display = "none";
        } else {
          nav.style.display = "inline-flex";
        }
    }
    </script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-114643083-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-114643083-1');
    </script>
</head>
<body class="" data-mz="">
    <header class="site-header">
        <div class="container">
            <h1><a href="http://localhost:4000/" title="Peiqin Lin's Homepage">Peiqin Lin's Homepage</a></h1>
            <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <nav class="site-header-nav" role="navigation">
                
            </nav>
        </div>
    </header>
    <!-- / header -->

    <section class="collection-head small geopattern" data-pattern-id="Computer Scienc">
<div class="container">
  <div class="columns">
    <div class="column three-fourths">
      <div class="collection-title">
        <h1 class="collection-header">Computer Science Papers</h1>
      </div>
    </div>
  </div>
</div>
</section>
<!-- / .banner -->
<section class="container content">
<div class="columns">
  <div class="column three-fourths" >
    <article class="article-content markdown-body">
    <h3 id="paper-list">Paper List</h3>

<p><a href="https://aclanthology.coli.uni-saarland.de/">ACL, EMNLP, COLING, SemEval Paper List</a></p>

<p><a href="http://www.jmlr.org/papers/">JMLR Paper List</a></p>

<p><a href="http://openaccess.thecvf.com/menu.py">ICCV CVPR Paper List</a></p>

<p><a href="http://www.aaai.org/Library/AAAI/aaai-library.php">AAAI Paper List</a></p>

<p><a href="https://www.ijcai.org/proceedings/2017/">IJCAI Paper List</a></p>

<h3 id="natural-language-processing">Natural Language Processing</h3>

<h4 id="language-model">Language Model</h4>

<p><a href="https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00349">A Primer in BERTology: What We Know About How BERT Works</a> (Anna Rogers, TACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2003.08271.pdf">Pre-trained Models for Natural Language Processing: A Survey</a> (Xipeng Qiu, 2020)</p>

<h5 id="discrete-word-vector">Discrete Word Vector</h5>

<p><a href="https://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/classlm.pdf">Class-Based n-gram Models of Natural Language</a> (Peter F. Brown, CL 1992)</p>

<h5 id="continuous-word-vector">Continuous Word Vector</h5>

<p><a href="http://www.iro.umontreal.ca/~vincentp/Publications/lm_jmlr.pdf">A Neural Probabilistic Language Model</a> (Yoshua Bengio, JMLR 2003, <a href="https://zhuanlan.zhihu.com/p/21101055">note</a>)</p>

<p><a href="http://machinelearning.org/archive/icml2008/papers/391.pdf">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</a> (Ronan Collobert, ICML 2008)</p>

<p><a href="https://papers.nips.cc/paper/2008/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf">A Scalable Hierarchical Distributed Language Model</a> (Andriy Mnih, NeuIPS 2008)</p>

<p><a href="https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf?source=post_page---------------------------">Natural Language Processing (Almost) from Scratch</a> (Ronan Collobert, JMLR 2011)</p>

<p><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> (Tomas Mikolov, 2013, <a href="https://github.com/danielfrg/word2vec">code</a>, <a href="https://zhuanlan.zhihu.com/p/34718114">note</a>)</p>

<p><a href="https://aclanthology.org/P14-1023.pdf">Don’t Count, Predict! A Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors</a> (Marco Baroni, ACL 2014)</p>

<p><a href="http://www.anthology.aclweb.org/D/D14/D14-1162.pdf">GloVe: Global Vectors for Word Representation</a> (Jeffrey Pennington, EMNLP 2014, <a href="https://github.com/stanfordnlp/GloVe">code</a>, <a href="https://zhuanlan.zhihu.com/p/34959040">note</a>)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017">Character-Aware Neural Language Models</a> (Yoon Kim, AAAI 2016, <a href="https://zhuanlan.zhihu.com/p/21242454">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1607.01759v2.pdf">Bag of Tricks for Efficient Text Classification</a> (Armand Joulin, 2016, <a href="https://github.com/facebookresearch/fastText">code</a>, <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650716942&amp;idx=3&amp;sn=0d48c0218131de502ac5e2ef9b700967">note</a>)</p>

<p><a href="https://pdfs.semanticscholar.org/e2db/a792360873aef125572812f3673b1a85d850.pdf">Enriching Word Vectors with Subword Information</a> (Piotr Bojanowski, 2016, <a href="https://github.com/xwzhong/papernote/blob/master/embedding/Enriching%20Word%20Vectors%20with%20Subword%20Information.md">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1712.09405.pdf">Advances in Pre-Training Distributed Word Representations</a> (Tomas Mikolov, 2017)</p>

<p><a href="https://arxiv.org/pdf/1802.06893.pdf">Learning Word Vectors for 157 Languages</a> (Edouard Grave, LREC 2018)</p>

<p><a href="https://dl.acm.org/doi/10.1145/3357384.3358005">Learning Chinese Word Embeddings from Stroke, Structure and Pinyin of Characters</a> (Yun Zhang, CIKM 2019)</p>

<p><a href="https://arxiv.org/pdf/1901.10125.pdf">Glyce: Glyph-vectors for Chinese Character Representations</a> (Yuxian Meng, NeurIPS 2019, <a href="https://github.com/ShannonAI/glyce">code</a>, <a href="https://zhuanlan.zhihu.com/p/55967737">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2106.04302.pdf">Obtaining Better Static Word Embeddings Using Contextual Embedding Models</a> (Prakhar Gupta, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2104.07500.pdf">Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via Multi-Task Training</a> (Hassan Shahmohammadi, 2021)</p>

<h5 id="sentenceparagraphdocument-embedding">Sentence/Paragraph/Document Embedding</h5>

<p><a href="https://arxiv.org/pdf/1405.4053.pdf">Distributed Representations of Sentences and Documents</a> (Quoc Le, ICML 2014, <a href="https://github.com/jhlau/doc2vec">code</a>, <a href="https://blog.acolyer.org/2016/06/01/distributed-representations-of-sentences-and-documents/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1506.06726.pdf">Skip-Thought Vectors</a> (Ryan Kiros, NeurIPS 2015)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/LSTM_DSSM_IEEE_TASLP.pdf">Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval</a> (Hamid Palangi, TASLP 2017, <a href="https://github.com/zhaosm/dssm-lstm">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1703.03130.pdf">A Structured Self-attentive Sentence Embedding</a> (Zhouhan Lin, ICLR 2017, <a href="https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding">code</a>, <a href="https://www.sohu.com/a/130767150_505880">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1803.11175.pdf">Universal Sentence Encoder</a> (Daniel Cer, 2018, <a href="https://tfhub.dev/google/universal-sentence-encoder/1">code</a>, <a href="https://zhuanlan.zhihu.com/p/35174235">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.10084.pdf">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> (Nils Reimers, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/2011.05864.pdf">On the Sentence Embeddings from Pre-trained Language Models</a> (Bohan Li, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.12061.pdf">An Unsupervised Sentence Embedding Method by Mutual Information Maximization</a> (Yan Zhang, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2108.08877.pdf">Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models</a> (Jianmo Ni, 2021)</p>

<h5 id="contextual-language-model">Contextual Language Model</h5>

<p><a href="https://arxiv.org/pdf/1511.01432.pdf">Semi-supervised Sequence Learning</a> (Andrew M. Dai, NeurIPS 2015, <a href="https://zhuanlan.zhihu.com/p/21313501">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1705.00108.pdf">Semi-supervised Sequence Tagging with Bidirectional Language Models</a> (Matthew E. Peters, ACL 2017, <a href="https://zhuanlan.zhihu.com/p/105879581">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1708.00107.pdf">Learned in Translation: Contextualized Word Vectors</a> (Bryan McCann, NeurIPS 2017, <a href="https://github.com/salesforce/cove">code</a>, <a href="https://www.sohu.com/a/162634620_610300">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1802.05365.pdf">Deep Contextualized Word Representations</a> (Matthew E. Peters, NAACL 2018, <a href="https://github.com/allenai/allennlp">code</a>, <a href="https://zhuanlan.zhihu.com/p/38254332">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1801.06146.pdf">Universal Language Model Fine-tuning for Text Classification</a> (Jeremy Howard, ACL 2018, <a href="http://nlp.fast.ai/category/classification.html">code</a>, <a href="https://zhuanlan.zhihu.com/p/61590026">note</a>)</p>

<p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a> (Alec Radford, 2018, <a href="https://github.com/openai/finetune-transformer-lm">code</a>, <a href="https://zhuanlan.zhihu.com/p/54754171">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.10529.pdf">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a> (Mandar Joshi, TACL 2019, <a href="https://zhuanlan.zhihu.com/p/75893972">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Jacob Devlin, NAACL 2019, <a href="https://github.com/google-research/bert">code</a>, <a href="https://zhuanlan.zhihu.com/p/46652512">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.04726.pdf">What Kind of Language Is Hard to Language-Model?</a> (Sabrina J. Mielke, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1903.07785.pdf">Cloze-driven Pretraining of Self-attention Networks</a> (Alexei Baevski, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1908.08593.pdf">Revealing the Dark Secrets of BERT</a> (Olga Kovaleva, EMNLP 2019, <a href="https://zhuanlan.zhihu.com/p/117645185">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.03197.pdf">Unified Language Model Pre-training for Natural Language Understanding and Generation</a> (Li Dong, NeurIPS 2019, <a href="https://zhuanlan.zhihu.com/p/68755034">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.08237.pdf">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a> (Zhilin Yang, NeurIPS 2019, <a href="https://github.com/zihangdai/xlnet">code</a>, <a href="https://zhuanlan.zhihu.com/p/70257427">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.08101.pdf">Pre-Training with Whole Word Masking for Chinese BERT</a> (Yiming Cui, 2019, <a href="https://github.com/ymcui/Chinese-BERT-wwm">code</a>, <a href="https://zhuanlan.zhihu.com/p/96792453">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> (Yinhan Liu, 2019, <a href="https://zhuanlan.zhihu.com/p/82804993">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a> (Mohammad Shoeybi, 2019, <a href="https://github.com/NVIDIA/Megatron-LM">code</a>, <a href="https://www.infoq.cn/article/Ex_tDlV5VoMzLKpOObAf">note</a>)</p>

<p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> (Alec Radford, 2019, <a href="https://github.com/openai/gpt-2">code</a>, <a href="https://zhuanlan.zhihu.com/p/79714797">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> (Colin Raffel, 2019, <a href="https://github.com/google-research/text-to-text-transfer-transformer">code</a>, <a href="https://zhuanlan.zhihu.com/p/88377084">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.11942.pdf">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a> (Zhenzhong Lan, ICLR 2020, <a href="https://www.zhihu.com/question/347898375/answer/863537122">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2003.10555.pdf">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a> (Kevin Clark, ICLR 2020, <a href="https://github.com/google-research/electra">code</a>, <a href="https://zhuanlan.zhihu.com/p/89763176">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2004.04092.pdf">Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space</a> (Chunyuan Li, EMNLP 2020, <a href="https://github.com/ChunyuanLI/Optimus">code</a>, <a href="https://zhuanlan.zhihu.com/p/143517152">note</a>)</p>

<p><a href="https://aclanthology.org/2020.emnlp-main.20.pdf">Pre-Training Transformers as Energy-Based Cloze Models</a> (Kevin Clark, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a> (Tom B. Brown, 2020, <a href="https://github.com/openai/gpt-3">code</a>, <a href="https://zhuanlan.zhihu.com/p/144764546">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a> (William Fedus, 2020)</p>

<p><a href="https://arxiv.org/pdf/2105.03322.pdf">Are Pre-trained Convolutions Better than Pre-trained Transformers?</a> (Yi Tay, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2105.11447.pdf">True Few-Shot Learning with Language Models</a> (Ethan Perez, 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.13884.pdf">Multimodal Few-Shot Learning with Frozen Language Models</a> (Maria Tsimpoukelli, 2021)</p>

<p><a href="https://arxiv.org/pdf/2107.13586.pdf">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a> (Pengfei Liu, 2021, <a href="http://pretrain.nlpedia.ai/">code</a>, <a href="https://zhuanlan.zhihu.com/p/395795968">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2101.00190.pdf">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a> (Xiang Lisa Li, 2021)</p>

<p><a href="https://arxiv.org/pdf/2105.13626.pdf">ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models</a> (Linting Xue, TACL 2022)</p>

<h5 id="knowledge-enriched-language-model">Knowledge-Enriched Language Model</h5>

<p><a href="https://arxiv.org/pdf/1905.07129.pdf">ERNIE: Enhanced Language Representation with Informative Entities</a> (Zhengyan Zhang, ACL 2019, <a href="https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE">code</a>, <a href="https://zhuanlan.zhihu.com/p/87008569">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.12412.pdf">ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a> (Yu Sun, AAAI 2020, <a href="https://zhuanlan.zhihu.com/p/76125042">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2107.02137.pdf">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation</a> (Yu Sun, 2021)</p>

<p><a href="https://arxiv.org/pdf/1909.02209.pdf">Semantics-aware BERT for Language Understanding</a> (Zhuosheng Zhang, AAAI 2020, <a href="https://zhuanlan.zhihu.com/p/115457267">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2002.01808.pdf">K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters</a> (Ruize Wang, 2020)</p>

<h5 id="compressed-language-model">Compressed Language Model</h5>

<p><a href="https://www.aclweb.org/anthology/D19-1361.pdf">Fine-tune BERT with Sparse Self-Attention Mechanism</a> (Baiyun Cui, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1910.01108.pdf">DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter</a> (Victor Sahn, 2019, <a href="https://github.com/huggingface/transformers/tree/master/examples/distillation">code</a>, <a href="https://zhuanlan.zhihu.com/p/89522799">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2001.04246.pdf">AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search</a> (Daoyuan Chen, IJCAI 2020, <a href="https://zhuanlan.zhihu.com/p/144549207">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.537.pdf">FastBERT: a Self-distilling BERT with Adaptive Inference Time</a> (Weijie Liu, ACL 2020, <a href="https://zhuanlan.zhihu.com/p/127869267">note</a>)</p>

<h4 id="text-classification">Text Classification</h4>

<p><a href="http://www.jmlr.org/papers/v2/manevitz01a.html">One-Class SVMs for Document Classification</a> (Larry M. Manevitz, JMLR 2001)</p>

<p><a href="http://www.aclweb.org/anthology/D14-1181">Convolutional Neural Networks for Sentence Classification</a> (Yoon Kim, EMNLP 2014, <a href="https://github.com/brightmart/text_classification">code</a>, <a href="https://zhuanlan.zhihu.com/p/21242710">note</a>)</p>

<p><a href="http://www.nlpr.ia.ac.cn/cip/~liukang/liukangPageFile/Recurrent%20Convolutional%20Neural%20Networks%20for%20Text%20Classification.pdf">Recurrent Convolutional Neural Networks for Text Classification</a> (Siwei Lai, AAAI 2015, <a href="https://github.com/brightmart/text_classification">code</a>, <a href="https://zhuanlan.zhihu.com/p/21253220">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/N15-1011">Effective Use of Word Order for Text Categorization with Convolutional Neural Networks</a> (Rie Johnson, NAACL 2015, <a href="https://github.com/riejohnson/ConText">code</a>)</p>

<p><a href="http://www.aclweb.org/anthology/P15-1162">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</a> (Mohit Iyyer, ACL 2015, <a href="https://github.com/lpq29743/text_classification/blob/master/models/dl_models/dan.py">code1</a>, <a href="https://github.com/miyyer/dan">code2</a>)</p>

<p><a href="https://aclweb.org/anthology/D15-1279">Discriminative Neural Sentence Modeling by Tree-Based Convolution</a> (Lili Mou, EMNLP 2015)</p>

<p><a href="https://arxiv.org/pdf/1509.01626.pdf">Character-level Convolutional Networks for Text Classification</a> (Xiang Zhang, NeurIPS 2015, <a href="https://github.com/dongjun-Lee/text-classification-models-tf">code</a>, <a href="https://zhuanlan.zhihu.com/p/51698513">note</a>)</p>

<p><a href="https://papers.nips.cc/paper/5849-semi-supervised-convolutional-neural-networks-for-text-categorization-via-region-embedding.pdf">Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding</a> (Rie Johnson, NeurIPS 2015, <a href="https://github.com/riejohnson/ConText">code</a>)</p>

<p><a href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf">Hierarchical Attention Networks for Document Classification</a> (Zichao Yang, NAACL 2016, <a href="https://github.com/brightmart/text_classification">code</a>, <a href="https://zhuanlan.zhihu.com/p/26892711">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1602.02373.pdf">Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings</a> (Rie Johnson, ICML 2016, <a href="https://github.com/riejohnson/ConText">code</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf">Recurrent Neural Network for Text Classification with Multi-Task Learning</a> (Pengfei Liu, IJCAI 2016, <a href="https://github.com/brightmart/text_classification">code</a>, <a href="https://zhuanlan.zhihu.com/p/27562717">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/C16-1329">Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling</a> (Peng Zhou, COLING 2016)</p>

<p><a href="https://arxiv.org/pdf/1602.00367.pdf">Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers</a> (Yijun Xiao, 2016)</p>

<p><a href="http://www.aclweb.org/anthology/E17-2071">A Hybrid CNN-RNN Alignment Model for Phrase-Aware Sentence Classification</a> (Shiou Tian Hsu, EACL 2017, <a href="https://zhuanlan.zhihu.com/p/35008282">note</a>)</p>

<p><a href="http://cn.arxiv.org/pdf/1606.01781">Very Deep Convolutional Networks for Text Classification</a> (Alexis Conneau, EACL 2017, <a href="https://github.com/zonetrooper32/VDCNN">code</a>, <a href="https://zhuanlan.zhihu.com/p/39593725">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P17-1001">Adversarial Multi-task Learning for Text Classification</a> (Pengfei Liu, ACL 2017, <a href="https://github.com/FrankWork/fudan_mtl_reviews">code</a>, <a href="https://zhuanlan.zhihu.com/p/31653852">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P17-1052">Deep Pyramid Convolutional Neural Networks for Text Categorization</a> (Rie Johnson, ACL 2017, <a href="https://github.com/riejohnson/ConText">code</a>, <a href="https://zhuanlan.zhihu.com/p/56189443">note</a>)</p>

<p><a href="https://aclweb.org/anthology/D18-1484">Multi-Task Label Embedding for Text Classification</a> (Honglun Zhang, EMNLP 2017, <a href="https://zhuanlan.zhihu.com/p/37669263">note</a>)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/zhang.pdf">Learning Structured Representation for Text Classification via Reinforcement Learning</a> (Tianyang Zhang, AAAI 2018, <a href="https://github.com/keavil/AAAI18-code">code</a>, <a href="https://zhuanlan.zhihu.com/p/36836402">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1806.05516.pdf">Translations as Additional Contexts for Sentence Classification</a> (Reinald Kim Amplayo, IJCAI 2018, <a href="https://github.com/rktamplayo/MCFA">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.09843.pdf">Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms</a> (Dinghan Shen, ACL 2018, <a href="https://github.com/dinghanshen/SWEM">code</a>, <a href="https://zhuanlan.zhihu.com/p/38056365">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1216">Joint Embedding of Words and Labels for Text Classification</a> (Guoyin Yang, ACL 2018, <a href="https://github.com/guoyinwang/LEAM">code</a>, <a href="https://zhuanlan.zhihu.com/p/54734708">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.05588.pdf">Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding</a> (Bingfeng Luo, ACL 2018, <a href="https://zhuanlan.zhihu.com/p/43815470">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.05679v1.pdf">Graph Convolutional Networks for Text Classification</a> (Liang Yao, AAAI 2019, <a href="https://github.com/yao8839836/text_gcn">code</a>, <a href="https://zhuanlan.zhihu.com/p/75708556">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.00453.pdf">Topics to Avoid: Demoting Latent Confounds in Text Classification</a> (Sachin Kumar, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1904.08398.pdf">DocBERT: BERT for Document Classification</a> (Ashutosh Adhikari, 2019, <a href="https://github.com/castorini/hedwig">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2010.07245.pdf">Text Classification Using Label Names Only: A Language Model Self-Training Approach</a> (Yu Meng, EMNLP 2020, <a href="https://github.com/yumeng5/LOTClass">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2021.naacl-main.333.pdf">Inductive Topic Variational Graph Auto-Encoder for Text Classification</a> (Qianqian Xie, NAACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2108.02035.pdf">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</a> (Shengding Hu, 2021)</p>

<h5 id="multi-label-text-classification">Multi-Label Text Classification</h5>

<p><a href="https://arxiv.org/pdf/1808.08561.pdf">Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification</a> (Junyang Lin, EMNLP 2018, <a href="https://github.com/lancopku/SU4MLC">code</a>, <a href="https://zhuanlan.zhihu.com/p/59546989">note</a>)</p>

<p><a href="http://aclweb.org/anthology/C18-1330">SGM: Sequence Generation Model for Multi-Label Classification</a> (Pengcheng Yang, COLING 2018, <a href="https://github.com/lancopku/SGM">code</a>, <a href="https://zhuanlan.zhihu.com/p/53910836">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1518.pdf">A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification</a> (Pengcheng Yang, ACL 2019, <a href="https://github.com/lancopku/Seq2Set">code</a>, <a href="https://blog.csdn.net/MaybeForever/article/details/102822057">note</a>)</p>

<p><a href="http://papers.nips.cc/paper/8817-attentionxml-label-tree-based-attention-aware-deep-model-for-high-performance-extreme-multi-label-text-classification.pdf">AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification</a> (Ronghui You, NeurIPS 2019, <a href="https://github.com/yourh/AttentionXML">code</a>, <a href="https://zhuanlan.zhihu.com/p/96759318">note</a>)</p>

<h4 id="text-matching">Text Matching</h4>

<p><a href="https://dl.acm.org/doi/abs/10.1145/2505515.2505665">Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</a> (Po-Sen Huang, CIKM 2013, <a href="https://github.com/liaha/dssm">code</a>, <a href="https://zhuanlan.zhihu.com/p/53326791">note</a>)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/www2014_cdssm_p07.pdf">Learning Semantic Representations Using Convolutional Neural Networks for Web Search</a> (Yelong Shen, WWW 2014)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2014_cdssm_final.pdf">A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval</a> (Yelong Shen, CIKM 2014, <a href="https://github.com/airalcorn2/Deep-Semantic-Similarity-Model">code</a>, <a href="https://zhuanlan.zhihu.com/p/32915377">note</a>)</p>

<p><a href="http://www.hangli-hl.com/uploads/3/1/6/8/3168008/hu-etal-nips2014.pdf">Convolutional Neural Network Architectures for Matching Natural Language Sentences</a> (Baotian Hu, NeurIPS 2014, <a href="https://github.com/ddddwy/ARCII-for-Matching-Natural-Language-Sentences">code</a>)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.723.6492&amp;rep=rep1&amp;type=pdf">Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks</a> (Aliaksei Severyn, SIGIR 2015, <a href="https://github.com/zhangzibin/PairCNN-Ranking">code</a>, <a href="https://zhuanlan.zhihu.com/p/32915377">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1511.08277.pdf">A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations</a> (Shengxian Wan, 2015, <a href="https://github.com/coderbyr/MV-LSTM">code</a>, <a href="https://zhuanlan.zhihu.com/p/40741576">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1512.05193.pdf">ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</a> (Wenpeng Yin, TACL 2016, <a href="https://github.com/shamalwinchurkar/question-classification?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1602.06359.pdf">Text Matching as Image Recognition</a> (Liang Pang, AAAI 2016, <a href="https://github.com/ddddwy/MatchPyramid-for-semantic-matching">code</a>, <a href="https://zhuanlan.zhihu.com/p/40741576">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/N16-1108">Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement</a> (Hua He, NAACL 2016, <a href="https://github.com/lanwuwei/Subword-PWIM">code</a>)</p>

<p><a href="http://www.aclweb.org/anthology/P16-1044">Improved Representation Learning for Question Answer Matching</a> (Ming Tan, ACL 2016, <a href="https://github.com/person-lee/qa_lstm">code</a>, <a href="https://zhuanlan.zhihu.com/p/23163137">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1711.08611.pdf">A Deep Relevance Matching Model for Ad-hoc Retrieval</a> (Jiafeng Guo, CIKM 2016, <a href="https://github.com/sebastian-hofstaetter/neural-ranking-drmm">code</a>, <a href="https://zhuanlan.zhihu.com/p/94195125">note</a>)</p>

<p><a href="http://maroo.cs.umass.edu/pub/web/getpdf.php?id=1240">aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model</a> (Liu Yang, CIKM 2016, <a href="https://github.com/yangliuy/aNMM-CIKM16">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1611.01747.pdf">A Compare-Aggregate Model for Matching Text Sequences</a> (Shuohang Wang, 2016, <a href="https://github.com/pcgreat/SeqMatchSeq">code</a>, <a href="https://zhuanlan.zhihu.com/p/27805225">note</a>)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/wwwfp0192-mitra.pdf">Learning to Match using Local and Distributed Representations of Text for Web Search</a> (Bhaskar Mitra, WWW 2017, <a href="https://www.jianshu.com/p/ab24387a076b">note</a>)</p>

<p><a href="http://delivery.acm.org/10.1145/3090000/3080809/p55-xiong.pdf?ip=218.19.145.8&amp;id=3080809&amp;acc=CHORUS&amp;key=BF85BBA5741FDC6E%2E3D07CFA6C3F555EA%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1545743485_1f9609809da82437ccf634dc7f881b4b">End-to-End Neural Ad-hoc Ranking with Kernel Pooling</a> (Chenyan Xiong, SIGIR 2017, <a href="https://github.com/AdeDZY/K-NRM">code</a>, <a href="https://blog.csdn.net/SrdLaplace/article/details/86481422">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1702.03814.pdf">Bilateral Multi-Perspective Matching for Natural Language Sentences</a> (Zhiguo Wang, IJCAI 2017, <a href="https://github.com/zhiguowang/BiMPM">code</a>, <a href="https://zhuanlan.zhihu.com/p/50184415">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/C/C16/C16-1127.pdf">Sentence Similarity Learning by Lexical Decomposition and Composition</a> (Zhiguo Wang, COLING 2017, <a href="https://github.com/mcrisc/lexdecomp">code</a>, <a href="http://octopuscoder.github.io/2017/08/18/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB-Sentence-Similarity-Learning-by-Lexical-Decomposition-and-Composition/">note</a>)</p>

<p><a href="http://delivery.acm.org/10.1145/3160000/3159659/p126-dai.pdf?ip=218.19.145.8&amp;id=3159659&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E3D07CFA6C3F555EA%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1545800172_2be1e8f46ecb3f388bf3ceab8566848e">Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search</a> (Zhuyun Dai, WSDM 2018, <a href="https://github.com/thunlp/EntityDuetNeuralRanking">code</a>, <a href="https://blog.csdn.net/SrdLaplace/article/details/86481422">note</a>)</p>

<p><a href="http://www2.aueb.gr/users/ion/docs/emnlp2018.pdf">Deep Relevance Ranking Using Enhanced Document-Query Interactions</a> (Ryan McDonald, EMNLP 2018, <a href="https://github.com/nlpaueb/deep-relevance-ranking">code</a>, <a href="https://zhuanlan.zhihu.com/p/46755219">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.11360.pdf">Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information</a> (Seonhoon Kim, AAAI 2019, <a href="https://zhuanlan.zhihu.com/p/47948866">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2005.13111.pdf">Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport</a> (Kyle Swanson, ACL 2020)</p>

<h4 id="natural-language-inference">Natural Language Inference</h4>

<p><a href="https://arxiv.org/pdf/1508.05326.pdf">A Large Annotated Corpus for Learning Natural Language Inference</a> (Samuel R. Bowman, EMNLP 2015, <a href="https://nlp.stanford.edu/projects/snli/">code</a>, <a href="https://blog.eson.org/pub/a1c27ad7/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1512.08422.pdf">Natural Language Inference by Tree-Based Convolution and Heuristic Matching</a> (Lili Mou, ACL 2016, <a href="https://www.paperweekly.site/papers/notes/344">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1606.01933.pdf">A Decomposable Attention Model for Natural Language Inference</a> (Ankur P. Parikh, EMNLP 2016, <a href="https://github.com/harvardnlp/decomp-attn">code</a>, <a href="https://zhuanlan.zhihu.com/p/26237357">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1609.06038.pdf">Enhanced LSTM for Natural Language Inference</a> (Chen Qian, ACL 2017, <a href="https://github.com/coetaur0/ESIM">code</a>, <a href="https://zhuanlan.zhihu.com/p/47580077">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1705.02364.pdf">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a> (Alexis Conneau, EMNLP 2017, <a href="https://github.com/facebookresearch/InferSent">code</a>, <a href="https://blog.csdn.net/sinat_31188625/article/details/77992960">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1709.04348.pdf">Natural Language Inference over Interaction Space</a> (Yichen Gong, ICLR 2018, <a href="https://github.com/YichenGong/Densely-Interactive-Inference-Network">code</a>, <a href="https://www.cnblogs.com/databingo/p/9311892.html">note</a>)</p>

<p><a href="https://aclweb.org/anthology/P18-1091">Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference</a> (Boyuan Pan, ACL 2018, <a href="https://github.com/ZJULearning/DMP">code</a>, <a href="https://zhuanlan.zhihu.com/p/37899900">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1711.04289.pdf">Neural Natural Language Inference Models Enhanced with External Knowledge</a> (Chen Qian, ACL 2018, <a href="https://github.com/feifengwhu/NLP_External_knowledge">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.05724.pdf">Improving Natural Language Inference Using External Knowledge in the Science Questions Domain</a> (Xiaoyan Wang, 2018, <a href="https://zhuanlan.zhihu.com/p/77646912">note</a>)</p>

<p><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4614">Gaussian Transformer: A Lightweight Approach for Natural Language Inference</a> (Maosheng Guo, AAAI 2019, <a href="https://github.com/lzy1732008/GaussionTransformer">code</a>, <a href="https://zhuanlan.zhihu.com/p/75411024">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2004.03066.pdf">Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition</a> (Paloma Jeretic, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.14839.pdf">Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?</a> (Hitomi Yanaka, ACL 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.774.pdf">Uncertain Natural Language Inference</a> (Tongfei Chen, ACL 2020)</p>

<h4 id="text-summarization">Text Summarization</h4>

<p><a href="https://arxiv.org/pdf/1904.03651.pdf">SEQ3 : Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</a> (Christos Baziotis, NAACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.01610.pdf">Answers Unite! Unsupervised Metrics for Reinforced Summarization Models</a> (Thomas Scialom, EMNLP 2019, <a href="https://github.com/recitalAI/summa-qa?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.01214.pdf">Better Rewards Yield Better Summaries: Learning to Summarise Without References</a> (Florian Bohm, EMNLP 2019, <a href="https://github.com/yg211/summary-reward-no-reference?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.08960.pdf">Neural Text Summarization: A Critical Evaluation</a> (Wojciech Kryscinski, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1908.08345.pdf">Text Summarization with Pretrained Encoders</a> (Yang Liu, EMNLP 2019, <a href="https://github.com/nlpyang/PreSumm">code</a>, <a href="https://zhuanlan.zhihu.com/p/88953532">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2010.04529.pdf">What Have We Achieved on Text Summarization?</a> (Dandan Huang, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.07100.pdf">Re-evaluating Evaluation in Text Summarization</a> (Manik Bhandari, EMNLP 2020, <a href="https://github.com/neulab/REALSumm?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2010.01781.pdf">Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning</a> (Hanlu Wu, EMNLP 2020, <a href="https://github.com/whl97/LS-Score?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2012.07419.pdf">The Style-Content Duality of Attractiveness: Learning to Write Eye-Catching Headlines via Disentanglement</a> (Mingzhe Li, AAAI 2021)</p>

<h5 id="extractive-summarization">Extractive Summarization</h5>

<p><a href="http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf">The Automatic Creation of Literature Abstracts</a> (H. P. Luhn, 1958, <a href="https://github.com/miso-belica/sumy">code</a>)</p>

<p><a href="http://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf">New Methods in Automatic Extracting</a> (H. P. Edmundson, 1969, <a href="https://github.com/miso-belica/sumy">code</a>)</p>

<p><a href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">TextRank: Bringing Order into Texts</a> (Rada Mihalcea, EMNLP 2004, <a href="https://github.com/miso-belica/sumy">code</a>, <a href="https://zhuanlan.zhihu.com/p/55270310">note</a>)</p>

<p><a href="http://www.kiv.zcu.cz/~jstein/publikace/isim2004.pdf">Using Latent Semantic Analysis in Text Summarization and Summary Evaluation</a> (Josef Steinberger, 2004, <a href="https://github.com/miso-belica/sumy">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1109.2128.pdf">LexRank: Graph-based Lexical Centrality as Salience in Text Summarization</a> (Gunes Erkan, 2004, <a href="https://github.com/miso-belica/sumy">code</a>)</p>

<p><a href="http://www.cis.upenn.edu/~nenkova/papers/ipm.pdf">Beyond SumBasic: Task-Focused Summarization with Sentence Simplification and Lexical Expansion</a> (Lucy Vanderwende, 2007, <a href="https://github.com/miso-belica/sumy">code</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/10958">SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents</a> (Ramesh Nallapati, AAAI 2017)</p>

<p><a href="https://arxiv.org/pdf/1802.08636.pdf">Ranking Sentences for Extractive Summarization with Reinforcement Learning</a> (Shashi Narayan, NAACL 2018, <a href="https://github.com/EdinburghNLP/Refresh?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1804.11283.pdf">Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies</a> (Max Grusky, NAACL 2018, <a href="https://github.com/SumUpAnalytics/goldsum?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.09672.pdf">BanditSum: Extractive Summarization as a Contextual Bandit</a> (Yue Dong, EMNLP 2018, <a href="https://github.com/yuedongP/BanditSum?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1808.07187.pdf">Neural Latent Extractive Document Summarization</a> (Xingxing Zhang, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1904.02321.pdf">Guiding Extractive Summarization with Question-Answering Rewards</a> (Kristjan Arumae, NAACL 2019, <a href="https://github.com/ucfnlp/summ_qa_rewards?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/N19-1173.pdf">Single Document Summarization as Tree Induction</a> (Yang Liu, NAACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1499.pdf">HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</a> (Xingxing Zhang, ACL 2019, <a href="https://zhuanlan.zhihu.com/p/93598582">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.03491.pdf">Searching for Effective Neural Extractive Summarization: What Works and What’s Next</a> (Ming Zhong, ACL 2019, <a href="https://github.com/maszhongming/Effective_Extractive_Summarization?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1902.00863.pdf">Neural Extractive Text Summarization with Syntactic Compression</a> (Jiacheng Xu, EMNLP 2019, <a href="https://github.com/jiacheng-xu/neu-compression-sum?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2004.08795.pdf">Extractive Summarization as Text Matching</a> (MIng Zhong, ACL 2020, <a href="https://github.com/maszhongming/MatchSum">code</a>, <a href="https://zhuanlan.zhihu.com/p/138351484">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1910.14142.pdf">Discourse-Aware Neural Extractive Text Summarization</a> (Jiacheng Xu, ACL 2020, <a href="https://github.com/jiacheng-xu/DiscoBERT">code</a>, <a href="https://procjx.github.io/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Discourse-Aware-Neural-Extractive-Model-for-Text-Summarization/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2004.12393.pdf">Heterogeneous Graph Neural Networks for Extractive Document Summarization</a> (Danqing Wang, ACL 2020, <a href="https://github.com/brxx122/HeterSumGraph">code</a>, <a href="https://zhuanlan.zhihu.com/p/138600416">note</a>)</p>

<h5 id="abstractive-summarization">Abstractive Summarization</h5>

<p><a href="https://arxiv.org/pdf/1509.00685.pdf">A Neural Attention Model for Abstractive Sentence Summarization</a> (Alexander M. Rush, EMNLP 2015, <a href="https://github.com/Ganeshpadmanaban/Neural-Attention-Model-Abstractive-Summarization?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1602.06023.pdf">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</a> (Ramesh Nallapati, CoNLL 2016, <a href="https://zhuanlan.zhihu.com/p/21388527">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P17-1108.pdf">Abstractive Document Summarization with a Graph-Based Attentional Neural Model</a> (Jiwei Tan, ACL 2017)</p>

<p><a href="https://arxiv.org/pdf/1704.04368.pdf">Get To The Point: Summarization with Pointer-Generator Networks</a> (Abigail See, ACL 2017)</p>

<p><a href="https://arxiv.org/pdf/1705.04304.pdf">A Deep Reinforced Model for Abstractive Summarization</a> (Romain Paulus, 2017, <a href="https://github.com/oceanypt/A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION">code</a>, <a href="https://zhuanlan.zhihu.com/p/59510696">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1803.10357.pdf">Deep Communicating Agents for Abstractive Summarization</a> (Asli Celikyilmaz, NAACL 2018, <a href="https://github.com/theDoctor2013/DCA-AbstractiveSummarization?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://www.ijcai.org/proceedings/2018/0619.pdf">A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization</a> (Li Wang, IJCAI 2018, <a href="https://blog.csdn.net/imsuhxz/article/details/82655811">note</a>)</p>

<p><a href="https://aclweb.org/anthology/W18-2706">Controllable Abstractive Summarization</a> (Angela Fan, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1805.11080.pdf">Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</a> (Yen-Chun Chen, ACL 2018, <a href="https://github.com/ChenRocks/fast_abs_rl?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1808.10792.pdf">Bottom-Up Abstractive Summarization</a> (Sebastian Gehrmann, EMNLP 2018, <a href="https://github.com/sebastianGehrmann/bottom-up-summary?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://aclweb.org/anthology/D18-1206">Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization</a> (Shashi Narayan, EMNLP 2018, <a href="https://github.com/EdinburghNLP/XSum/tree/master/XSum-Topic-ConvS2S">code</a>, <a href="https://zhuanlan.zhihu.com/p/92994889">note</a>)</p>

<p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/5056">Abstractive Summarization: A Survey of the State of the Art</a> (Hui Lin, AAAI 2019)</p>

<p><a href="https://arxiv.org/pdf/1811.00783.pdf">Abstractive Summarization of Reddit Posts with Multi-level Memory Networks</a> (Byeongchang Kim, NAACL 2019, <a href="https://github.com/ctr4si/MMN">code</a>, <a href="https://zhuanlan.zhihu.com/p/62333393">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.00077.pdf">Scoring Sentence Singletons and Pairs for Abstractive Summarization</a> (Logan Lebanoff, ACL 2019, <a href="https://github.com/ucfnlp/summarization-sing-pair-mix">code</a>, <a href="https://www.aminer.cn/research_report/5d47c526d5e908133c9468eb">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.08837.pdf">How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing</a> (Shen Gao, EMNLP 2019, <a href="https://github.com/gsh199449/proto-summ">code</a>, <a href="https://www.icst.pku.edu.cn/xwgg/xwdt/2019/1318876.htm">note</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6420">Controlling the Amount of Verbatim Copying in Abstractive Summarization</a> (Kaiqiang Song, AAAI 2020)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6419">Joint Parsing and Generation for Abstractive Summarization</a> (Kaiqiang Song, AAAI 2020)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6333">Keywords-Guided Abstractive Sentence Summarization</a> (Haoran Li, AAAI 2020)</p>

<p><a href="https://arxiv.org/pdf/1912.08777.pdf">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</a> (Jingqing Zhang, ICML 2020, <a href="https://github.com/google-research/pegasus">code</a>, <a href="https://www.linkresearcher.com/theses/7054cdb3-b934-4fd8-9e5b-b4a320f5c6c7">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2002.10375.pdf">Discriminative Adversarial Search for Abstractive Summarization</a> (Thomas Scialom, ICML 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.455.pdf">Fact-based Content Weighting for Evaluating Abstractive Summarisation</a> (Xinnuo Xu, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.00661.pdf">On Faithfulness and Factuality in Abstractive Summarization</a> (Joshua Maynez, ACL 2020, <a href="https://github.com/google-research-datasets/xsum_hallucination_annotations?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1911.02541.pdf">Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports</a> (Yuhao Zhang, ACL 2020, <a href="https://zhuanlan.zhihu.com/p/166193118">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.125.pdf">Self-Attention Guided Copy Mechanism for Abstractive Summarization</a> (Song Xu, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.03754.pdf">FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization</a> (Esin Durmus, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.04228.pdf">Asking and Answering Questions to Evaluate the Factual Consistency of Summaries</a> (Alex Wang, ACL 2020, <a href="https://zhuanlan.zhihu.com/p/130280217">note</a>)</p>

<p><a href="http://people.ischool.berkeley.edu/~hearst/papers/Laban_ACL2020_Abstractive_Summarization.pdf">The Summary Loop: Learning to Write Abstractive Summaries Without Examples</a> (Philippe Laban, ACL 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.750.pdf">Evaluating the Factual Consistency of Abstractive Text Summarization</a> (Wojciech Kryscinski, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.13312.pdf">Reducing Quantity Hallucinations in Abstractive Summarization</a> (Zheng Zhao, EMNLP 2020 Findings)</p>

<p><a href="https://arxiv.org/pdf/2009.01325.pdf">Learning to Summarize from Human Feedback</a> (Nisan Stiennon, NeurIPS 2020)</p>

<h5 id="multi-document-summarization">Multi-Document Summarization</h5>

<p><a href="https://www.aclweb.org/anthology/N09-1041">Exploring Content Models for Multi-Document Summarization</a> (Aria Haghighi, NAACL 2009, <a href="https://github.com/miso-belica/sumy">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1808.06218.pdf">Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization</a> (Logan Lebanoff, EMNLP 2018, <a href="https://github.com/ucfnlp/multidoc_summarization?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/8736808">Abstractive Multi-Document Summarization Based on Semantic Link Network</a> (Wei Li, TKDE 2019)</p>

<p><a href="https://arxiv.org/pdf/1905.13164.pdf">Hierarchical Transformers for Multi-Document Summarization</a> (Yang Liu, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.00072.pdf">Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization</a> (Sangwoo Cho, ACL 2019, <a href="https://github.com/ucfnlp/summarization-dpp-capsnet">code</a>, <a href="https://wemp.app/posts/beb15b57-0dd1-4ec8-8d7a-47a1de75dbbb">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.01749.pdf">Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model</a> (Alexander R. Fabbri, ACL 2019, <a href="https://github.com/Alex-Fabbri/Multi-News">code</a>, <a href="https://zhuanlan.zhihu.com/p/83768781">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2005.10043.pdf">Leveraging Graph to Improve Abstractive Multi-Document Summarization</a> (Wei Li, ACL 2020, <a href="https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2020-GraphSum?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2010.00117.pdf">Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning</a> (Yuning Mao, EMNLP 2020, <a href="https://github.com/morningmoni/RL-MMR?utm_source=catalyzex.com">code</a>)</p>

<h5 id="opinion-summarization">Opinion Summarization</h5>

<p><a href="https://arxiv.org/pdf/2004.10150.pdf">Unsupervised Opinion Summarization with Noising and Denoising</a> (Reinald Kim Amplayo, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2012.07808.pdf">Unsupervised Opinion Summarization with Content Planning</a> (Reinald Kim Amplayo, AAAI 2021)</p>

<h5 id="cross-lingual-summarization">Cross-Lingual Summarization</h5>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.121.pdf">Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization</a> (Junnan Zhu, ACL 2020)</p>

<h4 id="text-style-transfer">Text Style Transfer</h4>

<p><a href="https://arxiv.org/pdf/1705.09655.pdf">Style Transfer from Non-Parallel Text by Cross-Alignment</a> (Tianxiao Shen, NeurIPS 2017, <a href="https://github.com/shentianxiao/language-style-transfer?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1711.06861.pdf">Style Transfer in Text: Exploration and Evaluation</a> (Zhenxin Fu, AAAI 2018, <a href="https://github.com/fuzhenxin/text_style_transfer?utm_source=catalyzex.com">code</a>, <a href="https://zhuanlan.zhihu.com/p/32300981">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/N18-1169.pdf">Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer</a> (Juncen Li, NAACL 2018, <a href="https://github.com/rpryzant/delete_retrieve_generate">code</a>, <a href="https://blog.csdn.net/u014475479/article/details/81945534">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1804.09000.pdf">Style Transfer Through Back-Translation</a> (Shrimai Prabhumoye, ACL 2018, <a href="https://github.com/shrimai/Style-Transfer-Through-Back-Translation?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.11749.pdf">Unsupervised Text Style Transfer using Language Models as Discriminators</a> (Zichao Yang, NeurIPS 2018, <a href="https://github.com/asyml/texar/tree/master/examples/text_style_transfer?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1903.10671.pdf">Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus</a> (Hongyu Gong, NAACL 2019, <a href="https://github.com/HongyuGong/TextStyleTransfer">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.10060.pdf">A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer</a> (Fuli Luo, IJCAI 2019, <a href="https://github.com/luofuli/DualRL?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.08039.pdf">Mask and Infill: Applying Masked Language Model for Sentiment Transfer</a> (Xing Wu, IJCAI 2019, <a href="https://github.com/IIEKES/MLM_transfer">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1041.pdf">Disentangled Representation Learning for Non-Parallel Text Style Transfer</a> (Vineet John, ACL 2019, <a href="https://github.com/h3lio5/linguistic-style-transfer-pytorch">code</a>, <a href="https://zhuanlan.zhihu.com/p/102783862">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.01833.pdf">A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer</a> (Chen Wu, ACL 2019, <a href="https://github.com/ChenWu98/Point-Then-Operate">code</a>, <a href="https://www.cnblogs.com/bernieloveslife/p/12748942.html">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.05621.pdf">Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation</a> (Ning Dai, ACL 2019, <a href="https://github.com/fastnlp/style-transformer?utm_source=catalyzex.com">code</a>, <a href="http://ziyangluo.tech/2020/05/12/TSTpaper2StyleTran/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.11493.pdf">Semi-supervised Text Style Transfer: Cross Projection in Latent Space</a> (Mingyue Shang, EMNLP 2019)</p>

<p><a href="https://openreview.net/pdf?id=H1g2NhC5KQ">Multiple-Attribute Text Style Transfer</a> (Guillaume Lample, ICLR 2019)</p>

<h4 id="topic-modeling">Topic Modeling</h4>

<h5 id="unsupervised-topic-modeling">Unsupervised Topic Modeling</h5>

<p><a href="http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf">Indexing by Latent Semantic Analysis</a> (Scott Deerwester, 1990)</p>

<p><a href="http://lsa.colorado.edu/papers/dp1.LSAintro.pdf">An Introduction to Latent Semantic Analysis</a> (Thomas K Landauer, 1998, <a href="https://github.com/josephwilk/semanticpy">code</a>, <a href="https://zhuanlan.zhihu.com/p/37873878">note</a>)</p>

<p><a href="http://www.iro.umontreal.ca/~nie/IFT6255/Hofmann-UAI99.pdf">Probabilistic Latent Semantic Analysis</a> (Thomas K Landauer, 1999, <a href="https://github.com/laserwave/plsa">code</a>, <a href="https://zhuanlan.zhihu.com/p/37873878">note</a>)</p>

<p><a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Latent Dirichlet Allocation</a> (David M. Blei, JMLR 2003, <a href="https://github.com/lda-project/lda">code</a>, <a href="https://zhuanlan.zhihu.com/p/37873878">note</a>)</p>

<p><a href="http://papers.neurips.cc/paper/2906-correlated-topic-models.pdf">Correlated Topic Models</a> (David M. Blei, NeurIPS 2005)</p>

<p><a href="https://papers.nips.cc/paper/4613-a-neural-autoregressive-topic-model.pdf">A Neural Autoregressive Topic Model</a> (Hugo Larochelle, NeurIPS 2012, <a href="https://github.com/AYLIEN/docnade">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1412.1576.pdf">LightLDA: Big Topic Models on Modest Computer Clusters</a> (Jinhui Yuan, WWW 2015, <a href="https://github.com/microsoft/LightLDA">code</a>, <a href="http://d0evi1.com/lightlda/">note</a>)</p>

<p><a href="https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/view/10847/10978">Short and Sparse Text Topic Modeling via Self-Aggregation</a> (Xiaojun Quan, IJCAI 2015, <a href="https://github.com/WHUIR/SATM">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1605.02019.pdf">Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec</a> (Christopher Moody, 2016, <a href="https://github.com/cemoody/lda2vec">code</a>, <a href="https://zhuanlan.zhihu.com/p/37873878">note</a>)</p>

<p><a href="https://www.kdd.org/kdd2016/papers/files/rpp1190-zuoA.pdf">Topic Modeling of Short Texts: A Pseudo-Document View</a> (Yuan Zuo, KDD 2016)</p>

<p><a href="http://proceedings.mlr.press/v77/zhao17a/zhao17a.pdf">A Word Embeddings Informed Focused Topic Model</a> (He Zhao, ACML 2017)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14170/14086">Incorporating Knowledge Graph Embeddings into Topic Modeling</a> (Liang Yao, AAAI 2017, <a href="https://blog.csdn.net/smileyk/article/details/78221342">note</a>)</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/8594882">ASTM: An Attentional Segmentation Based Topic Model for Short Texts</a> (Jiamiao Wang, ICDM 2018, <a href="https://github.com/wjmzjx/ASTM">code</a>)</p>

<p><a href="http://dmkd.cs.vt.edu/papers/WWW18.pdf">Short-Text Topic Modeling via Non-negative Matrix Factorization Enriched with Local Word-Context Correlations</a> (Tian Shi, WWW 2018, <a href="https://github.com/tshi04/SeaNMF">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-2040.pdf">Improving Topic Quality by Promoting Named Entities in Topic Modeling</a> (Katsiaryna Krasnashchok, ACL 2018)</p>

<p><a href="http://proceedings.mlr.press/v80/zhao18a/zhao18a.pdf">Inter and Intra Topic Structure Learning with Word Embeddings</a> (He Zhao, ICML 2018, <a href="https://github.com/ethanhezhao/WEDTM">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.06709.pdf">Document Informed Neural Autoregressive Topic Models with Distributional Prior</a> (Pankaj Gupta, AAAI 2019, <a href="https://github.com/pgcool/iDocNADEe">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1810.03947.pdf">textTOvec: Deep Contextualized Neural Autoregressive Topic Models of Language with Distributed Compositional Prior</a> (Pankaj Gupta, ICLR 2019, <a href="https://github.com/pgcool/textTOvec">code</a>)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3289600.3291032">CluWords: Exploiting Semantic Word Clustering Representation for Enhanced Topic Modeling</a> (Felipe Viegas, WSDM 2019)</p>

<p><a href="https://arxiv.org/pdf/1907.05545.pdf">The Dynamic Embedded Topic Model</a> (Adji B. Dieng, 2019, <a href="https://github.com/adjidieng/DETM">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.04907.pdf">Topic Modeling in Embedding Spaces</a> (Adji B. Dieng, 2019, <a href="https://github.com/adjidieng/ETM">code</a>, <a href="https://zhuanlan.zhihu.com/p/105741773">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.548.pdf">Neural Mixed Counting Models for Dispersed Topic Discovery</a> (Jiemin Wu, ACL 2020)</p>

<p><a href="https://yangliang.github.io/pdf/www20.pdf">Graph Attention Topic Modeling Network</a> (Liang Yang, WWW 2020)</p>

<h5 id="supervised-topic-modeling">Supervised Topic Modeling</h5>

<p><a href="https://papers.nips.cc/paper/3328-supervised-topic-models.pdf">Supervised Topic Models</a> (David M. Blei, NeurIPS 2008)</p>

<p><a href="https://www.aclweb.org/anthology/D09-1026.pdf">Labeled LDA: A Supervised Topic Model for Credit Attribution in Multi-Labeled Corpora</a> (Daniel Ramage, EMNLP 2009, <a href="https://github.com/JoeZJH/Labeled-LDA-Python">code</a>, <a href="https://blog.csdn.net/qy20115549/article/details/90771054">note</a>)</p>

<p><a href="http://papers.nips.cc/paper/3599-disclda-discriminative-learning-for-dimensionality-reduction-and-classification.pdf">DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a> (Simon Lacoste-Julien, NeurIPS 2009, <a href="https://github.com/teffland/disclda">code</a>)</p>

<p><a href="https://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf">Replicated Softmax: an Undirected Topic Model</a> (Ruslan Salakhutdinov, NeurIPS 2009)</p>

<p><a href="http://susandumais.com/kdd2011-pldp-final.pdf">Partially Labeled Topic Models for Interpretable Text Mining</a> (Daniel Ramage, KDD 2011)</p>

<p><a href="http://www.jmlr.org/papers/volume13/zhu12a/zhu12a.pdf">MedLDA: Maximum Margin Supervised Topic Models</a> (Jun Zhu, JMLR 2013)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.4032&amp;rep=rep1&amp;type=pdf">A Biterm Topic Model for Short Texts</a> (Xiaohui Yan, WWW 2013, <a href="https://github.com/markoarnauto/biterm">code</a>, <a href="https://blog.csdn.net/windows2/article/details/16812363">note</a>)</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/6778764">BTM: Topic Modeling over Short Texts</a> (Xueqi Chen, TKDE 2014)</p>

<p><a href="https://www.aclweb.org/anthology/D15-1037.pdf">Efficient Methods for Incorporating Knowledge into Topic Models</a> (Yi Yang, EMNLP 2015)</p>

<p><a href="https://www.aclweb.org/anthology/Q15-1022v2.pdf">Improving Topic Models with Latent Feature Word Representations</a> (Dat Quoc Nguyen, TACL 2015, <a href="https://github.com/datquocnguyen/LFTM">code</a>)</p>

<p><a href="https://www.ntu.edu.sg/home/AXSun/paper/sigir16text.pdf">Topic Modeling for Short Texts with Auxiliary Word Embeddings</a> (Chenliang Li, SIGIR 2016, <a href="https://github.com/NobodyWHU/GPUDMM">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1707.00206.pdf">Efficient Correlated Topic Modeling with Topic Embedding</a> (Junxian He, KDD 2017)</p>

<p><a href="https://www.aclweb.org/anthology/D17-1203.pdf">Adapting Topic Models using Lexical Associations with Tree Priors</a> (Weiwei Yang, EMNLP 2017)</p>

<p><a href="https://arxiv.org/pdf/1709.06365.pdf">MetaLDA: A Topic Model that Efficiently Incorporates Meta Information</a> (He Zhao, ICDM 2017, <a href="https://github.com/ethanhezhao/MetaLDA">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1611.10277.pdf">Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge</a> (Ryan J. Gallagher, TACL 2017, <a href="https://github.com/gregversteeg/corex_topic">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-2083.pdf">PhraseCTM: Correlated Topic Modeling on Phrases within Markov Random Fields</a> (Weijie Huang, ACL 2018)</p>

<p><a href="http://papers.nips.cc/paper/8020-dirichlet-belief-networks-for-topic-structure-learning.pdf">Dirichlet Belief Networks for Topic Structure Learning</a> (He Zhao, NeurIPS 2018)</p>

<p><a href="https://arxiv.org/pdf/1908.07162.pdf">Discriminative Topic Mining via Category-Name Guided Text Embedding</a> (Yu Meng, WWW 2020, <a href="https://github.com/yumeng5/CatE">code</a>)</p>

<h4 id="keyphrase-extraction">Keyphrase Extraction</h4>

<p><a href="http://202.116.81.74/cache/16/03/web.eecs.umich.edu/ed616fd7b9f50b15ac2f92467a16c9f7/mihalcea.emnlp04.pdf">TextRank: Bringing Order into Texts</a> (Rada Mihalcea, EMNLP 2014, <a href="https://github.com/summanlp/textrank">code</a>, <a href="https://www.jiqizhixin.com/articles/2018-12-28-18">note</a>)</p>

<p><a href="http://memray.me/uploads/acl17-keyphrase-generation.pdf">Deep Keyphrase Generation</a> (Rui Meng, ACL 2017, <a href="https://github.com/memray/seq2seq-keyphrase">code</a>, <a href="https://www.jianshu.com/p/5492e7c916f8">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1808.06773.pdf">Semi-Supervised Learning for Neural Keyphrase Generation</a> (Hai Ye, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1808.07185.pdf">Keyphrase Generation with Correlation Constraints</a> (Jun Chen, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1808.08575.pdf">Title-Guided Encoding for Keyphrase Generation</a> (Wang Chen, AAAI 2019)</p>

<p><a href="https://arxiv.org/pdf/1904.03454.pdf">An Integrated Approach for Keyphrase Generation via Exploring the Power of Retrieval and Extraction</a> (Wang Chen, NAACL 2019, <a href="https://github.com/Chen-Wang-CUHK/KG-KE-KR-M">code</a>, <a href="https://github.com/Chen-Wang-CUHK/KG-KE-KR-M">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/N19-1182.pdf">Glocal: Incorporating Global Information in Local Convolution for Keyphrase Extraction</a> (Animesh Prasad, NAACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/N19-1070.pdf">Keyphrase Generation: A Text Summarization Struggle</a> (Erion Cano, NAACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1515.pdf">Incorporating Linguistic Constraints into Keyphrase Generation</a> (Jing Zhao, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.03889.pdf">Topic-Aware Neural Keyphrase Generation for Social Media Language</a> (Yue Wang, ACL 2019, <a href="https://github.com/yuewang-cuhk/TAKG">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.04106.pdf">Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</a> (Hou Pong Chan, ACL 2019, <a href="https://github.com/kenchan0226/keyphrase-generation-rl">code</a>, <a href="https://procjx.github.io/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Neural-Keyphrase-Generation-via-Reinforcement-Learning-with-Adaptive-Rewards/">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1588.pdf">Using Human Attention to Extract Keyphrase from Microblog Post</a> (Yingyi Zhang, ACL 2019, <a href="https://blog.csdn.net/qq_34325086/article/details/102365847">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1911.02671.pdf">Open Domain Web Keyphrase Extraction Beyond Language Modeling</a> (Lee Xiong, EMNLP 2019)</p>

<h4 id="word-segmentation">Word Segmentation</h4>

<p><a href="https://www.aclweb.org/anthology/P17-1110.pdf">Adversarial Multi-Criteria Learning for Chinese Word Segmentation</a> (Xinchi Chen, ACL 2017)</p>

<p><a href="https://arxiv.org/pdf/1808.06511.pdf">State-of-the-art Chinese Word Segmentation with Bi-LSTMs</a> (Ji Ma, 2018, <a href="https://github.com/efeatikkan/Chinese_Word_Segmenter?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.734v2.pdf">Improving Chinese Word Segmentation with Wordhood Memory Networks</a> (Yuanhe Tian, ACL 2020, <a href="https://github.com/SVAIGBA/WMSeg">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.12035.pdf">A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder</a> (Xipeng Qiu, EMNLP 2020, <a href="https://github.com/acphile/MCCWS">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1903.04190.pdf">Towards Fast and Accurate Neural Chinese Word Segmentation with Multi-Criteria Learning</a> (Weipeng Huang, COLING 2020)</p>

<h4 id="spelling-correction">Spelling Correction</h4>

<p><a href="https://aclanthology.org/C90-2036.pdf">A Spelling Correction Program Based on a Noisy Channel Model</a> (Mark D. Kemighan, COLING 1990)</p>

<p><a href="https://www.sciencedirect.com/science/article/pii/030645739190066U">Context Based Spelling Correction</a> (Eric Mays, 1991)</p>

<h4 id="structured-prediction">Structured Prediction</h4>

<p><a href="https://arxiv.org/abs/2101.05779">Structured Prediction as Translation Between Augmented Natural Languages</a> (Giovanni Paolini, ICLR 2021)</p>

<h4 id="sequence-labeling">Sequence Labeling</h4>

<p><a href="https://arxiv.org/pdf/1603.01354.pdf">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a> (Xuezhe Ma, ACL 2016, <a href="https://github.com/jayavardhanr/End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial">code</a>, <a href="https://jeffchy.github.io/2018/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9AEnd-to-End-Sequence-Labeling-via-Bi-directional-LSTM-CNN-CRF/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1703.06345.pdf">Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks</a> (Zhilin Yang, ICLR 2017, <a href="https://github.com/kimiyoung/transfer">code</a>, <a href="https://blog.csdn.net/Raina_qing/article/details/88830027">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P17-1194">Semi-supervised Multitask Learning for Sequence Labeling</a> (Marek Rei, ACL 2017, <a href="https://zhuanlan.zhihu.com/p/34643000">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1705.00108.pdf">Semi-supervised Sequence Tagging with Bidirectional Language Models</a> (Matthew E. Peters, ACL 2017, <a href="https://zhuanlan.zhihu.com/p/38140507">note</a>)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17123/16075">Empower Sequence Labeling with Task-Aware Neural Language Model</a> (Liyuan Liu, AAAI 2018, <a href="https://github.com/LiyuanLucasLiu/LM-LSTM-CRF">code</a>, <a href="https://zhuanlan.zhihu.com/p/32716990">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/C18-1139.pdf">Contextual String Embeddings for Sequence Labeling</a> (Alan Akbik, COLING 2018, <a href="https://github.com/flairNLP/flair">code</a>, <a href="https://www.cnblogs.com/Arborday/p/9960031.html">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.08676.pdf">Hierarchically-Refined Label Attention Network for Sequence Labeling</a> (Leyang Cui, EMNLP 2019, <a href="https://github.com/Nealcly/BiLSTM-LAN">code</a>, <a href="https://zhuanlan.zhihu.com/p/92672564">note</a>)</p>

<h5 id="part-of-speech-tagging">Part-Of-Speech Tagging</h5>

<p><a href="https://aclanthology.org/N03-1033.pdf">Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network</a> (Kristina Toutanova, NAACL 2013)</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-642-19400-9_14">Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?</a> (Christopher D. Manning, 2011)</p>

<p><a href="http://proceedings.mlr.press/v32/santos14.pdf">Learning Character-level Representations for Part-of-Speech Tagging</a> (C´ıcero Nogueira dos Santos, ICML 2014)</p>

<h5 id="semantic-role-labeling">Semantic Role Labeling</h5>

<p><a href="https://dl.acm.org/doi/pdf/10.3115/980845.980860?download=true">The Berkeley FrameNet Project</a> (Collin F. Baker, ACL 1998)</p>

<p><a href="https://www.aclweb.org/anthology/W05-0620.pdf">Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling</a> (Xavier Carreras, 2005)</p>

<p><a href="https://www.aclweb.org/anthology/P15-1109.pdf">End-to-end Learning of Semantic Role Labeling Using Recurrent Neural Networks</a> (Jie Zhou, ACL 2015, <a href="https://github.com/sanjaymeena/semantic_role_labeling_deep_learning">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P17-1044.pdf">Deep Semantic Role Labeling: What Works and What’s Next</a> (Luheng He, ACL 2017, <a href="https://github.com/luheng/deep_srl">code</a>, <a href="https://www.sohu.com/a/154327575_473283">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1703.04826.pdf">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</a> (Diego Marcheggiani, EMNLP 2017, <a href="https://github.com/diegma/neural-dep-srl">code</a>, <a href="https://zhuanlan.zhihu.com/p/31805975">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/K17-1041.pdf">A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling</a> (Diego Marcheggiani, 2017, <a href="https://github.com/diegma/neural-dep-srl">code</a>)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16725/16025">Deep Semantic Role Labeling with Self-Attention</a> (Zhixing Tan, AAAI 2018, <a href="https://github.com/XMUNLP/Tagger">code</a>, <a href="https://zhuanlan.zhihu.com/p/35179449">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1804.08199.pdf">Linguistically-Informed Self-Attention for Semantic Role Labeling</a> (Emma Strubell, EMNLP 2018, <a href="https://github.com/strubell/LISA">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1810.02245.pdf">A Span Selection Model for Semantic Role Labeling</a> (Hiroki Ouchi, EMNLP 2018, <a href="https://github.com/hiroki13/span-based-srl">code</a>, <a href="https://github.com/BrambleXu/knowledge-graph-learning/issues/136">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.04787.pdf">Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling</a> (Luheng He, ACL 2018, <a href="https://github.com/luheng/lsgn">code</a>, <a href="https://blog.csdn.net/choose_c/article/details/90273333">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1901.05280.pdf">Dependency or Span, End-to-End Uniform Semantic Role Labeling Sentiment Analysis</a> (Zuchao Li, AAAI 2019, <a href="https://github.com/bcmi220/unisrl">code</a>, <a href="https://www.sohu.com/a/295644284_129720">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.02367.pdf">Semantic Role Labeling with Associated Memory Network</a> (Chaoyu Guan, NAACL 2019, <a href="https://github.com/Frozenmad/AMN_SRL">code</a>)</p>

<h4 id="entity-and-relation-extraction">Entity and Relation Extraction</h4>

<h5 id="entity-extraction">Entity Extraction</h5>

<p><a href="https://arxiv.org/pdf/1805.02023.pdf">Chinese NER Using Lattice LSTM</a> (Yue Zhang, ACL 2018)</p>

<p><a href="https://www.aclweb.org/anthology/D19-1396.pdf">Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network</a> (Dianbo Sui, EMNLP 2019)</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/9039685">A Survey on Deep Learning for Named Entity Recognition</a> (Jing Li, TKDE 2020)</p>

<p><a href="https://arxiv.org/pdf/1910.11476.pdf">A Unified MRC Framework for Named Entity Recognition</a> (Xiaoya Li, ACL 2020, <a href="https://github.com/ShannonAI/mrc-for-flat-nested-ner?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2106.00641.pdf">SPANNER: Named Entity Re-/Recognition as Span Prediction</a> (Jinlan Fu, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.00218.pdf">Discontinuous Named Entity Recognition as Maximal Clique Discovery</a> (Yucheng Wang, ACL 2021)</p>

<h5 id="relation-extraction">Relation Extraction</h5>

<p><a href="https://www.jmlr.org/papers/volume3/zelenko03a/zelenko03a.pdf">Kernel Methods for Relation Extraction</a> (Dmitry Zelenko, JMLR 2003)</p>

<p><a href="https://aclanthology.org/D09-1013.pdf">A Rich Feature Vector for Protein-Protein Interaction Extraction from Multiple Corpora</a> (Makoto Miwa, EMNLP 2009)</p>

<p><a href="https://aclanthology.org/P11-1056.pdf">Exploiting Syntactico-Semantic Structures for Relation Extraction</a> (Yee Seng Chan, ACL 2011)</p>

<p><a href="https://aclanthology.org/C14-1220.pdf">Relation Classification via Convolutional Deep Neural Network</a> (Daojian Zeng, COLING 2014)</p>

<p><a href="https://aclanthology.org/P16-2034.pdf">Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</a> (Peng Zhou, ACL 2016)</p>

<p><a href="https://arxiv.org/pdf/1601.00770.pdf">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</a> (Makoto Miwa, ACL 2016)</p>

<p><a href="https://aclanthology.org/P16-1123.pdf">Relation Classification via Multi-Level Attention CNNs</a> (Linlin Wang, ACL 2016)</p>

<p><a href="https://aclanthology.org/P16-1200.pdf">Neural Relation Extraction with Selective Attention over Instances</a> (Yankai Lin, ACL 2016)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/12063">Reinforcement Learning for Relation Classification From Noisy Data</a> (Jun Feng, AAAI 2018)</p>

<p><a href="https://arxiv.org/pdf/1809.10185.pdf">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</a> (Yuhao Zhang, EMNLP 2018)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/4688">A Hierarchical Framework for Relation Extraction with Reinforcement Learning</a> (Ryuichi Takanobu, AAAI 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.07510.pdf">Attention Guided Graph Convolutional Networks for Relation Extraction</a> (Zhijiang Guo, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/2005.06312.pdf">Reasoning with Latent Structure Refinement for Document-Level Relation Extraction</a> (Guoshun Nan, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.13752.pdf">Double Graph Based Reasoning for Document-level Relation Extraction</a> (Shuang Zeng, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2106.03618.pdf">Document-level Relation Extraction as Semantic Segmentation</a> (Ningyu Zhang, IJCAI 2021)</p>

<p><a href="https://arxiv.org/pdf/2104.07650.pdf">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction</a> (Xiang Chen, WWW 2022)</p>

<h5 id="joint-entity-and-relation-extraction">Joint Entity and Relation Extraction</h5>

<p><a href="https://aclanthology.org/P14-1038.pdf">Incremental Joint Extraction of Entity Mentions and Relations</a> (Qi Li, ACL 2014)</p>

<p><a href="https://aclanthology.org/D14-1200.pdf">Modeling Joint Entity and Relation Extraction with Table Representation</a> (Makoto Miwa, EMNLP 2014)</p>

<p><a href="https://arxiv.org/pdf/1706.05075.pdf">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</a> (Songcong Zheng, 2017)</p>

<p><a href="https://aclanthology.org/P19-1136.pdf">GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction</a> (Tsu-Jui Fu, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1905.05529.pdf">Entity-Relation Extraction as Multi-turn Question Answering</a> (Xiaoya Li, ACL 2019)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6374">Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction</a> (Tapas Nayak, AAAI 2020)</p>

<p><a href="https://arxiv.org/pdf/1909.07755.pdf">Span-based Joint Entity and Relation Extraction with Transformer Pre-training</a> (Markus Eberts, ECAI 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.03851.pdf">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</a> (Jue Wang, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.13415.pdf">TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</a> (Yucheng Wang, COLING 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.12812.pdf">A Frustratingly Easy Approach for Entity and Relation Extraction</a> (Zexuan Zhong, NAACL 2021)</p>

<h4 id="dependency-parsing">Dependency Parsing</h4>

<p><a href="https://pdfs.semanticscholar.org/f0e1/883cf9d1b3c911125f46359f908557fc5827.pdf">Statistical Dependency Analysis with Support Vector machines</a> (Hiroyasu Yamada, 2003, <a href="https://github.com/rohit-jain/parzer">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/C12-1059">A Dynamic Oracle for Arc-Eager Dependency Parsing</a> (Yoav Goldberg, COLING 2012, <a href="https://github.com/dpressel/arcs-py">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/Q13-1033.pdf">Training Deterministic Parsers with Non-Deterministic Oracles</a> (Yoav Goldberg, TACL 2013, <a href="https://github.com/dpressel/arcs-py">code</a>)</p>

<p><a href="https://cs.stanford.edu/~danqi/papers/emnlp2014.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a> (Danqi Chen, EMNLP 2014, <a href="https://github.com/khenrix/stanford_nn_parser">code</a>, <a href="https://nocater.github.io/2018/11/13/%E8%AE%BA%E6%96%87-A-Fast-and-Accurate-Dependency-Parserusing-Neural-Networks/">note</a>)</p>

<p><a href="https://aclweb.org/anthology/D15-1162">An Improved Non-monotonic Transition System for Dependency Parsing</a> (Matthew Honnibal, EMNLP 2015)</p>

<p><a href="https://aclweb.org/anthology/Q16-1023">Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations</a> (Eliyahu Kiperwasser, TACL 2016, <a href="https://github.com/elikip/bist-parser">code</a>, <a href="http://fancyerii.github.io/books/nndepparser/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1611.01734.pdf">Deep Biaffine Attention for Neural Dependency Parsing</a> (Timothy Dozat, ICLR 2017, <a href="https://github.com/yzhangcs/parser">code</a>, <a href="https://www.hankcs.com/nlp/parsing/deep-biaffine-attention-for-neural-dependency-parsing.html">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1704.06855.pdf">Deep Multitask Learning for Semantic Dependency Parsing</a> (Hao Peng, ACL 2017, <a href="https://github.com/Noahs-ARK/NeurboParser">code</a>, <a href="https://chao1224.gitbooks.io/running-paper/content/nlp/acl/acl2017/deep-multitask-learning-for-semantic-dependency-parsing.html">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-2077.pdf">Simpler but More Accurate Semantic Dependency Parsing</a> (Timothy Dozat, ACL 2018, <a href="https://github.com/tdozat/Parser-v3">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.01239.pdf">Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies</a> (Shuhei Kurita, ACL 2019)</p>

<h4 id="sentiment-analysis">Sentiment Analysis</h4>

<h5 id="overview">Overview</h5>

<p><a href="https://www.cse.iitb.ac.in/~pb/cs626-449-2009/prev-years-other-things-nlp/sentiment-analysis-opinion-mining-pang-lee-omsa-published.pdf">Opinion Mining and Sentiment Analysis</a> (Bo Pang, 2008)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.244.9480&amp;rep=rep1&amp;type=pdf">Sentiment Analysis and Opinion Mining</a> (Bing Liu, 2012)</p>

<h5 id="dataset">Dataset</h5>

<p><a href="http://www.aclweb.org/anthology/S14-2004">SemEval-2014 Task 4: Aspect Based Sentiment Analysis</a> (Maria Pontiki, SemEval 2014, <a href="http://alt.qcri.org/semeval2014/task4/">code</a>, <a href="https://zhuanlan.zhihu.com/p/59494279">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/S15-2082">SemEval-2015 Task 12: Aspect Based Sentiment Analysis</a> (Maria Pontiki, SemEval 2015, <a href="http://alt.qcri.org/semeval2015/task12/">code</a>, <a href="https://zhuanlan.zhihu.com/p/59791999">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/S16-1002">SemEval-2016 Task 5: Aspect Based Sentiment Analysis</a> (Maria Pontiki, SemEval 2016, <a href="http://alt.qcri.org/semeval2016/task5/">code</a>)</p>

<h5 id="sentiment-lexicon">Sentiment Lexicon</h5>

<p><a href="http://www.aclweb.org/anthology/C14-1018">Building Large-Scale Twitter-Specific Sentiment Lexicon: A Representation Learning Approach</a> (Duyu Tang, COLING 2014)</p>

<p><a href="http://www.aclweb.org/anthology/D16-1057">Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora</a> (William L. Hamilton, EMNLP 2016)</p>

<h5 id="sentiment-embedding">Sentiment Embedding</h5>

<p><a href="http://www.anthology.aclweb.org/P/P14/P14-1146.pdf">Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification</a> (Duyu Tang, ACL 2014, <a href="https://zhuanlan.zhihu.com/p/24217324">note</a>)</p>

<p><a href="https://ieeexplore.ieee.org/document/7296633">Sentiment Embeddings with Applications to Sentiment Analysis</a> (Duyu Tang, TKDE 2015)</p>

<p><a href="https://aclanthology.org/D17-1056.pdf">Refining Word Embeddings for Sentiment Analysis</a> (Liang-Chih Yu, EMNLP 2017)</p>

<p><a href="http://sentic.net/senticnet-5.pdf">SenticNet 5: Discovering Conceptual Primitives for Sentiment Analysis by Means of Context Embeddings</a> (Erik Cambria, AAAI 2018, <a href="http://sentic.net/downloads/">code1</a>, <a href="https://github.com/yurimalheiros/senticnetapi">code2</a>)</p>

<p><a href="https://aclanthology.org/W18-6243.pdf">Emo2Vec: Learning Generalized Emotion Representation by Multi-task Training</a> (Peng Xu, EMNLP 2018 Workshop)</p>

<p><a href="https://aclanthology.org/C18-1081.pdf">Learning Emotion-enriched Word Representations</a> (Ameeta Agrawal, COLING 2018)</p>

<p><a href="https://aclanthology.org/2021.acl-long.184.pdf">Distributed Representations of Emotion Categories in Emotion Space</a> (Xiangyu Wang, ACL 2021)</p>

<h5 id="sentiment-classification">Sentiment Classification</h5>

<p><a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf">Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a> (Richard Socher, EMNLP 2013)</p>

<p><a href="https://www.aclweb.org/anthology/C14-1008.pdf">Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts</a> (C´ıcero Nogueira dos Santos, COLING 2014)</p>

<p><a href="https://www.aclweb.org/anthology/D15-1167.pdf">Document Modeling with Gated Recurrent Neural Network for Sentiment Classification</a> (Duyu Tang, EMNLP 2015)</p>

<p><a href="https://arxiv.org/pdf/1610.04989.pdf">Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</a> (Jiacheng Xu, EMNLP 2016)</p>

<p><a href="https://www.aclweb.org/anthology/D16-1171.pdf">Neural Sentiment Classification with User and Product Attention</a> (Huimin Chen, EMNLP 2016)</p>

<p><a href="https://www.aclweb.org/anthology/C16-1229.pdf">Combination of Convolutional and Recurrent Neural Network for Sentiment Analysis of Short Texts</a> (Xingyou Wang, COLING 2016)</p>

<p><a href="https://www.aclweb.org/anthology/D17-1048.pdf">A Cognition Based Attention Model for Sentiment Analysis</a> (Yunfei Long, EMNLP 2017)</p>

<p><a href="https://arxiv.org/pdf/1801.07861.pdf">Improving Review Representations with User Attention and Product Attention for Sentiment Classification</a> (Zhen Wu, AAAI 2018)</p>

<p><a href="https://pdfs.semanticscholar.org/e82d/6ac78f83ceca584ed56f6c5591e964bf2406.pdf">SNNN: Promoting Word Sentiment and Negation in Neural Sentiment Classification</a> (Qinmin Hu, AAAI 2018)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1235.pdf">A Helping Hand: Transfer Learning for Deep Sentiment Analysis</a> (Xin Dong, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1806.05507.pdf">Cold-Start Aware User and Product Attention for Sentiment Classification</a> (Reinald Kim Amplayo, ACL 2018)</p>

<p><a href="https://www.aclweb.org/anthology/C18-1074.pdf">A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis</a> (Yicheng Zou, COLING 2018)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358138">Neural Review Rating Prediction with User and Product Memory</a> (Zhiguan Yuan, CIKM 2019)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3357384.3357973">Sentiment Lexicon Enhanced Neural Sentiment Classification</a> (Chuhan Wu, CIKM 2019)</p>

<h5 id="opinion-target-extraction">Opinion Target Extraction</h5>

<p><a href="https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf">Mining and Summarizing Customer Reviews</a> (Minqing Hu, KDD 2004, <a href="https://zhuanlan.zhihu.com/p/76436724">note</a>)</p>

<p><a href="http://turing.cs.washington.edu/papers/emnlp05_opine.pdf">Extracting Product Features and Opinions from Reviews</a> (Ana-Maria Popescu, EMNLP 2005)</p>

<p><a href="http://ivan-titov.org/papers/www08.pdf">Modeling Online Reviews with Multi-grain Topic Models</a> (Ivan Titov, WWW 2008, <a href="https://github.com/m-ochi/mglda">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/D09-1159">Phrase Dependency Parsing for Opinion Mining</a> (Yuanbin Wu, EMNLP 2009)</p>

<p><a href="http://people.cs.pitt.edu/~huynv/research/aspect-sentiment/A%20novel%20lexicalized%20HMM-based%20learning%20framework%20for%20web%20opinion%20mining.pdf">A Novel Lexicalized HMM-based Learning Framework for Web Opinion Mining</a> (Wei Jin, ICML 2009)</p>

<p><a href="https://pdfs.semanticscholar.org/1256/c05bd50a80bb0a223ca94674c71fd61fad5a.pdf">Structure-Aware Review Mining and Summarization</a> (Fangtao Li, COLING 2010, <a href="https://www.jianshu.com/p/b3ccdf21fef0">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/C10-2090">Opinion Target Extraction in Chinese News Comments</a> (Tengfei Ma, COLING 2010)</p>

<p><a href="https://www.aclweb.org/anthology/D10-1101">Extracting Opinion Targets in a Single- and Cross-Domain Setting</a> (Niklas Jakob, EMNLP 2010)</p>

<p><a href="https://web.science.mq.edu.au/~rdale/transfer/CL/10-010.pdf">Opinion Word Expansion and Target Extraction through Double Propagation</a> (Guang Qiu, CL 2011)</p>

<p><a href="http://www.nlpr.ia.ac.cn/cip/ZhaoJunPublications/paper/EMNLP2012.LK.pdf">Opinion Target Extraction Using Word-Based Translation Model</a> (Kang Liu, EMNLP 2012)</p>

<p><a href="https://pdfs.semanticscholar.org/9751/81c84a0991bb69f5af825e2019080d22cfcd.pdf">Opinion Target Extraction Using Partially-Supervised Word Alignment Model</a> (Kang Liu, IJCAI 2013)</p>

<p><a href="https://www.aclweb.org/anthology/D13-1172.pdf">Exploiting Domain Knowledge in Aspect Extraction</a> (Zhiyuan Chen, EMNLP 2013)</p>

<p><a href="http://www.aclweb.org/anthology/D16-1059">Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis</a> (Wenya Wang, EMNLP 2016, <a href="https://github.com/happywwy/Recursive-Neural-Conditional-Random-Field">code</a>, <a href="https://www.jianshu.com/p/419cee7f2814">note</a>)</p>

<p><a href="https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11973/12051">Improving Opinion Aspect Extraction Using Semantic Similarity and Aspect Associations</a> (Qian Liu, AAAI 2016, <a href="https://blog.csdn.net/qifeiyang112358/article/details/82849248">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1605.07843.pdf">Unsupervised word and dependency path embeddings for aspect term extraction</a> (Yichun Yin, IJCAI 2016, <a href="https://zhuanlan.zhihu.com/p/27246419">note</a>)</p>

<p><a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14441/14256">Coupled Multi-Layer Attentions for Co-Extraction of Aspect and Opinion Terms</a> (Wenya Wang, AAAI 2017, <a href="https://github.com/happywwy/Coupled-Multi-layer-Attentions">code</a>, <a href="https://zhuanlan.zhihu.com/p/33088676">note</a>)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14865/14130">Recurrent Neural Networks with Auxiliary Labels for Cross-Domain Opinion Target Extraction</a> (Ying Ding, AAAI 2017)</p>

<p><a href="https://arxiv.org/pdf/1702.01776.pdf">Multi-task Memory Networks for Category-specific Aspect and Opinion Terms Co-extraction</a> (Wenya Wang, 2017)</p>

<p><a href="http://www.aclweb.org/anthology/P17-1036">An Unsupervised Neural Attention Model for Aspect Extraction</a> (Ruidan He, ACL 2017, <a href="https://github.com/ruidan/Unsupervised-Aspect-Extraction">code</a>, <a href="https://www.jianshu.com/p/241cb238e21f">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P17-2023">Lifelong Learning CRF for Supervised Aspect Extraction</a> (Lei Shu, ACL 2017)</p>

<p><a href="http://www.aclweb.org/anthology/D17-1310">Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction</a> (Xin Li, EMNLP 2017, <a href="https://zhuanlan.zhihu.com/p/51632476">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.00760.pdf">Aspect Term Extraction with History Attention and Selective Transformation</a> (Xin Li, IJCAI 2018, <a href="https://github.com/lixin4ever/HAST">code</a>, <a href="https://zhuanlan.zhihu.com/p/51189078">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-2094">Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction</a> (Hu Xu, ACL 2018, <a href="https://github.com/howardhsu/DE-CNN">code</a>, <a href="https://zhuanlan.zhihu.com/p/72092287">note</a>)</p>

<p><a href="https://aclweb.org/anthology/D18-1384">ExtRA: Extracting Prominent Review Aspects from Customer Feedback</a> (Zhiyi Luo, EMNLP 2018, <a href="https://zhuanlan.zhihu.com/p/51767759">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/N19-1259">Target-oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling</a> (Zhifang Fan, NAACL 2019, <a href="https://github.com/NJUNLP/TOWE">code</a>, <a href="https://www.linkresearcher.com/theses/761656d7-5d1e-4d54-a723-361ee1eaa113">note</a>)</p>

<h5 id="aspect-based-sentiment-classification">Aspect-Based Sentiment Classification</h5>

<p><a href="http://www.anthology.aclweb.org/P/P11/P11-1016.pdf">Target-dependent twitter sentiment classification</a> (Long Jiang, ACL 2011)</p>

<p><a href="http://www.aclweb.org/anthology/P14-2009">Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification</a> (Li Dong, ACL 2014, <a href="https://blog.csdn.net/VictoriaW/article/details/51943563">note</a>)</p>

<p><a href="http://www.ijcai.org/Proceedings/15/Papers/194.pdf">Target-dependent twitter sentiment classification with rich automatic features</a> (Duy-Tin Vo, IJCAI 2015, <a href="https://github.com/duytinvo/ijcai2015">code</a>)</p>

<p><a href="http://www.aclweb.org/anthology/C16-1311">Effective LSTMs for Target-Dependent Sentiment Classification</a> (Duyu Tang, COLING 2016, <a href="https://github.com/scaufengyang/TD-LSTM">code</a>, <a href="https://zhuanlan.zhihu.com/p/33986102">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/D16-1058">Attention-based LSTM for Aspect-level Sentiment Classification</a> (Yequan Wang, EMNLP 2016, <a href="https://github.com/scaufengyang/TD-LSTM">code</a>, <a href="https://zhuanlan.zhihu.com/p/34005136">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/D16-1021">Aspect Level Sentiment Classification with Deep Memory Network</a> (Duyu Tang, EMNLP 2016, <a href="https://github.com/pcgreat/mem_absa">code</a>, <a href="https://zhuanlan.zhihu.com/p/34033477">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/D16-1103">A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis</a> (Sebastian Ruder, EMNLP 2016, <a href="https://zhuanlan.zhihu.com/p/23477057">note</a>)</p>

<p><a href="https://www.ijcai.org/proceedings/2017/0568.pdf">Interactive Attention Networks for Aspect-Level Sentiment Classification</a> (Dehong Ma, IJCAI 2017, <a href="https://github.com/lpq29743/IAN">code</a>, <a href="https://zhuanlan.zhihu.com/p/34041012">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/D17-1047">Recurrent Attention Network on Memory for Aspect Sentiment Analysis</a> (Peng Chen, EMNLP 2017, <a href="https://github.com/lpq29743/RAM">code</a>, <a href="https://zhuanlan.zhihu.com/p/34043504">note</a>)</p>

<p><a href="http://leoncrashcode.github.io/Documents/EACL2017.pdf">Attention Modeling for Targeted Sentiment</a> (Jiangming Liu, EACL 2017, <a href="https://github.com/vipzgy/AttentionTargetClassifier">code</a>)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16541/16152">Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM</a> (Yukun Ma, AAAI 2018, <a href="https://zhuanlan.zhihu.com/p/53251543">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/N18-2043">Modeling Inter-Aspect Dependencies for Aspect-Based Sentiment Analysis</a> (Devamanyu Hazarika, NAACL 2018, <a href="https://github.com/xgy221/lstm-inter-aspect">code</a>, <a href="https://zhuanlan.zhihu.com/p/54327441">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/D18-1377">IARM: Inter-aspect relation modeling with memory networks in aspect-based sentiment analysis</a> (Navonil Majumder, EMNLP 2018, <a href="https://github.com/SenticNet/IARM">code</a>)</p>

<p><a href="https://dl.acm.org/citation.cfm?doid=3178876.3186001">Content Attention Model for Aspect Based Sentiment Analysis</a> (Qiao Liu, WWW 2018, <a href="https://github.com/uestcnlp/Cabasc">code1</a>, <a href="https://github.com/songyouwei/ABSA-PyTorch">code2</a>, <a href="https://zhuanlan.zhihu.com/p/61575551">note</a>)</p>

<p><a href="https://dl.acm.org/citation.cfm?id=3209978.3210115">Convolution-based Memory Network for Aspect-based Sentiment Analysis</a> (Chuang Fan, SIGIR 2018)</p>

<p><a href="http://www.aclweb.org/anthology/P18-1234">Aspect Based Sentiment Analysis with Gated Convolutional Networks</a> (Wei Xue, ACL 2018, <a href="https://github.com/wxue004cs/GCAE">code</a>, <a href="https://zhuanlan.zhihu.com/p/50284374">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-2092.pdf">Exploiting Document Knowledge for Aspect-level Sentiment Classification</a> (Ruidan He, ACL 2018, <a href="https://github.com/ruidan/Aspect-level-sentiment">code</a>, <a href="https://zhuanlan.zhihu.com/p/52123748">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1087">Transformation Networks for Target-Oriented Sentiment Classification</a> (Xin Li, ACL 2018, <a href="https://github.com/lixin4ever/TNet">code</a>, <a href="https://zhuanlan.zhihu.com/p/61586882">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1380.pdf">Multi-grained Attention Network for Aspect-Level Sentiment Classification</a> (Feifan Fan, EMNLP 2018, <a href="https://zhuanlan.zhihu.com/p/64301255">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/C18-1066.pdf">A Position-aware Bidirectional Attention Network for Aspect-level Sentiment Analysis</a> (Shuqin Gu, COLING 2018, <a href="https://github.com/hiyouga/PBAN-PyTorch">code</a>, <a href="https://zhuanlan.zhihu.com/p/62696026">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1902.09314.pdf">Attentional Encoder Network for Targeted Sentiment Classification</a> (Youwei Song, 2019, <a href="https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/aen.py">code</a>, <a href="https://zhuanlan.zhihu.com/p/68858093">note</a>)</p>

<p><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4635">A Human-Like Semantic Cognition Network for Aspect-Level Sentiment Classification</a> (Zeyang Lei, AAAI 2019, <a href="https://github.com/eeGuoJun/AAAI2019_HSCN">code</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/2019/0751.pdf">Adapting BERT for Target-Oriented Multimodal Sentiment Classification</a> (Jianfei Yu, IJCAI 2019, <a href="https://github.com/jefferyYu/TomBERT">code</a>, <a href="https://zhuanlan.zhihu.com/p/239892083">note</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/2019/0707.pdf">Deep Mask Memory Networks with Semantic Dependency and Context Moment for Aspect-based Sentiment Analysis</a> (Peiqin Lin, IJCAI 2019, <a href="https://github.com/lpq29743/DMMN-SDCM">code</a>, <a href="https://zhuanlan.zhihu.com/p/150462314">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1904.02232.pdf">BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</a> (Hu Xu, NAACL 2019, <a href="https://github.com/howardhsu/BERT-for-RRC-ABSA">code</a>, <a href="https://zhuanlan.zhihu.com/p/72092287">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1903.09588.pdf">Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence</a> (Chi Sun, NAACL 2019, <a href="https://github.com/HSLCY/ABSA-BERT-pair">code</a>, <a href="https://zhuanlan.zhihu.com/p/69786643">note</a>)</p>

<p><a href="https://www.researchgate.net/profile/Samuel_Mensah8/publication/342238197_Replicate_Walk_and_Stop_on_Syntax_An_Effective_Neural_Network_Model_for_Aspect-Level_Sentiment_Classification/links/5f0ceff392851c38a51ccd83/Replicate-Walk-and-Stop-on-Syntax-An-Effective-Neural-Network-Model-for-Aspect-Level-Sentiment-Classification.pdf">Replicate, Walk, and Stop on Syntax: an Effective Neural Network Model for Aspect-Level Sentiment Classification</a> (Yaowei Zheng, AAAI 2020, <a href="https://github.com/hiyouga/RepWalk">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.451.pdf">Inducing Target-Specific Latent Structures for Aspect Sentiment Classification</a> (Chenhua Chen, EMNLP 2020, <a href="https://zhuanlan.zhihu.com/p/311246774">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2009.07964.pdf">Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis</a> (Xiaoyu Xing, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2104.11681.pdf">Interventional Aspect-Based Sentiment Analysis</a> (Zhen Bi, 2021)</p>

<h5 id="aspect-based-sentiment-analysis">Aspect-Based Sentiment Analysis</h5>

<p><a href="http://ivan-titov.org/papers/acl08.pdf">A Joint Model of Text and Aspect Ratings for Sentiment Summarization</a> (Ivan Titov, ACL 2008)</p>

<p><a href="https://pdfs.semanticscholar.org/6047/235275b2b8d414b8ac472fd19f2a1a6144b6.pdf">Bidirectional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model</a> (Roman Klinger, ACL 2013)</p>

<p><a href="https://www.aclweb.org/anthology/P13-1161">Joint Inference for Fine-grained Opinion Extraction</a> (Bishan Yang, ACL 2013)</p>

<p><a href="https://www.aclweb.org/anthology/D13-1171">Open Domain Targeted Sentiment</a> (Margaret Mitchell, EMNLP 2013)</p>

<p><a href="https://www.aclweb.org/anthology/Q14-1039.pdf">Joint Modeling of Opinion Expression Extraction and Attribute Classification</a> (Bishan Yang, TACL 2014)</p>

<p><a href="https://www.aclweb.org/anthology/D15-1073">Neural Networks for Open Domain Targeted Sentiment</a> (Meishan Zhang, EMNLP 2015, <a href="https://github.com/SUTDNLP/OpenTargetedSentiment">code</a>)</p>

<p><a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14931/14137">Learning Latent Sentiment Scopes for Entity-Level Sentiment Analysis</a> (Hao Li, AAAI 2017)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1504">Joint Learning for Targeted Sentiment Analysis</a>  (Dehong Ma, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1811.05082.pdf">A Unified Model for Opinion Target Extraction and Target Sentiment Prediction</a> (Xin Li, AAAI 2019, <a href="https://github.com/lixin4ever/E2E-TBSA">code</a>, <a href="https://zhuanlan.zhihu.com/p/52705613">note</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/2019/0762.pdf">A Span-based Joint Model for Opinion Target Extraction and Target Sentiment Classification</a> (Yan Zhou, IJCAI 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.06906.pdf">An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis</a> (Ruidan He, ACL 2019, <a href="https://github.com/ruidan/IMN-E2E-ABSA">code</a>, <a href="https://blog.csdn.net/BeforeEasy/article/details/104219019">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.01794.pdf">DOER: Dual Cross-Shared RNN for Aspect Term-Polarity Co-Extraction</a> (Huaishao Luo, ACL 2019, <a href="https://github.com/ArrowLuo/DOER">code</a>, <a href="https://blog.csdn.net/weixin_44740082/article/details/103281743">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.03820.pdf">Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification</a> (Minghao Hu, ACL 2019, <a href="https://github.com/huminghao16/SpanABSA">code</a>, <a href="https://zhuanlan.zhihu.com/p/144393570">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.382.pdf">A Shared-Private Representation Model with Coarse-to-Fine Extraction for Target Sentiment Analysis</a> (Peiqin Lin, EMNLP 2020 Findings, <a href="https://github.com/lpq29743/SPRM">code</a>, <a href="https://zhuanlan.zhihu.com/p/268419578">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2011.00169.pdf">Understanding Pre-trained BERT for Aspect-based Sentiment Analysis</a> (Hu Xu, COLING 2020, <a href="https://github.com/howardhsu/BERT-for-RRC-ABSA">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2106.04300.pdf">A Unified Generative Framework for Aspect-Based Sentiment Analysis</a> (Hang Yan, ACL 2021)</p>

<h5 id="emotion-cause-detection">Emotion Cause Detection</h5>

<p><a href="https://www.researchgate.net/profile/Chu-Ren_Huang/publication/220746716_Emotion_Cause_Events_Corpus_Construction_and_Analysis/links/0912f508ff080541ac000000/Emotion-Cause-Events-Corpus-Construction-a">Emotion Cause Events: Corpus Construction and Analysis</a> (Sophia Yat Mei Lee, LREC 2010)</p>

<p><a href="https://dl.acm.org/doi/pdf/10.5555/1860631.1860637?download=true">A Text-driven Rule-based System for Emotion Cause Detection</a> (Sophia Yat Mei Lee, NAACL 2010)</p>

<p><a href="https://dl.acm.org/doi/pdf/10.5555/1873781.1873802?download=true">Emotion Cause Detection with Linguistic Constructions</a> (Ying Chen, COLING 2010)</p>

<p><a href="https://www.aclweb.org/anthology/W11-1720.pdf">EMOCause: An Easy-adaptable Approach to Emotion Cause Contexts</a> (Irene Russo, 2011)</p>

<p><a href="https://www.sciencedirect.com/science/article/pii/S0957417413006945">Text-based Emotion Classification Using Emotion Cause Extraction</a> (Weiyuan Li, 2013)</p>

<p><a href="https://www.aclweb.org/anthology/D16-1170.pdf">Event-Driven Emotion Cause Extraction with Corpus Construction</a> (Lin Gui, EMNLP 2016)</p>

<p><a href="https://arxiv.org/pdf/1708.05482.pdf">A Question Answering Approach to Emotion Cause Extraction</a> (Liu Gui, EMNLP 2017)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1506.pdf">A Co-Attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness</a> (Xiangju Li, EMNLP 2018)</p>

<p><a href="https://www.aclweb.org/anthology/C18-1114.pdf">Who Feels What and Why? Annotation of a Literature Corpus with Semantic Roles of Emotions</a> (Evgeny Kim, COLING 2018)</p>

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705119301273">Context-Aware Emotion Cause Analysis with Multi-Attention-Based Neural Network</a> (Xiangju Li, KBS 2019)</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/8598785">Multiple Level Hierarchical Network-Based Clause Selection for Emotion Cause Extraction</a> (Xinyi Yu, IEEE Access 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.01230.pdf">From Independent Prediction to Reordered Prediction: Integrating Relative Position and Global Label Information to Emotion Cause Identification</a> (Zixiang Ding, AAAI 2019, <a href="https://github.com/NUSTM/PAEDGL">code</a>, <a href="https://zhuanlan.zhihu.com/p/240460324">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.01236.pdf">RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause Extraction</a> (Rui Xia, IJCAI 2019, <a href="https://github.com/NUSTM/RTHN">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/D19-1563.pdf">A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis</a> (Chuang Fan, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/2106.03518.pdf">Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction</a> (Hanqi Yan, ACL 2021, <a href="https://github.com/hanqi-qi/Position-Bias-Mitigation-in-Emotion-Cause-Analysis">code</a>)</p>

<h5 id="emotion-cause-analysis">Emotion Cause Analysis</h5>

<p><a href="https://www.aclweb.org/anthology/D18-1066">Joint Learning for Emotion Classification and Emotion Cause Detection</a> (Ying Chen, EMNLP 2018)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1096">Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts</a> (Rui Xia, ACL 2019, <a href="https://github.com/NUSTM/ECPE">code</a>, <a href="https://mikito.mythsman.com/post/5d2bf2685ed28235d7573179/">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.288.pdf">ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction</a> (Zixiang Ding, ACL 2020, <a href="https://github.com/NUSTM/ECPE-2D">code</a>)</p>

<h4 id="dialogue-system">Dialogue System</h4>

<p><a href="https://arxiv.org/pdf/1711.01731.pdf">A Survey on Dialogue Systems: Recent Advances and New Frontiers</a> (Hongshen Chen, 2017, <a href="https://cloud.tencent.com/developer/article/1337267">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/P/P17/P17-2079.pdf">AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine</a> (Minghui Qiu, ACL 2017, <a href="https://blog.csdn.net/u011239443/article/details/83829265">note</a>)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3209978.3210183">Neural Approaches to Conversational AI</a> (Jianfeng Gao, SIGIR 2018)</p>

<p><a href="https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00368">The Design and Implementation of XiaoIce, an Empathetic Social Chatbot</a> (Li Zhou, CL 2020)</p>

<p><a href="https://arxiv.org/pdf/1905.05709.pdf">Challenges in Building Intelligent Open-domain Dialog Systems</a> (Minlie Huang, TIS 2020)</p>

<p><a href="https://arxiv.org/pdf/2001.09977.pdf">Towards a Human-like Open-Domain Chatbot</a> (Daniel Adiwardana, 2020, <a href="https://github.com/rustyoldrake/Character-Cartridges-Embodied-Identity?utm_source=catalyzex.com">code</a>)</p>

<h5 id="dataset-1">Dataset</h5>

<p><a href="https://arxiv.org/pdf/1810.00278.pdf">MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling</a> (Paweł Budzianowski, EMNLP 2018, <a href="https://github.com/budzianowski/multiwoz">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.08205.pdf">Towards Exploiting Background Knowledge for Building Conversation Systems</a> (Mikita Moghe, EMNLP 2018, <a href="https://github.com/nikitacs16/Holl-E?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.01984.pdf">Training Millions of Personalized Dialogue Agents</a> (Pierre-Emmanuel Mazare, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1907.01669.pdf">MultiWOZ 2.1: Multi-Domain Dialogue State Corrections and State Tracking Baselines</a> (Mihail Eric, 2019, <a href="https://github.com/budzianowski/multiwoz?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1811.01241.pdf">Wizard of Wikipedia: Knowledge-Powered Conversational Agents</a> (Emily Dinan, ICLR 2019)</p>

<p><a href="https://arxiv.org/pdf/1810.02508.pdf">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</a> (Soujanya Poria, ACL 2019, <a href="https://github.com/declare-lab/MELD">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.01388.pdf">How to Build User Simulators to Train RL-based Dialog Systems</a> (Weiyan Shi, EMNLP 2019, <a href="https://github.com/wyshi/user-simulator">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.05855.pdf">Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset</a> (Abhinav Rastogi, AAAI 2020, <a href="https://github.com/google-research-datasets/dstc8-schema-guided-dialogue">code</a>, <a href="https://blog.csdn.net/weixin_44385551/article/details/103098092">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2002.11893.pdf">CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset</a> (Qi Zhu, TACL 2020, <a href="https://github.com/thu-coai/CrossWOZ">code</a>, <a href="https://zhuanlan.zhihu.com/p/115366490">note</a>)</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-030-60450-9_8">A Large-Scale Chinese Short-Text Conversation Dataset</a> (Yida Wang, NLPCC 2020, <a href="https://github.com/thu-coai/CDial-GPT">code</a>)</p>

<h5 id="dialogue-state-tracking">Dialogue State Tracking</h5>

<p><a href="http://www.aclweb.org/anthology/W14-4337">The Second Dialog State Tracking Challenge</a> (Matthew Henderson, SIGDAIL 2014, <a href="http://camdial.org/~mh521/dstc/">code</a>)</p>

<p><a href="http://www.aclweb.org/anthology/W14-4340">Word-Based Dialog State Tracking with Recurrent Neural Networks</a> (Matthew Henderson, SIGDAIL 2014)</p>

<p><a href="https://ai.google/research/pubs/pub44018">Machine Learning for Dialog State Tracking: A Review</a> (Matthew Henderson, 2015)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/williams2016dstc_overview-1.pdf">The Dialog State Tracking Challenge Series: A Review</a> (Jason D. Williams  2016)</p>

<p><a href="http://www.aclweb.org/anthology/E17-1042">A Network-based End-to-End Trainable Task-oriented Dialogue System</a> (Tsung-Hsien Wen, EACL 2017, <a href="https://github.com/edward-zhu/dialog">code</a>, <a href="https://www.jianshu.com/p/96c8fd2d2876">note</a>)</p>

<p><a href="http://aclweb.org/anthology/P17-1163">Neural Belief Tracker: Data-Driven Dialogue State Tracking</a> (Nikola Mrksic, ACL 2017, <a href="https://github.com/nmrksic/neural-belief-tracker">code</a>, <a href="https://zhuanlan.zhihu.com/p/27470864">note</a>)</p>

<p><a href="http://aclweb.org/anthology/P18-2018">Fully Statistical Neural Belief Tracking</a> (Nikola Mrksic, ACL 2018, <a href="https://github.com/nmrksic/neural-belief-tracker">code</a>)</p>

<p><a href="http://aclweb.org/anthology/P18-1135">Global-Locally Self-Attentive Dialogue State Tracker</a> (Victor Zhong, ACL 2018, <a href="https://github.com/salesforce/glad">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1299.pdf">Towards Universal Dialogue State Tracking</a> (Liliang Ren, EMNLP 2018, <a href="https://github.com/renll/StateNet?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.01946.pdf">Dialog State Tracking: A Neural Reading Comprehension Approach</a> (Shuyang Gao, SIGDIAL 2019)</p>

<p><a href="https://arxiv.org/pdf/1905.08743.pdf">Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems</a> (Chien-Sheng Wu, ACL 2019, <a href="https://github.com/jasonwu0731/trade-dst">code</a>, <a href="https://zhuanlan.zhihu.com/p/72580652">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.07421.pdf">SUMBT: Slot-Utterance Matching for Universal and Scalable Belief Tracking</a> (Hwaran Lee, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.00754.pdf">Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation</a> (Liliang Ren, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1907.00883.pdf">HyST: A Hybrid Approach for Flexible and Accurate Dialogue State Tracking</a> (Rahul Goel, Interspeech 2019)</p>

<p><a href="https://arxiv.org/pdf/1911.03906.pdf">Efficient Dialogue State Tracking by Selectively Overwriting Memory</a> (Sungdong Kim, ACL 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.151.pdf">Parallel Interactive Networks for Multi-Domain Dialogue State Generation</a> (Junfan Chen, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.03386.pdf">Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking</a> (Su Zhu, EMNLP 2020 Findings)</p>

<p><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.95.pdf">GCDST: A Graph-based and Copy-augmented Multi-domain Dialogue State Tracking</a> (Peng Wu, EMNLP 2020 Findings)</p>

<p><a href="https://arxiv.org/pdf/2002.08024.pdf">Non-Autoregressive Dialog State Tracking</a> (Hung Le, ICLR 2020, <a href="https://github.com/henryhungle/NADST">code</a>)</p>

<h5 id="dialogue-act-recognition">Dialogue Act Recognition</h5>

<p><a href="https://www.aclweb.org/anthology/J00-3003.pdf">Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech</a> (Andreas Stolcke, CL 2000)</p>

<p><a href="https://www.aclweb.org/anthology/C16-1189.pdf">Dialogue Act Classification in Domain-Independent Conversations Using a Deep Recurrent Neural Network</a> (Hamed Khanpour, COLING 2016)</p>

<p><a href="https://arxiv.org/pdf/1910.01822.pdf">Multi-level Gated Recurrent Neural Network for Dialog Act Classification</a> (Wei Li, COLING 2016)</p>

<p><a href="https://arxiv.org/pdf/1708.02561.pdf">Neural-based Context Representation Learning for Dialog Act Classification</a> (Daniel Ortega, SIGDIAL 2017)</p>

<p><a href="https://www.aclweb.org/anthology/D17-1231.pdf">Using Context Information for Dialog Act Classification in DNN Framework</a> (Yang Liu, EMNLP 2017)</p>

<p><a href="https://www.aclweb.org/anthology/E17-1041.pdf">A Hierarchical Neural Model for Learning Sequences of Dialogue Acts</a> (Quan Hung Tran, EACL 2017)</p>

<p><a href="https://arxiv.org/pdf/1711.05568.pdf">Dialogue Act Recognition via CRF-Attentive Structured Network</a> (Zheqian Chen, SIGIR 2018)</p>

<p><a href="https://arxiv.org/pdf/1709.04250.pdf">Dialogue Act Sequence Labeling using Hierarchical encoder with CRF</a> (Harshit Kumar, AAAI 2018, <a href="https://github.com/YanWenqiang/HBLSTM-CRF">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.06280.pdf">A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks</a> (Chandrakant Bothe, LREC 2018)</p>

<p><a href="https://arxiv.org/pdf/1805.06242.pdf">Conversational Analysis using Utterance-level Attention-based Bidirectional Recurrent Neural Networks</a> (Chandrakant Bothe, INTERSPEECH 2018)</p>

<p><a href="https://arxiv.org/pdf/1810.09154.pdf">A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification</a> (Ruizhe Li, CONLL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/N19-1373.pdf">Dialogue Act Classification with Context-Aware Self-Attention</a> (Vipul Raheja, NAACL 2019)</p>

<p><a href="https://dl.acm.org/doi/pdf/10.1145/3357384.3358145">Modeling Long-Range Context for Concurrent Dialogue Acts Recognition</a> (Yue Yu, CIKM 2019)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.402.pdf">Towards Emotion-aided Multi-modal Dialogue Act Classification</a> (Tulika Saha, ACL 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.coling-main.372.pdf">Integrating User History into Heterogeneous Graph for Dialogue Act Recognition</a> (Dong Wang, COLING 2020)</p>

<h5 id="dialogue-emotion-recognition">Dialogue Emotion Recognition</h5>

<p><a href="https://ieeexplore.ieee.org/abstract/document/1395974">Toward Detecting Emotions in Spoken Dialogs</a> (Chul Min Lee, 2005)</p>

<p><a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2006/i06_1636.pdf">Real-Life Emotions Detection with Lexical and Paralinguistic Cues on Human-Human Call Center Dialogs</a> (Laurence Devillers, 2006)</p>

<p><a href="https://w.sentic.net/conversational-memory-network.pdf">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos</a> (Devamanyu Hazarika, NAACL 2018)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1280.pdf">ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection</a> (Devamanyu Hazarika, EMNLP 2018, <a href="https://zhuanlan.zhihu.com/p/63506119">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1811.00405.pdf">DialogueRNN: An Attentive RNN for Emotion Detection in Conversations</a> (Navonil Majumder, AAAI 2019, <a href="https://zhuanlan.zhihu.com/p/68497862">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1904.04446.pdf">HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition</a> (Wenxiang Jiao, NAACL 2019)</p>

<p><a href="https://www.ijcai.org/Proceedings/2019/0752.pdf">Modeling both Context- and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations</a> (Dong Zhang, IJCAI 2019)</p>

<p><a href="https://arxiv.org/pdf/1908.11540.pdf">DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation</a> (Deepanway Ghosal, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.10681.pdf">Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations</a> (Peixiang Zhong, EMNLP 2019, <a href="https://github.com/zhongpeixiang/KET">code</a>, <a href="https://zhuanlan.zhihu.com/p/90548422">note</a>)</p>

<h5 id="dialogue-summarization">Dialogue Summarization</h5>

<p><a href="https://www.aclweb.org/anthology/P18-1062.pdf">Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization</a> (Guokan Shang, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1809.05715.pdf">Abstractive Dialogue Summarization with Sentence-Gated Modeling Optimized by Dialogue Acts</a> (Chih-Wen Goo SLT 2018)</p>

<p><a href="https://dl.acm.org/doi/10.1145/3308558.3313619">Abstractive Meeting Summarization via Hierarchical Adaptive Segmental Network Learning</a> (Zhou Zhao, WWW 2019)</p>

<p><a href="https://dl.acm.org/doi/10.1145/3292500.3330683">Automatic Dialogue Summary Generation for Customer Service</a> (Chunyi Liu, KDD 2019)</p>

<p><a href="https://arxiv.org/pdf/1910.01335.pdf">Topic-aware Pointer-Generator Networks for Summarizing Spoken Conversations</a> (Zhengyuan Liu, ASRU 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1210.pdf">Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization</a> (Manling Li, ACL 2019)</p>

<p><a href="https://arxiv.org/abs/2004.02016v4">A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining</a> (Chenguang Zhu, EMNLP 2020, <a href="https://github.com/JudeLee19/HMNet-End-to-End-Abstractive-Summarization-for-Meetings?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2011.00692.pdf">How Domain Terminology Affects Meeting Summarization Performance</a> (Jia Jin Koay, COLING 2020)</p>

<h5 id="task-oriented-dialogue-system">Task-Oriented Dialogue System</h5>

<p><a href="https://arxiv.org/pdf/1804.08217.pdf">Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems</a> (Andrea Madotto, ACL 2018)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1133.pdf">Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures</a> (Wenqiang Lei, ACL 2018, <a href="https://github.com/WING-NUS/sequicity">code</a>, <a href="https://blog.csdn.net/weixin_40533355/article/details/82997788">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1810.10647.pdf">Multi-level Memory for Task Oriented Dialogs</a> (Revanth Reddy, NAACL 2019, <a href="https://zhuanlan.zhihu.com/p/64595503">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1258.pdf">A Working Memory Model for Task-oriented Dialog Response Generation</a> (Xiuyi Chen, ACL 2019, <a href="https://blog.csdn.net/weixin_44487404/article/details/105665796">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1901.04713.pdf">Global-to-local Memory Pointer Networks for Task-Oriented Dialogue</a> (Chien-Sheng Wu, ICLR 2019, <a href="https://github.com/jasonwu0731/GLMP">code</a>, <a href="https://zhuanlan.zhihu.com/p/57535074">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.06762.pdf">Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever</a> (Libo Qin, EMNLP 2019, <a href="https://github.com/yizhen20133868/Retriever-Dialogue">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.05774.pdf">Hello, It’s GPT-2 – How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems</a> (Paweł Budzianowski, 2019)</p>

<p><a href="https://arxiv.org/pdf/2004.06871.pdf">TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue</a> (Chien-Sheng Wu, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.12005.pdf">MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems</a> (Zhaojiang Lin, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.13656.pdf">Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems</a> (Andrea Madotto, EMNLP 2020 Findings)</p>

<h5 id="dialogue-modeling-and-generation">Dialogue Modeling and Generation</h5>

<p><a href="https://www.aclweb.org/anthology/D15-1199.pdf">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</a> (Tsung-Hsien Wen, EMNLP 2015)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11957/12160">Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</a> (Iulian V.Serban, AAAI 2016)</p>

<p><a href="https://arxiv.org/pdf/1510.03055.pdf">A Diversity-Promoting Objective Function for Neural Conversation Models</a> (Jiwei Li, NAACL 2016, <a href="https://zhuanlan.zhihu.com/p/35496909">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1603.08023.pdf">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</a> (Chia-Wei Liu, EMNLP 2016)</p>

<p><a href="https://arxiv.org/pdf/1606.01541.pdf">Deep Reinforcement Learning for Dialogue Generation</a> (Jiwei Li, 2016, <a href="https://zhuanlan.zhihu.com/p/21587758">note</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/10983">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</a> (Iulian Serban, AAAI 2017, <a href="https://github.com/mike-n-7/ADEM?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14471/14267">Mechanism-Aware Neural Machine for Dialogue Response Generation</a> (Ganbin Zhou, AAAI 2017)</p>

<p><a href="https://www.aclweb.org/anthology/P17-2080.pdf">A Conditional Variational Framework for Dialog Generation</a> (Xiaoyu Shen, ACL 2017)</p>

<p><a href="https://www.aclweb.org/anthology/P17-1061.pdf">Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders</a> (Tiancheng Zhao, ACL 2017, <a href="http://www.xuwei.io/2019/04/05/%E3%80%8Alearning-discourse-level-diversity-for-neural-dialog-models-using-conditional-variational-autoencoders%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1701.03185.pdf">Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models</a> (Louis Shao, EMNLP 2017)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/11960">Improving Variational Encoder-Decoders in Dialogue Generation</a> (Xiaoyu Shen, AAAI 2018)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/11321">RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems</a> (Chongyang Tao, AAAI 2018)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3178876.3186077">Hierarchical Variational Memory Network for Dialogue Generation</a> (Hongshen Chen, WWW 2018, <a href="https://github.com/chenhongshen/HVMN">code</a>, <a href="https://blog.csdn.net/qq_38684093/article/details/84038264">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1354.pdf">Variational Autoregressive Decoder for Neural Response Generation</a> (Jiachen Du, EMNLP 2018)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3269206.3271683">Explicit State Tracking with Semi-Supervisionfor Neural Dialogue Generation</a> (Xisen Jin, CIKM 2018, <a href="https://github.com/AuCson/SEDST">code</a>, <a href="https://zhuanlan.zhihu.com/p/62306940">note</a>)</p>

<p><a href="https://papers.nips.cc/paper/2018/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf">Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization</a> (Yizhe Zhang, NeurIPS 2018)</p>

<p><a href="https://arxiv.org/pdf/1902.11205.pdf">Jointly Optimizing Diversity and Relevance in Neural Response Generation</a> (Xiang Gao, NAACL 2019, <a href="https://github.com/golsun/SpaceFusion?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.03520.pdf">Domain Adaptive Dialog Generation via Meta Learning</a> (Kun Qian, ACL 2019, <a href="https://github.com/qbetterk/DAML">code</a>, <a href="https://liusih.github.io/2019/09/09/Domain%20Adaptive%20Dialog%20Generation%20via%20Meta%20Learning/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.00414.pdf">Pretraining Methods for Dialog Context Representation Learning</a> (Shikib Mehri, ACL 2019, <a href="https://zhuanlan.zhihu.com/p/82001834">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.08854.pdf">Incremental Transformer with Deliberation Decoder for Document Grounded Conversations</a> (Zekang Li, ACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1567.pdf">Improving Neural Conversational Models with Entropy-Based Data Filtering</a> (Richard Csaky, ACL 2019, <a href="https://github.com/ricsinaruto/dialog-eval">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.05339.pdf">ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation</a> (Hainan Zhang, ACL 2019, <a href="https://github.com/zhanghainan/ReCoSa">code</a>, <a href="https://zhuanlan.zhihu.com/p/74229505">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.12866.pdf">Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention</a> (Wenhu Chen, ACL 2019, <a href="https://github.com/wenhuchen/HDSA-Dialog">code</a>, <a href="https://zhuanlan.zhihu.com/p/82460398">note</a>)</p>

<p><a href="https://ieeexplore.ieee.org/document/9020173">Hierarchical Prediction and Adversarial Learning For Conditional Response Generation</a> (Yanran Li, TKDE 2020)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6400">Hierarchical Reinforcement Learning for Open-Domain Dialog</a> (Abdelrhman Saleh, AAAI 2020)</p>

<p><a href="https://arxiv.org/pdf/1911.00536.pdf">DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation</a> (Yizhe Zhang, ACL 2020, <a href="https://github.com/microsoft/DialoGPT?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1910.07931.pdf">PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable</a> (Siqi Bao, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/1910.14326.pdf">Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks</a> (Yiping Song, ACL 2020, <a href="https://github.com/zequnl/CMAML?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.166.pdf">Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation</a> (Jun Xu, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.07543.pdf">Group-wise Contrastive Learning for Neural Dialogue Generation</a> (Hengyi Cai, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.04344.pdf">Plug-and-Play Conversational Models</a> (Andrea Madotto, EMNLP 2020 Findings)</p>

<p><a href="https://arxiv.org/pdf/2003.04195.pdf">An Empirical Investigation of Pre-Trained Transformer Language Models for Open-Domain Dialogue Generation</a> (Piji Li, 2020)</p>

<p><a href="https://arxiv.org/pdf/2008.12579.pdf">The Adapter-Bot: All-In-One Controllable Conversational Model</a> (Andrea Madotto, 2020)</p>

<p><a href="https://arxiv.org/pdf/2003.12738.pdf">Variational Transformers for Diverse Response Generation</a> (Zhaojiang Lin, 2020)</p>

<p><a href="https://arxiv.org/pdf/2105.14556.pdf">Diversifying Dialog Generation via Adaptive Label Smoothing</a> (Yida Wang, ACL 2021)</p>

<h5 id="stylized-response-generation">Stylized Response Generation</h5>

<p><a href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00027">Polite Dialogue Generation Without Parallel Data</a> (Tong Niu, TACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1909.05361.pdf">Structuring Latent Spaces for Stylized Response Generation</a> (Xiang Gao, EMNLP 2019, <a href="https://github.com/golsun/StyleFusion?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2009.12719.pdf">Stylized Dialogue Response Generation Using Stylized Unpaired Texts</a> (Yinhe Zheng, AAAI 2021)</p>

<h5 id="empathetic-dialogue-generation">Empathetic Dialogue Generation</h5>

<p><a href="https://www.aclweb.org/anthology/P13-1095.pdf">Predicting and Eliciting Addressee’s Emotion in Online Dialogue</a> (Takayuki Hasegawa, ACL 2013)</p>

<p><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00111/43371/Large-scale-Analysis-of-Counseling-Conversations">Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health</a> (Tim Althoff, TACL 2016, <a href="http://snap.stanford.edu/counseling/">code</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/11325">Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory</a> (Hao Zhou, AAAI 2018, <a href="https://github.com/tuxchow/ecm?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/16317/16080">Eliciting Positive Emotion through Affect-Sensitive Dialogue Response Generation: A Neural Network Approach</a> (Nurul Lubis, AAAI 2018)</p>

<p><a href="https://www.aclweb.org/anthology/N18-2008.pdf">Automatic Dialogue Generation with Expressed Emotions</a> (Chenyang Huang, NAACL 2018)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1071.pdf">A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation</a> (Jingyuan Li, EMNLP 2018)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1104.pdf">MOJITALK: Generating Emotional Responses at Scale</a> (Xianda Zhou, EMNLP 2018)</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-319-76941-7_12">Affective Neural Response Generation</a> (Nabiha Asghar, ECIR 2018)</p>

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S095070511830457X">Topic-Enhanced Emotional Conversation Generation with Attention Mechanism</a> (Yehong Peng, KBS 2019)</p>

<p><a href="https://ieeexplore.ieee.org/document/8649596">Positive Emotion Elicitation in Chat-Based Dialogue Systems</a> (Nurul Lubis, TASLP 2019)</p>

<p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/4740">An Affect-Rich Neural Conversational Model with Biased Attention and Weighted Cross-Entropy Loss</a> (Peixiang Zhong, AAAI 2019, <a href="https://github.com/zhongpeixiang/affect-rich-conversational-model?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1904.02793.pdf">Affect-Driven Dialog Generation</a> (Pierre Colombo, NAACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1359.pdf">Generating Responses with a Specific Emotion in Dialog</a> (Zhenqiao Song, ACL 2019, <a href="https://www.dazhuanlan.com/2020/02/10/5e403d064876d/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1811.00207.pdf">Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset</a> (Hannah Rashkin, ACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/D19-1012.pdf">MoEL: Mixture of Empathetic Listeners</a> (Zhaojiang Lin, EMNLP 2019, <a href="https://github.com/HLTCHKUST/MoEL">code</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/7098">CAiRE: An End-to-End Empathetic Chatbot</a> (Zhaojiang Lin, AAAI 2020)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401108">What If Bots Feel Moods?</a> (Lisong Qiu, SIGIR 2020)</p>

<p><a href="https://www.ijcai.org/proceedings/2020/0503.pdf">EmoElicitor: An Open Domain Response Generation Model with User Emotional Reaction Awareness</a> (Shifeng Li, IJCAI 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.00329.pdf">CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation</a> (Lei Shen, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.04245.pdf">Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards</a> (Justine Zhang, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.08441.pdf">A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support</a> (Ashish Sharma, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.01454.pdf">MIME: MIMicking Emotions for Empathetic Response Generation</a> (Navonil Majumder, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.09708.pdf">Towards Empathetic Dialogue Generation over Multi-type Knowledge</a> (Qintong Li, 2020)</p>

<p><a href="https://arxiv.org/pdf/1911.08698.pdf">EmpDG: Multiresolution Interactive Empathetic Dialogue Generation</a> (Qintong Li, COLING 2020)</p>

<p><a href="https://www.sciencedirect.com/science/article/pii/S0950705121008091?casa_token=vk3Q_bzjsjcAAAAA:RwdXd8y98F7xfYW2kVbavsN6jsOPoM2cEGYuGSmSnSme79P2RSOKgDFaVcqrbA8sDCvZ4rasZw">Empathetic Response Generation through Graph-based Multi-hop Reasoning on Emotional Causality</a> (Jiashuo Wang, KBS 2021)</p>

<p><a href="https://dl.acm.org/doi/full/10.1145/3481890?casa_token=edSpJGyvtvkAAAAA%3Af97AVCWaJGAh9ggDIrb3hz3R3TwEc6lHCveuZLrKgTqHavmuNWAVsjtuqGQO13SnjXfVO989McQ">Dual-View Conditional Variational Auto-Encoder for Emotional Dialogue Generation</a> (Mei Li, TALLIP 2021)</p>

<p><a href="https://arxiv.org/pdf/2101.07714.pdf">Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach</a> (Ashish Sharma, WWW 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.01144.pdf">Towards Emotional Support Dialog Systems</a> (Siyang Liu, ACL 2021, <a href="https://github.com/thu-coai/Emotional-Support-Conversation">code</a>, <a href="https://mp.weixin.qq.com/s/Gj_h4YSK0cxDxh6EIiTWKw">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2105.08316.pdf">CoMAE: A Multi-factor Hierarchical Framework for Empathetic Response Generation</a> (Chujie Zheng, ACL 2021 Findings)</p>

<p><a href="https://arxiv.org/pdf/2109.08828.pdf">Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes</a> (Hyunwoo Kim, EMNLP 2021)</p>

<p><a href="https://arxiv.org/pdf/2109.07779.pdf">Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation</a> (Lei Shen, EMNLP 2021 Findings)</p>

<p><a href="https://aclanthology.org/2021.findings-emnlp.70.pdf">Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations</a> (Jun Gao, EMNLP 2021 Findings)</p>

<p><a href="https://arxiv.org/pdf/2105.08251.pdf">Emotion Eliciting Machine: Emotion Eliciting Conversation Generation based on Dual Generator</a> (Hao Jiang, 2021)</p>

<p><a href="https://arxiv.org/pdf/2111.00310.pdf">EmpBot: A T5-based Empathetic Chatbot focusing on Sentiments</a> (Emmanouil Zaranis, 2021)</p>

<p><a href="https://arxiv.org/pdf/2109.05739.pdf">CEM: Commonsense-aware Empathetic Response Generation</a> (Sahand Sabour, AAAI 2022)</p>

<h5 id="persona-based-dialogue-system">Persona-Based Dialogue System</h5>

<p><a href="https://arxiv.org/pdf/1603.06155.pdf">A Persona-Based Neural Conversation Model</a> (Jiwei Li, ACL 2016, <a href="https://github.com/shrebox/Personified-Chatbot?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1205.pdf">Personalizing Dialogue Agents: I have a dog, do you have pets too?</a> (Saizheng Zhang, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1905.12188.pdf">Exploiting Persona Information for Diverse Generation of Conversational Responses</a> (Haoyu Song, IJCAI 2019, <a href="https://github.com/vsharecodes/percvae?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1542.pdf">Personalizing Dialogue Agents via Meta-Learning</a> (Andrea Madotto, ACL 2019)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6417">Generating Persona Consistent Dialogues by Exploiting Natural Language Inference</a> (Haoyu Song, AAAI 2020)</p>

<p><a href="https://arxiv.org/pdf/2002.02153.pdf">A Neural Topical Expansion Framework for Unstructured Persona-oriented Dialogue Generation</a> (Minghong Xu, ECAI 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.07672.pdf">Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation</a> (Hanyu Song, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.05388.pdf">You Impress Me: Dialogue Generation via Mutual Persona Perception</a> (Qian Liu, ACL 2020, <a href="https://github.com/SivilTaram/Persona-Dialogue-Generation?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2010.03205.pdf">Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions</a> (Bodhisattwa Prasad Majumder, EMNLP 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.531.pdf">Towards Persona-Based Empathetic Conversational Model</a> (Peixiang Zhong, EMNLP 2020, <a href="https://github.com/zhongpeixiang/PEC?utm_source=catalyzex.com">code</a>)</p>

<h5 id="knowledge-grounded-dialogue-system">Knowledge-Grounded Dialogue System</h5>

<p><a href="https://arxiv.org/pdf/1605.05110.pdf">Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling</a> (IJCNN 2017, <a href="https://zhuanlan.zhihu.com/p/92605720">note</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/11977">A Knowledge-Grounded Neural Conversation Model</a> (Marjan Ghazvininejad, AAAI 2018, <a href="https://github.com/mgalley/DSTC7-End-to-End-Conversation-Modeling?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/11923">Augmenting End-to-End Dialogue Systems With Commonsense Knowledge</a> (Tom Young, AAAI 2018)</p>

<p><a href="https://www.ijcai.org/Proceedings/2018/0643.pdf">Commonsense Knowledge Aware Conversation Generation with Graph Attention</a> (Hao Zhou, IJCAI 2018)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1138.pdf">Knowledge Diffusion for Neural Dialogue Generation</a> (Shuman Liu, ACL 2018, <a href="https://zhuanlan.zhihu.com/p/51939126">note</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/2019/0706.pdf">Learning to Select Knowledge for Response Generation in Dialog Systems</a> (Rongzhong Lian, IJCAI 2019)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3357384.3357889">Enhancing Conversational Dialogue Models with Grounded Knowledge</a> (Wen Zheng, CIKM 2019)</p>

<p><a href="https://arxiv.org/pdf/1903.10245.pdf">Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs</a> (Zhibin Liu, EMNLP 2019, <a href="https://github.com/PaddlePaddle/models?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6395">Thinking Globally, Acting Locally: Distantly Supervised Global-to-Local Knowledge Selection for Background Based Conversation</a> (Pengjie Ren, AAAI 2020)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6370">RefNet: A Reference-Aware Network for Background Based Conversation</a> (Chuan Meng, AAAI 2020)</p>

<p><a href="https://arxiv.org/pdf/2002.10348.pdf">Low-Resource Knowledge-Grounded Dialogue Generation</a> (Xueliang Zhao, ICLR 2020)</p>

<p><a href="https://arxiv.org/pdf/2002.07510.pdf">Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue</a> (Byeongchang Kim, ICLR 2020, <a href="https://github.com/bckim92/sequential-knowledge-transformer?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/2020/0521.pdf">TopicKA: Generating Commonsense Knowledge-Aware Dialogue Responses Towards the Recommended Topic Fact</a> (Sixing Wu, IJCAI 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.515.pdf">Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness</a> (Sixing Wu, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.08824.pdf">Knowledge-Grounded Dialogue Generation with Pre-trained Language Models</a> (Xueliang Zhao, EMNLP 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.275.pdf">Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation</a> (Xiuyi Chen, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2105.06232.pdf">Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters</a> (Yan Xu, 2021)</p>

<h5 id="conversational-recommender-system">Conversational Recommender System</h5>

<p><a href="https://www.kdd.org/kdd2016/papers/files/rfp0063-christakopoulouA.pdf">Towards Conversational Recommender Systems</a> (Konstantina Christakopoulou, KDD 2016)</p>

<p><a href="https://arxiv.org/pdf/1806.03277.pdf">Conversational Recommender System</a> (Yueming Sun, SIGIR 2018)</p>

<p><a href="https://papers.nips.cc/paper/2018/file/800de15c79c8d840f4e78d3af937d4d4-Paper.pdf">Towards Deep Conversational Recommendations</a> (Raymond Li, NeurIPS 2018, <a href="http://www.xuwei.io/2019/05/02/%E3%80%8Atowards-deep-conversational-recommendations%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.05391.pdf">Towards Knowledge-Based Recommender Dialog System</a> (Qibin Chen, EMNLP 2019, <a href="https://github.com/THUDM/KBRD">code</a>, <a href="https://zhuanlan.zhihu.com/p/270386920">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.03922.pdf">Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue</a> (Dongyeop Kang, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/2008.08247.pdf">Leveraging Historical Interaction Data for Improving Conversational Recommender System</a> (Kun Zhou, CIKM 2020)</p>

<p><a href="https://arxiv.org/pdf/2007.04032.pdf">Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion</a> (Kun Zhou, KDD 2020, <a href="https://cloud.tencent.com/developer/article/1663741">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2005.03954.pdf">Towards Conversational Recommendation over Multi-Type Dialogs</a> (Zeming Liu, ACL 2020, <a href="https://www.jiqizhixin.com/articles/2020-09-10-3">note</a>)</p>

<h4 id="question-answering-and-machine-reading-comprehension">Question Answering and Machine Reading Comprehension</h4>

<h5 id="dataset-2">Dataset</h5>

<p><a href="https://www.aclweb.org/anthology/D13-1020">Mctest: A challenge dataset for the open-domainmachine comprehension of text</a> (Matthew Richardson, EMNLP 2013)</p>

<p><a href="https://aclweb.org/anthology/D15-1237">WIKIQA: A Challenge Dataset for Open-Domain Question Answering</a> (Yi Yang, EMNLP 2015)</p>

<p><a href="https://arxiv.org/pdf/1606.05250.pdf">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a> (Pranav Rajpurkar, 2016, <a href="https://rajpurkar.github.io/SQuAD-explorer/">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1611.09268.pdf">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</a> (Payal Bajaj, 2016, <a href="http://www.msmarco.org/">code</a>, <a href="https://bingning.wang/research/Article/?id=88">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1711.05073.pdf">DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications</a> (Wei He, 2017, <a href="https://github.com/baidu/DuReader">code</a>)</p>

<p><a href="http://aclweb.org/anthology/P17-1147">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a> (Mandar Joshi, ACL 2017, <a href="https://github.com/mandarjoshi90/triviaqa">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1704.04683.pdf">RACE: Large-scale ReAding Comprehension Dataset From Examinations</a> (Guokun Lai, 2017, <a href="https://github.com/qizhex/RACE_AR_baselines">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/Q18-1023">The NarrativeQA Reading Comprehension Challenge</a> (Tomas Kocisky, TACL 2018, <a href="https://github.com/deepmind/narrativeqa">code</a>, <a href="https://www.paperweekly.site/papers/notes/400">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1806.03822.pdf">Know What You Don’t Know: Unanswerable Questions for SQuAD</a> (Pranav Rajpurkar, ACL 2018, <a href="https://rajpurkar.github.io/SQuAD-explorer/">code</a>, <a href="https://www.sohu.com/a/235513642_129720">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1808.07042.pdf">CoQA: A Conversational Question Answering Challenge</a> (Siva Reddy, 2018, <a href="https://stanfordnlp.github.io/coqa/">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1803.05457.pdf">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge</a> (Peter Clark, 2018, <a href="http://data.allenai.org/arc/">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1808.07036v1.pdf">QuAC : Question Answering in Context</a> (Eunsol Choi, EMNLP 2018, <a href="http://quac.ai/">code</a>, <a href="https://zhuanlan.zhihu.com/p/84110287">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1134">A Dataset and Baselines for Sequential Open-Domain Question Answering</a> (Ahmed Elgohary, EMNLP 2018, <a href="http://sequential.qanta.org/">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.01494.pdf">Interpretation of Natural Language Rules in Conversational Machine Reading</a> (Marzieh Saeidi, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1809.02789.pdf">Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering</a> (Todor Mihaylov, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1903.00161.pdf">DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</a> (Dheeru Dua, NAACL 2019, <a href="https://allennlp.org/drop">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1811.00937.pdf">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</a> (Alon Talmor, NAACL 2019, <a href="https://little1tow.github.io/2019/06/14/2019-05-31/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.05803.pdf">Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning</a> (Pradeep Dasigi, EMNLP 2019, <a href="https://allennlp.org/quoref">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.00277.pdf">COSMOS QA: Machine Reading Comprehension with Contextual Commonsense Reasoning</a> (Lifu Huang, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1904.09728.pdf">SocialIQA: Commonsense Reasoning about Social Interactions</a> (Maarten Sap, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1911.11641.pdf">PIQA: Reasoning about Physical Commonsense in Natural Language</a> (Yonatan Bisk, AAAI 2020)</p>

<p><a href="https://openreview.net/pdf?id=qF7FlUT5dxa">CommonsenseQA 2.0: Exposing the Limits of AI through Gamification</a> (Alon Talmor, 2021)</p>

<h5 id="machine-reading-comprehension">Machine Reading Comprehension</h5>

<p><a href="https://lanl.arxiv.org/pdf/1506.03340.pdf">Teaching Machines to Read and Comprehend</a> (Karl Moritz Hermann, NeurIPS 2015, <a href="https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend">code</a>, <a href="https://zhuanlan.zhihu.com/p/21343662">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/P16-1086">Text Understanding with the Attention Sum Reader Network</a> (Rudolf Kadlec, ACL 2016, <a href="https://github.com/rkadlec/asreader">code</a>, <a href="https://zhuanlan.zhihu.com/p/21354432">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1609.05284.pdf">ReasoNet: Learning to Stop Reading in Machine Comprehension</a> (Yelong Shen, KDD 2017, <a href="http://cairohy.github.io/2017/05/22/deeplearning/NLP-RC-ReasoNet-NeurIPS2016-%E3%80%8AReasoNet%20Learning%20to%20Stop%20Reading%20in%20Machine%20Comprehension%E3%80%8B/">note</a>)</p>

<p><a href="https://openreview.net/pdf?id=B1-q5Pqxl">Machine Comprehension Using Match-LSTM and Answer Pointer</a> (Shuohang Wang, ICLR 2017, <a href="https://github.com/MurtyShikhar/Question-Answering">code</a>, <a href="https://zhuanlan.zhihu.com/p/55957106">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1611.01603.pdf">Bidirectional Attention Flow for Machine Comprehension</a> (Minjoon Seo, ICLR 2017, <a href="https://github.com/allenai/bi-att-flow">code</a>, <a href="https://zhuanlan.zhihu.com/p/55975534">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/P/P17/P17-1055.pdf">Attention-over-Attention Neural Networks for Reading Comprehension</a> (Yiming Cui, ACL 2017, <a href="https://github.com/OlavHN/attention-over-attention">code</a>, <a href="https://zhuanlan.zhihu.com/p/56246026">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1710.10723.pdf">Simple and Effective Multi-Paragraph Reading Comprehension</a> (Christopher Clark, 2017, <a href="https://github.com/allenai/document-qa">code</a>, <a href="https://zhuanlan.zhihu.com/p/36812682">note</a>)</p>

<p><a href="http://aclweb.org/anthology/P17-1018">Gated Self-Matching Networks for Reading Comprehension and Question Answering</a> (Wenhui Wang, ACL 2017, <a href="https://github.com/HKUST-KnowComp/R-Net">code</a>, <a href="https://www.jianshu.com/p/71d3b4737c23">note</a>)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf">R-Net: Machine Reading Comprehension with Self-Matching Networks</a> (Natural Language Computing Group, 2017, <a href="https://github.com/HKUST-KnowComp/R-Net">code</a>, <a href="https://zhuanlan.zhihu.com/p/40271565">note</a>)</p>

<p><a href="https://openreview.net/pdf?id=B14TlG-RW">QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</a> (Adams Wei Yu, ICLR 2018, <a href="https://github.com/minsangkim142/Fast-Reading-Comprehension">code</a>, <a href="https://zhuanlan.zhihu.com/p/56285539">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/P18-1158">Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</a> (Wei Wang, ACL 2018, <a href="https://github.com/SparkJiao/SLQA">code</a>, <a href="https://zhuanlan.zhihu.com/p/43556383">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1178">Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification</a> (Yizhong Wang, ACL 2018, <a href="https://indexfziq.github.io/2019/03/08/VNET/">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1159">Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension</a> (Zhen Wang, ACL 2018, <a href="https://zhuanlan.zhihu.com/p/41161714">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1076">Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge</a> (Todor Mihaylov, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1810.13441.pdf">Improving Machine Reading Comprehension with General Reading Strategies</a> (Kai Sun, NAACL 2019, <a href="https://github.com/nlpdata/strategy">code</a>, <a href="https://blog.csdn.net/mottled233/article/details/104535173">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.05147.pdf">SG-Net: Syntax-Guided Machine Reading Comprehension</a> (Zhuosheng Zhang, AAAI 2020, <a href="https://github.com/cooelf/SG-Net">code</a>, <a href="https://zhuanlan.zhihu.com/p/82073864">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2001.09694.pdf">Retrospective Reader for Machine Reading Comprehension</a> (Zhuosheng Zhang, 2020, <a href="https://zhuanlan.zhihu.com/p/137552707">note</a>)</p>

<h5 id="answer-selection">Answer Selection</h5>

<p><a href="https://arxiv.org/abs/1511.04108">LSTM-based Deep Learning Models for Non-factoid Answer Selection</a> (Ming Tan, ICLR 2016, <a href="https://blog.csdn.net/u010960155/article/details/86756911">note</a>)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16331/16177">Hierarchical Attention Flow for Multiple-Choice Reading Comprehension</a> (Haichao Zhu, AAAI 2018)</p>

<p><a href="https://www.aclweb.org/anthology/P18-2118.pdf">A Co-Matching Model for Multi-choice Reading Comprehension</a> (Shuohang Ming, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1903.03033.pdf">Option Comparison Network for Multiple-choice Reading Comprehension</a> (Qiu Ran, 2019, <a href="https://www.zybuluo.com/songying/note/1428013">note</a>)</p>

<h5 id="knowledge-based-question-answering">Knowledge Based Question Answering</h5>

<p><a href="http://cs.jhu.edu/~xuchen/paper/yao-jacana-freebase-acl2014.pdf">Information Extraction over Structured Data: Question Answering with Freebase</a> (Xuchen Yao, ACL 2014, <a href="https://blog.csdn.net/LAW_130625/article/details/78398888">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/P15-1026">Question Answering over Freebase with Multi-Column Convolutional Neural Networks</a> (Li Dong, ACL 2015, <a href="https://github.com/Evergcj/QA_multi-columnCNN">code</a>, <a href="https://blog.csdn.net/LAW_130625/article/details/78447156">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1603.00957v3.pdf">Question Answering on Freebase via Relation Extraction and Textual Evidence</a> (Kun Xu, ACL 2016, <a href="https://zhuanlan.zhihu.com/p/22630320?refer=c_51425207">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1704.08384.pdf">Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks</a> (Rajarshi Das, ACL 2017, <a href="https://zhuanlan.zhihu.com/p/26791788">note</a>)</p>

<p><a href="https://arxiv.org/vc/arxiv/papers/1804/1804.03317v2.pdf">Question Answering over Freebase via Attentive RNN with Similarity Matrix based CNN</a> (Yingqi Qu, ISMC 2018, <a href="https://github.com/quyingqi/kbqa-ar-smcnn">code</a>, <a href="https://blog.csdn.net/Evaooooes/article/details/88691356">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1709.04071.pdf">Variational Reasoning for Question Answering with Knowledge Graph</a> (Yuyu Zhang, AAAI 2018, <a href="http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E7%9A%84%E9%97%AE%E7%AD%94%E5%8F%98%E5%88%86%E6%8E%A8%E7%90%86/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2010.15875.pdf">Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning</a> (Yuncheng Hua, IJCAI 2020)</p>

<h5 id="conversational-question-answering">Conversational Question Answering</h5>

<p><a href="https://arxiv.org/pdf/1812.03593.pdf">SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering</a> (Chenguang Zhu, 2018, <a href="https://github.com/Microsoft/SDNet">code</a>, <a href="https://zhuanlan.zhihu.com/p/66100785">note</a>)</p>

<p><a href="https://papers.nips.cc/paper/2018/file/d63fbf8c3173730f82b150c5ef38b8ff-Paper.pdf">Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base</a> (Daya Guo, NeurIPS 2018)</p>

<p><a href="https://arxiv.org/pdf/1810.06683.pdf">FlowQA: Grasping Flow in History for Conversational Machine Comprehension</a> (Hsin-Yuan Huang, ICLR 2019, <a href="https://github.com/momohuang/FlowQA">code</a>, <a href="https://zhuanlan.zhihu.com/p/53028792">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.05412.pdf">BERT with History Answer Embedding for Conversational Question Answering</a> (Chen Qu, SIGIR 2019, <a href="https://github.com/prdwb/bert_hae">code</a>)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358016">Look before you Hop: Conversational Question Answering over Knowledge Graphs Using Judicious Context Expansion</a> (Philipp Christmann, CIKM 2019)</p>

<p><a href="https://arxiv.org/pdf/1908.09456.pdf">Attentive History Selection for Conversational Question Answering</a> (Chen Qu, CIKM 2019, <a href="https://github.com/prdwb/attentive_history_selection">code</a>, <a href="https://zhuanlan.zhihu.com/p/129800180">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1910.05069.pdf">Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base</a> (Tao Shen, EMNLP 2019)</p>

<h5 id="visual-question-answering">Visual Question Answering</h5>

<p><a href="https://arxiv.org/pdf/1505.00468.pdf">VQA: Visual Question Answering</a> (Aishwarya Agrawal, ICCV 2015)</p>

<p><a href="https://arxiv.org/pdf/1606.00061.pdf">Hierarchical Question-Image Co-Attention for Visual Question Answering</a> (Jiasen Lu, NeurIPS 2016)</p>

<p><a href="https://arxiv.org/pdf/1511.02570.pdf">Explicit Knowledge-based Reasoning for Visual Question Answering</a> (Peng Wang, IJCAI 2017)</p>

<p><a href="https://arxiv.org/pdf/1606.05433.pdf">FVQA: Fact-based Visual Question Answering</a> (Peng Wang, TPAMI 2018, <a href="https://zhuanlan.zhihu.com/p/66282581">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.01124.pdf">Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering</a> (Medhini Narasimhan, ECCV 2018)</p>

<p><a href="https://arxiv.org/pdf/1811.00538.pdf">Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering</a> (Medhini Narasimhan, NeurIPS 2018)</p>

<p><a href="https://arxiv.org/pdf/1906.00067.pdf">OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge</a> (Kenneth Marino, CVPR 2019, <a href="https://okvqa.allenai.org/">code</a>, <a href="https://blog.csdn.net/z704630835/article/details/100095787">note</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6713">KnowIT VQA: Answering Knowledge-Based Questions about Videos</a> (Noa Garcia, AAAI 2020)</p>

<p><a href="http://openaccess.thecvf.com/content_WACV_2020/html/Yang_BERT_representations_for_Video_Question_Answering_WACV_2020_paper.html">BERT Representations for Video Question Answering</a> (Zekun Yang, WACV 2020)</p>

<h4 id="knowledge-representation-and-reasoning">Knowledge Representation and Reasoning</h4>

<h5 id="knowledge-base">Knowledge Base</h5>

<p><a href="https://cis.upenn.edu/~zives/research/dbpedia.pdf">DBpedia: A Nucleus for a Web of Open Data</a> (Soren Auer, 2007, <a href="https://wiki.dbpedia.org/">code</a>)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.538.7139&amp;rep=rep1&amp;type=pdf">Freebase: A Collaboratively Created Graph Database For Structuring Human Knowledge</a> (Kurt Bollacker, 2008, <a href="https://developers.google.com/freebase/">code</a>)</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-319-60045-1_44">CN-DBpedia: A Never-Ending Chinese Knowledge Extraction System</a> (Bo Xu, IEA-AIE 2017, <a href="http://openkg.cn/dataset/cndbpedia">code</a>, <a href="https://blog.csdn.net/u013007703/article/details/90376440">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1612.03975.pdf">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</a> (Robyn Speer, AAAI 2017, <a href="http://conceptnet.io/">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1811.00146.pdf">ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning</a> (Maarten Sap, AAAI 2019)</p>

<p><a href="https://arxiv.org/pdf/2005.00660.pdf">GenericsKB: A Knowledge Base of Generic Statements</a> (Sumithra Bhakthavatsalam, 2020, <a href="https://allenai.org/data/genericskb">code</a>)</p>

<h5 id="knowledge-base-construction">Knowledge Base Construction</h5>

<p><a href="https://arxiv.org/pdf/1906.05317.pdf">COMET : Commonsense Transformers for Automatic Knowledge Graph Construction</a> (Antoine Bosselut, ACL 2019)</p>

<h5 id="knowledge-graph-embedding-and-completion">Knowledge Graph Embedding and Completion</h5>

<p><a href="https://www.utc.fr/~bordesan/dokuwiki/_media/en/transe_nips13.pdf">Translating Embeddings for Modeling Multi-relational Data</a> (Antoine Bordes, NeurIPS 2013, <a href="https://github.com/thunlp/KB2E">code</a>, <a href="https://zhuanlan.zhihu.com/p/32993044">note</a>)</p>

<p><a href="https://pdfs.semanticscholar.org/2a3f/862199883ceff5e3c74126f0c80770653e05.pdf">Knowledge Graph Embedding by Translating on Hyperplanes</a> (Zhen Wang, AAAI 2014, <a href="https://github.com/thunlp/KB2E">code</a>, <a href="https://zhuanlan.zhihu.com/p/32993044">note</a>)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.698.8922&amp;rep=rep1&amp;type=pdf">Learning Entity and Relation Embeddings for Knowledge Graph Completion</a> (Yankai Lin, AAAI 2015, <a href="https://github.com/thunlp/KB2E">code</a>, <a href="https://zhuanlan.zhihu.com/p/32993044">note</a>)</p>

<p><a href="http://anthology.aclweb.org/P/P15/P15-1067.pdf">Knowledge Graph Embedding via Dynamic Mapping Matrix</a> (Guoliang Ji, ACL 2015, <a href="https://github.com/thunlp/OpenKE">code</a>, <a href="https://zhuanlan.zhihu.com/p/32993044">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1509.05490.pdf">TransA: An Adaptive Approach for Knowledge Graph Embedding</a> (Han Xiao, 2015, <a href="https://blog.csdn.net/junruitian/article/details/87006668">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1506.00379.pdf">Modeling Relation Paths for Representation Learning of Knowledge Bases</a> (Yankai Lin, EMNLP 2015, <a href="https://github.com/thunlp/KB2E">code</a>, <a href="https://www.jianshu.com/p/c3ace92cd6ef">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/P16-1219">TransG : A Generative Model for Knowledge Graph Embedding</a> (Han Xiao, ACL 2016, <a href="https://github.com/BookmanHan/Embedding">code</a>, <a href="https://blog.csdn.net/junruitian/article/details/87006668">note</a>)</p>

<p><a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11982/11693">Knowledge Graph Completion with Adaptive Sparse Transfer Matrix</a> (Guoliang Ji, AAAI 2016, <a href="https://github.com/thunlp/Fast-TransX">code</a>, <a href="https://blog.csdn.net/qq_36426650/article/details/103483838">note</a>)</p>

<p><a href="https://www.computer.org/csdl/trans/tk/2017/12/08047276-abs.html">Knowledge Graph Embedding: A Survey of Approaches and Applications</a> (Quan Wang, TKDE 2017, <a href="https://zhuanlan.zhihu.com/p/106024679">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1707.01476.pdf">Convolutional 2D Knowledge Graph Embeddings</a> (Pasquale Minervini, AAAI 2018, <a href="https://github.com/TimDettmers/ConvE">code</a>, <a href="https://blog.csdn.net/damuge2/article/details/87974995">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1711.03438.pdf">Open-World Knowledge Graph Completion</a> (Baoxu Shi, AAAI 2018, <a href="https://github.com/bxshi/ConMask">code</a>, <a href="https://juewang.me/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1808.09040.pdf">One-Shot Relational Learning for Knowledge Graphs</a> (Wenhan Xiong, EMNLP 2018, <a href="https://github.com/xwhan/One-shot-Relational-Learning">code</a>, <a href="https://zhuanlan.zhihu.com/p/59646318">note</a>)</p>

<h5 id="entity-discovery-and-linking">Entity Discovery and Linking</h5>

<p><a href="https://www.aclweb.org/anthology/P/P11/P11-1095.pdf">A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a> (Xianpei Han, ACL 2011, <a href="https://www.cnblogs.com/dhName/p/11078630.html">note</a>)</p>

<p><a href="http://nlp.cs.rpi.edu/paper/edl2014overview.pdf">Overview of TAC-KBP2014 Entity Discovery and Linking Tasks</a> (Heng Ji, TAC 2014)</p>

<p><a href="https://www.aclweb.org/anthology/W16-1313">An Attentive Neural Architecture for Fine-grained Entity Type Classification</a> (Sonse Shimaoka, 2016, <a href="https://blog.csdn.net/weixin_40485502/article/details/104019427">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/E17-1119">Neural Architectures for Fine-grained Entity Type Classification</a> (Sonse Shimaoka, EACL 2017, <a href="https://github.com/shimaokasonse/NFGEC">code</a>)</p>

<p><a href="http://aclweb.org/anthology/E17-1075">Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings</a> (Abhishek Abhishek, EACL 2017, <a href="https://github.com/abhipec/fnet">code</a>)</p>

<p><a href="http://aclweb.org/anthology/N18-1002">Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss</a> (Peng Xu, NAACL 2018, <a href="https://github.com/billy-inn/NFETC">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P18-1009">Ultra-fine entity typing</a> (Eunsol Choi, ACL 2018, <a href="https://blog.csdn.net/xff1994/article/details/90293957">note</a>)</p>

<h5 id="entity-set-expansion">Entity Set Expansion</h5>

<p><a href="https://aclanthology.org/D09-1098.pdf">Web-Scale Distributional Similarity and Entity Set Expansion</a> (Patrick Pantel, EMNLP 2009)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/2835776.2835808?casa_token=WO5mp1Qk4WMAAAAA:OWY3rVjXu57DFQafsDOP6mz3u6GROVN-Z1O9uvRPNgZ6-IYVlFY_jN5yXmYHjkcI63NfabkWOa8D1lU">EgoSet: Exploiting Word Ego-networks and User-generated Ontology for Multifaceted Set Expansion</a> (Xin Rong, WSDM 2016)</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-319-71249-9_18">SetExpan: Corpus-Based Set Expansion via Context Feature Selection and Rank Ensemble</a> (Jiaming Shen, ECML PKDD 2017)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3219819.3220115">HiExpan: Task-Guided Taxonomy Construction by Hierarchical Tree Expansion</a> (Jiaming Shen, KDD 2018)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3366423.3380132?casa_token=ZxAWXTyFuO4AAAAA:J_673jkMVGIhJvBkHkyQGGlz6KeuD7aFghWvN7ARUVKqxABsD8G-at1HzLefvofpf_zo_xyAogcMfEI">TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network</a> (Jiaming Shen, WWW 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.13897.pdf">Empower Entity Set Expansion via Language Model Probing</a> (Yunyi Zhang, ACL 2020)</p>

<h5 id="causal-knowledge">Causal Knowledge</h5>

<p><a href="https://arxiv.org/pdf/1605.07895.pdf">Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey</a> (Nabiha Asghar, 2016)</p>

<p><a href="https://www.ijcai.org/Proceedings/2020/0502.pdf">Guided Generation of Cause and Effect</a> (Zhongyang Li, IJCAI 2020)</p>

<h5 id="knowledge-graph-application">Knowledge Graph Application</h5>

<p><a href="https://arxiv.org/pdf/1802.05930.pdf">Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing</a> (Annervaz K M, NAACL 2018, <a href="https://blog.lorrin.info/posts/%5B2018.5.10%5DKnowledge-Graph-Augmented-Neural-Networks-for-NLP/">note</a>)</p>

<p><a href="http://aclweb.org/anthology/P18-1223">Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval</a> (Zhenghao Liu, ACL 2018, <a href="https://github.com/thunlp/EntityDuetNeuralRanking">code</a>, <a href="https://blog.csdn.net/weixin_43087818/article/details/103764135">note</a>)</p>

<h4 id="coreference-resolution">Coreference Resolution</h4>

<p><a href="https://www.aclweb.org/anthology/D16-1245.pdf">Deep Reinforcement Learning for Mention-Ranking Coreference Models</a> (Kevin Clark, EMNLP 2016, <a href="https://github.com/clarkkev/deep-coref">code</a>)</p>

<p><a href="https://nlp.stanford.edu/pubs/clark2016improving.pdf">Improving Coreference Resolution by Learning Entity-Level Distributed Representations</a> (Kevin Clark, ACL 2016, <a href="https://github.com/clarkkev/deep-coref">code</a>, <a href="https://zhuanlan.zhihu.com/p/97097668">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/N18-2108.pdf">Higher-order Coreference Resolution with Coarse-to-fine Inference</a> (Kenton Lee, NAACL 2018, <a href="https://github.com/kentonl/e2e-coref">code</a>, <a href="https://zhuanlan.zhihu.com/p/93900881">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/D18-1518.pdf">Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution</a> (Hongyin Luo, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1908.09091.pdf">BERT for Coreference Resolution: Baselines and Analysis</a> (Mandar Joshi, EMNLP 2019, <a href="https://github.com/mandarjoshi90/coref">code</a>, <a href="https://blog.csdn.net/BeiLi_ShanGui/article/details/103156488">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.12086.pdf">GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue</a> (Jun Quan, 2019, <a href="https://blog.csdn.net/lwgkzl/article/details/102482928">note</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/2019/0700.pdf">Incorporating Structural Information for Better Coreference Resolution</a> (Kong Fang, IJCAI 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1064.pdf">End-to-end Deep Reinforcement Learning Based Coreference Resolution</a> (Hongliang Fei, ACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1593.pdf">The Referential Reader: A Recurrent Entity Network for Anaphora Resolution</a> (Fei Liu, ACL 2019)</p>

<h5 id="pronoun-resolution">Pronoun Resolution</h5>

<p><a href="https://arxiv.org/pdf/1611.04146.pdf">Commonsense Knowledge Enhanced Embeddings for Solving Pronoun Disambiguation Problems in Winograd Schema Challenge</a> (Quan Liu, 2016, <a href="https://www.jianshu.com/p/b94bcacce74c">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.08025.pdf">WikiCREM: A Large Unsupervised Corpus for Coreference Resolution</a> (Vid Kocijan, 2019)</p>

<p><a href="https://arxiv.org/pdf/1905.08868.pdf">Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution</a> (Yinchuan Xu, 2019, <a href="https://github.com/ianycxu/RGCN-with-BERT">code</a>)</p>

<p><a href="https://www.aclweb.org/anthology/N19-1093.pdf">Incorporating Context and External Knowledge for Pronoun Coreference Resolution</a> (Hongming Zhang, NAACL 2019, <a href="https://github.com/HKUST-KnowComp/Pronoun-Coref">code</a>, <a href="https://zhuanlan.zhihu.com/p/88020550">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1083.pdf">Knowledge-aware Pronoun Coreference Resolution</a> (Hongming Zhang, ACL 2019, <a href="https://github.com/HKUST-KnowComp/Pronoun-Coref-KG">code</a>, <a href="https://zhuanlan.zhihu.com/p/85180047">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.00421.pdf">What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues</a> (Xintong Yu, EMNLP 2019, <a href="https://zhuanlan.zhihu.com/p/91231002">note</a>)</p>

<h5 id="zero-pronoun-resolution">Zero Pronoun Resolution</h5>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=F05E1DD4B64B0771E279426984E7CDD1?doi=10.1.1.65.1935&amp;rep=rep1&amp;type=pdf">Identification and Resolution of Chinese Zero Pronouns: A Machine Learning Approach</a> (Shanheng Zhao and Hwee Tou Ng, EMNLP 2007)</p>

<p><a href="http://www.aclweb.org/anthology/P15-2053">Chinese Zero Pronoun Resolution: A Joint Unsupervised Discourse-Aware Model Rivaling State-of-the-Art Resolvers</a> (Chen Chen and Vincent Ng, ACL 2015)</p>

<p><a href="http://www.aclweb.org/anthology/P16-1074">Chinese Zero Pronoun Resolution with Deep Neural Networks</a> (Chen Chen and Vincent Ng, ACL 2016)</p>

<p><a href="http://aclweb.org/anthology/D17-1135">Chinese Zero Pronoun Resolution with Deep Memory Network</a> (Qingyu Yin, EMNLP 2017)</p>

<p><a href="https://arxiv.org/pdf/1604.05800.pdf">A Deep Neural Network for Chinese Zero Pronoun Resolution</a> (Qingyu Yin, IJCAI 2017)</p>

<p><a href="https://arxiv.org/pdf/1606.01603.pdf">Generating and Exploiting Large-Scale Pseudo Training Data for Zero Pronoun Resolution</a> (Ting Liu, ACL 2017, <a href="https://zhuanlan.zhihu.com/p/136544141">note</a>)</p>

<p><a href="http://aclweb.org/anthology/P18-1053">Deep Reinforcement Learning for Chinese Zero pronoun Resolution</a> (Qingyu Yin, ACL 2018, <a href="https://github.com/qyyin/Reinforce4ZP">code</a>, <a href="https://www.jiqizhixin.com/articles/2018-05-21-6">note</a>)</p>

<p><a href="http://aclweb.org/anthology/C18-1002">Zero Pronoun Resolution with Attention-based Neural Network</a> (Qingyu Yin, COLING 2018, <a href="https://github.com/qyyin/AttentionZP">code</a>, <a href="https://www.jiqizhixin.com/articles/2018-07-28-8">note</a>)</p>

<p><a href="https://144.208.67.177/ojs/index.php/AAAI/article/view/6352">Hierarchical Attention Network with Pairwise Loss for Chinese Zero Pronoun Resolution</a> (Peiqin Lin, AAAI 2020, <a href="https://github.com/lpq29743/HAN-PL">code</a>, <a href="https://zhuanlan.zhihu.com/p/151387067">note</a>)</p>

<h4 id="natural-language-processing-for-programming-language">Natural Language Processing for Programming Language</h4>

<p><a href="https://arxiv.org/pdf/1808.01400.pdf">code2seq: Generating sequences from structured representations of code</a> (Uri Alon, ICLR 2019, <a href="https://github.com/tech-srl/code2seq">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code</a> (Mark Chen, 2021)</p>

<h5 id="code-comment-generation">Code Comment Generation</h5>

<p><a href="https://dl.acm.org/doi/abs/10.1145/1858996.1859006">Towards automatically generating summary comments for java methods</a> (Giriprasad Sridhara, ASE 2010)</p>

<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.714.7445&amp;rep=rep1&amp;type=pdf">On automatically generating commit messages via summarization of source code changes</a> (Luis Fernando Cortés-Coy, SCAM 2014)</p>

<p><a href="http://www.jatit.org/volumes/Vol95No21/12Vol95No21.pdf">Source code analysis extractive approach to generate textual summary</a> (Kareem Abbas Dawood, 2017)</p>

<h5 id="code-retrieval">Code Retrieval</h5>

<p><a href="https://guxd.github.io/papers/deepcs.pdf">Deep code search</a> (Xiaodong Gu, ICSE 2018)</p>

<h4 id="natural-language-generation">Natural Language Generation</h4>

<p><a href="https://proceedings.neurips.cc/paper/2015/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf">Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</a> (Samy Bengio, NeurIPS 2015)</p>

<p><a href="https://arxiv.org/pdf/1703.09902.pdf">Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation</a> (Albert Gatt, 2017)</p>

<p><a href="https://arxiv.org/pdf/1711.09534.pdf">Neural Text Generation: A Practical Guide</a> (Ziang Xie, 2017)</p>

<p><a href="https://arxiv.org/pdf/1702.02390.pdf">A Hybrid Convolutional Variational Autoencoder for Text Generation</a> (Stanislau Semeniuta, 2017)</p>

<p><a href="https://arxiv.org/pdf/1808.02747.pdf">Natural Language Generation by Hierarchical Decoding with Linguistic Patterns</a> (Shang-Yu Su, NAACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1903.07137.pdf">Topic-Guided Variational Autoencoders for Text Generation</a> (Wenlin Wang, NAACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.05667.pdf">Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine Decoding</a> (Junyi Li, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.02181.pdf">Syntax-Infused Variational Autoencoder for Text Generation</a> (Xinyuan Zhang, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1902.00154.pdf">Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models</a> (Dinghan Shan, ACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1407.pdf">Keeping Notes: Conditional Natural Language Generation with a Scratchpad Mechanism</a> (Ryan Y. Benmalek, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.07651.pdf">Scheduled Sampling for Transformers</a> (Tsvetomila Mihaylova, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1908.06605.pdf">Long and Diverse Text Generation with Planning-based Hierarchical Variational Model</a> (Zhihong Shao, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1908.04319.pdf">Neural Text Generation With Unlikelihood Training</a> (Sean Welleck, 2019, <a href="https://zhuanlan.zhihu.com/p/78695564">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2007.03909.pdf">Best-First Beam Search</a> (Clara Meister, TACL 2020, <a href="https://github.com/huggingface/transformers/issues/6565">code</a>, <a href="https://zhuanlan.zhihu.com/p/187270580">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1904.09751.pdf">The Curious Case of Neural Text Degeneration</a> (Ari Holtzman, ICLR 2020, <a href="https://zhuanlan.zhihu.com/p/115076102">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1911.03829.pdf">Distilling Knowledge Learned in BERT for Text Generation</a> (Yen-Chun Chen, ACL 2020)</p>

<p><a href="https://openreview.net/pdf?id=Wga_hrCa3P3">Contrastive Learning with Adversarial Perturbations for Conditional Text Generation</a> (Seanie Lee, ICLR 2021, <a href="https://github.com/seanie12/CLAPS">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2102.01672.pdf">The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics</a> (Sebastian Gehrmann, 2021)</p>

<p><a href="https://arxiv.org/pdf/2105.03641.pdf">Neural Text Generation with Part-of-Speech Guided Softmax</a> (Zhixian Yang, 2021)</p>

<h5 id="automatic-metric">Automatic Metric</h5>

<p><a href="https://nymity.ch/sybilhunting/pdf/Levenshtein1966a.pdf">Binary Codes Capable of Correcting Deletions, Insertions and Reversals</a> (VI Levenshtein, 1966)</p>

<p><a href="http://www.aclweb.org/anthology/W/W04/W04-1013.pdf">ROUGE: A Package for Automatic Evaluation of Summaries</a> (Chin-Yew Lin, 2004)</p>

<p><a href="https://www.aclweb.org/anthology/P15-2073.pdf">∆BLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets</a> (Michel Galley, ACL 2015)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1264.pdf">Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts</a> (Elizabeth Clark, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.02622.pdf">MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance</a> (Wei Zhao, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1904.09675.pdf">BERTScore: Evaluating Text Generation with BERT</a> (Tianyi Zhang, ICLR 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.04696.pdf">BLEURT: Learning Robust Metrics for Text Generation</a> (Thibault Sellam, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2006.14799.pdf">Evaluation of Text Generation: A Survey</a> (Asli Celikyilmaz, 2020)</p>

<p><a href="https://arxiv.org/pdf/2106.01229.pdf">Lower Perplexity is Not Always Human-Like</a> (Tatsuki Kuribayashi, ACL 2021)</p>

<h5 id="sequence-to-sequence">Sequence to Sequence</h5>

<p><a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to Sequence Learning with Neural Networks</a> (Ilya Sutskever, NeurIPS 2014)</p>

<p><a href="https://pdfs.semanticscholar.org/bb3e/bc09b65728d6eced04929df72a006fb5210b.pdf">Convolutional Sequence to Sequence Learning</a> (Jonas Gehring, ICML 2017, <a href="https://github.com/tobyyouup/conv_seq2seq">code</a>, <a href="https://zhuanlan.zhihu.com/p/26918935">note</a>)</p>

<p><a href="https://papers.nips.cc/paper/2017/file/c6036a69be21cb660499b75718a3ef24-Paper.pdf">Deliberation Networks: Sequence Generation Beyond One-Pass Decoding</a> (Yingce Xia, NeurIPS 2017)</p>

<p><a href="https://arxiv.org/pdf/1805.09461.pdf">Deep Reinforcement Learning For Sequence to Sequence Models</a> (Yaser Keneshloo, 2018, <a href="https://github.com/yaserkl/RLSeq2Seq">code</a>, <a href="https://www.jianshu.com/p/1213de861491">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1812.04616.pdf">Von Mises<em>-</em>Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs</a> (Sachin Kumar, ICLR 2019, <a href="https://github.com/Sachin19/seq2seq-con">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.02450.pdf">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a> (Kaitao Song, 2019, <a href="https://github.com/microsoft/MASS">code</a>, <a href="https://zhuanlan.zhihu.com/p/71022527">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1910.13461.pdf">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> (Mike Lewis, ACL 2020)</p>

<h5 id="graph-to-sequence">Graph to Sequence</h5>

<p><a href="https://arxiv.org/pdf/1904.02342.pdf">Text Generation from Knowledge Graphs with Graph Transformers</a> (Rik Koncel-Kedziorski, NAACL 2019, <a href="https://github.com/rikdz/GraphWriter">code</a>, <a href="https://zhuanlan.zhihu.com/p/90084109">note</a>)</p>

<h5 id="controlled-text-generation">Controlled Text Generation</h5>

<p><a href="https://arxiv.org/pdf/1703.00955.pdf">Toward Controlled Generation of Text</a> (Zhiting Hu, ICML 2017)</p>

<p><a href="https://arxiv.org/pdf/1702.08139.pdf">Improved Variational Autoencoders for Text Modeling using Dilated Convolutions</a> (Zichao Yang, ICML 2017)</p>

<p><a href="https://arxiv.org/pdf/1706.04223.pdf">Adversarially Regularized Autoencoders</a> (Jake Zhao, 2017)</p>

<p><a href="https://www.ijcai.org/proceedings/2019/0727.pdf">T-CVAE: Transformer-Based Conditioned Variational Autoencoder for Story Completion</a> (Tianming Wang, IJCAI 2019, <a href="https://zhuanlan.zhihu.com/p/91166636">note</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6346">Complementary Auxiliary Classifiers for Label-Conditional Text Generation</a> (Yuan Li, AAAI 2020)</p>

<p><a href="https://arxiv.org/pdf/1912.02164.pdf">Plug and Play Language Models: a Simple Approach to Controlled Text Generation</a> (Sumanth Dathathri, ICLR 2020)</p>

<p><a href="https://arxiv.org/pdf/2105.03023.pdf">DEXPERTS: On-the-Fly Controlled Text Generation with Experts and Anti-Experts</a> (Alisa Liu, ACL 2021)</p>

<h5 id="amr-to-text-generation">AMR-to-Text Generation</h5>

<p><a href="https://arxiv.org/pdf/1909.00136.pdf">Modeling Graph Structure in Transformer for Better AMR-to-Text Generation</a> (Jie Zhu, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.00352.pdf">Enhancing AMR-to-Text Generation with Dual Graph Representations</a> (Leonardo F. R. Ribeiro, EMNLP 2019, <a href="https://github.com/UKPLab/emnlp2019-dualgraph">code</a>, <a href="https://zhuanlan.zhihu.com/p/105701258">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.67.pdf">Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks</a> (Yanbin Zhao, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.09123.pdf">GPT-too: A language-model-first approach for AMR-to-text generation</a> (Manuel Mager, ACL 2020, <a href="https://github.com/IBM/GPT-too-AMR2text">code</a>)</p>

<h5 id="data-to-text-generation">Data-to-Text Generation</h5>

<p><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4668">Data-to-Text Generation with Content Selection and Planning</a> (Ratish Puduppully, AAAI 2019, <a href="https://github.com/ratishsp/data2text-plan-py">code</a>, <a href="https://zhuanlan.zhihu.com/p/85275520">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.03221.pdf">Data-to-text Generation with Entity Modeling</a> (Ratish Puduppully, ACL 2019, <a href="https://github.com/ratishsp/data2text-entity-py">code</a>, <a href="https://zhuanlan.zhihu.com/p/82054729">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.02304.pdf">Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time)</a> (Heng Gong, EMNLP 2019)</p>

<h4 id="paraphrase-generation">Paraphrase Generation</h4>

<p><a href="https://www.aclweb.org/anthology/N19-1363.pdf">Submodular Optimization-based Diverse Paraphrasing and its Effectiveness in Data Augmentation</a> (Ashutosh Kumar, NAACL 2019, <a href="https://github.com/malllabiisc/DiPS">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1904.01173.pdf">A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations</a> (Mingda Chen, NAACL 2019, <a href="https://github.com/mingdachen/disentangle-semantics-syntax">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2001.01941.pdf">Paraphrase Generation with Latent Bag of Words</a> (Yao Fu, NeurIPS 2019)</p>

<h4 id="storytelling">Storytelling</h4>

<p><a href="https://www.aaai.org/AAAI21Papers/AAAI-10130.YuMH.pdf">Content Learning with Structure-Aware Writing: A Graph-Infused Dual Conditional Variational Autoencoder for Automatic Storytelling</a> (Meng-Hsuan Yu, AAAI 2021)</p>

<h4 id="machine-translation">Machine Translation</h4>

<p><a href="https://aclanthology.org/J90-2002.pdf">A Statistical Approach To Machine Translation</a> (Peter F. Brown, CL 1990)</p>

<p><a href="https://aclanthology.org/N03-1017.pdf">Statistical Phrase-Based Translation</a> (Philipp Koehn, NAACL 2003)</p>

<p><a href="http://aclweb.org/anthology/P03-1021">Minimum Error Rate Training in Statistical Machine Translation</a> (Franz Josef Och, ACL 2003)</p>

<p><a href="http://aclweb.org/anthology/J07-2003">Hierarchical Phrase-Based Translation</a> (David Chiang, CL 2007)</p>

<p><a href="https://aclanthology.org/P07-2045.pdf">Moses: Open Source Toolkit for Statistical Machine Translation</a> (Philipp Koehn, ACL 2007)</p>

<p><a href="https://books.google.com.hk/books?hl=en&amp;lr=&amp;id=kKYgAwAAQBAJ&amp;oi=fnd&amp;pg=PR11&amp;dq=info:a-eei_wZtkcJ:scholar.google.com&amp;ots=k7yo-Tb54u&amp;sig=3YW5kEUyuihW52UjZk3Aq1H1m0Q&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">Statistical Machine Translation</a> (Philipp Koehn, 2010)</p>

<p><a href="https://arxiv.org/pdf/1406.1078.pdf">Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation</a> (Kyunghyun Cho, EMNLP 2014, <a href="https://cuiqingcai.com/5737.html">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1409.1259.pdf">On the Properties of Neural Machine Translation: Encoder–Decoder Approaches</a> (Kyunghyun Cho, 2014, <a href="https://blog.csdn.net/BeforeEasy/article/details/80332497">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1511.06709.pdf">Improving Neural Machine Translation Models with Monolingual Data</a> (Rico Sennrich, ACL 2016)</p>

<p><a href="https://www.aclweb.org/anthology/P16-1008.pdf">Modeling Coverage for Neural Machine Translation</a> (Zhaopeng Tu, ACL 2016, <a href="https://zhuanlan.zhihu.com/p/22993927">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1609.08144.pdf">Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a> (Yonghui Wu, 2016, <a href="https://blog.csdn.net/Xiao_yanling/article/details/90290862">note</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/10950">Neural Machine Translation with Reconstruction</a> (Zhaopeng Tu, AAAI 2017)</p>

<p><a href="https://arxiv.org/pdf/1703.01619.pdf">Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a> (Graham Neubig, 2017, <a href="https://google.github.io/seq2seq/nmt/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1706.03872.pdf">Six Challenges for Neural Machine Translation</a> (Philipp Koehn, 2017)</p>

<p><a href="https://arxiv.org/pdf/1712.02109.pdf">Multi-channel Encoder for Neural Machine Translation</a> (Hao Xiong, AAAI 2018, <a href="https://www.jiqizhixin.com/articles/2017-12-14-10">note</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/11913">Translating Pro-Drop Languages with Reconstruction Models</a> (Longyue Wang, AAAI 2018)</p>

<p><a href="https://arxiv.org/pdf/1810.06195.pdf">Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism</a> (Longyue Wang, EMNLP 2018)</p>

<p><a href="https://www.aclweb.org/anthology/D19-5622.pdf">Mixed Multi-Head Self-Attention for Neural Machine Translation</a> (Hongyi Cui, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.08478.pdf">Simple, Scalable Adaptation for Neural Machine Translation</a> (Ankur Bapna, EMNLP 2019, <a href="https://zhuanlan.zhihu.com/p/114955522">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.01130.pdf">Dynamically Composing Domain-Data Selection with Clean-Data Selection by “Co-Curricular Learning” for Neural Machine Translation</a> (Wei Wang, ACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1623.pdf">Reducing Word Omission Errors in Neural Machine Translation: A Contrastive Learning Approach</a> (Zonghan Yang, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.02448.pdf">Bridging the Gap between Training and Inference for Neural Machine Translation</a> (Wen Zhang, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1912.02047.pdf">Neural Machine Translation: A Review and Survey</a> (Felix Stahlberg, 2019)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6479">Towards Making the Most of BERT in Neural Machine Translation</a> (Jiacheng Yang, AAAI 2020)</p>

<p><a href="https://openreview.net/pdf?id=HkxQRTNYPH">Mirror-Generative Neural Machine Translation</a> (Zaixiang Zheng, ICLR 2020)</p>

<p><a href="https://arxiv.org/abs/2002.06823">Incorporating Bert into Neural Machine Translation</a> (Jinhua Zhu, ICLR 2020)</p>

<p><a href="https://arxiv.org/pdf/2102.11403.pdf">Exploring Supervised and Unsupervised Rewards in Machine Translation</a> (Julia Ive, EACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.03151.pdf">Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation</a> (Dana Ruiter, EMNLP 2020)</p>

<p><a href="https://aclanthology.org/2021.naacl-main.18.pdf">Counterfactual Data Augmentation for Neural Machine Translation</a> (Qi Liu, NAACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2104.09554.pdf">Can Latent Alignments Improve Autoregressive Machine Translation?</a> (Adi Haviv, NAACL 2021)</p>

<p><a href="http://proceedings.mlr.press/v139/li21n/li21n.pdf">Mixed Cross Entropy Loss for Neural Machine Translation</a> (Haoran Li, ICML 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.00941.pdf">Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation</a> (Wenxiang Jiao, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2105.11269.pdf">Neural Machine Translation with Monolingual Translation Memory</a> (Deng Cai, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.03297.pdf">On the Language Coverage Bias for Neural Machine Translation</a> (Shuo Wang, ACL 2021 Findings)</p>

<p><a href="https://arxiv.org/pdf/2104.05336.pdf">Machine Translation Decoding beyond Beam Search</a> (Remi Leblond, 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.11375.pdf">Phrase-level Active Learning for Neural Machine Translation</a> (Junjie Hu, 2021)</p>

<h5 id="dataset-3">Dataset</h5>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.5497&amp;rep=rep1&amp;type=pdf">Europarl: A Parallel Corpus for Statistical Machine Translation</a> (Philipp Koehn, 2005, <a href="http://www.statmt.org/europarl/">code</a>)</p>

<p><a href="https://arxiv.org/pdf/cs/0609058.pdf">The JRC-Acquis: A Multilingual Aligned Parallel Corpus with 20+ Languages</a> (Ralf Steinberger, 2006, <a href="https://opus.nlpl.eu/JRC-Acquis.php">code</a>)</p>

<p><a href="http://cysouw.de/home/presentations_files/cysouwmayerPARALLELLREC.pdf">Creating a Massively Parallel Bible Corpus</a> (Thomas Mayer, 2014)</p>

<p><a href="https://link.springer.com/article/10.1007/s10579-014-9287-y">A Massively Parallel Corpus: the Bible in 100 Languages</a> (Christos Christodouloupoulos, 2015)</p>

<p><a href="https://conferences.unite.un.org/UNCorpus/Content/Doc/un.pdf">The United Nations Parallel Corpus v1.0</a> (Michał Ziemski, LREC 2016, <a href="https://conferences.unite.un.org/uncorpus">code</a>)</p>

<p><a href="https://www.repository.cam.ac.uk/bitstream/handle/1810/296987/P19-1310.pdf?sequence=3">JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages</a> (Zeljko Agic, ACL 2019, <a href="https://opus.nlpl.eu/JW300.php">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1907.05791.pdf">WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia</a> (Holger Schwenk, 2019, <a href="https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1808.03738.pdf">Ancient–Modern Chinese Translation with a New Large Training Dataset</a> (Dayiheng Liu, 2020)</p>

<h5 id="automatic-metric-1">Automatic Metric</h5>

<p><a href="https://www.aclweb.org/anthology/C92-2067.pdf">A New Quantitative Quality Measure for Machine Translation Systems</a> (Keh-Yih Su, COLING 1992)</p>

<p><a href="http://www.lrec-conf.org/proceedings/lrec2000/pdf/278.pdf">An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research</a> (Sonja Nießen, LREC 2000)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.590.6755&amp;rep=rep1&amp;type=pdf">Using Multiple Edit Distances to Automatically Rank Machine Translation Output</a> (Yasuhiro Akiba, 2001)</p>

<p><a href="http://aclweb.org/anthology/P/P02/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a> (Kishore Papineni, ACL 2002, <a href="https://www.jianshu.com/p/320ffec4e99f">note</a>)</p>

<p><a href="https://dl.acm.org/doi/abs/10.5555/1289189.1289273">Automatic Evaluation of Machine Translation Quality using N-gram CoOccurrence Statistics</a> (George R Doddington, 2002)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.522&amp;rep=rep1&amp;type=pdf">A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation</a> (Gregor Leusch, 2003)</p>

<p><a href="https://www.aclweb.org/anthology/W05-0909.pdf">METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</a> (Satanjeev Banerjee, ACL 2005 Workshop)</p>

<p><a href="https://aclanthology.org/2021.acl-long.566.pdf">Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers</a> (Benjamin Marie, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2107.10821.pdf">To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation</a> (Tom Kocmi, 2021)</p>

<h5 id="vocabulary-coverage">Vocabulary Coverage</h5>

<p><a href="https://aclanthology.org/N15-1176.pdf">Learning Translation Models from Monolingual Continuous Representations</a> (Kai Zhao, NAACL 2015)</p>

<p><a href="https://arxiv.org/pdf/1410.8206.pdf">Addressing the Rare Word Problem in Neural Machine Translation</a> (Minh-Thang Luong, ACL 2015)</p>

<p><a href="https://arxiv.org/pdf/1508.07909.pdf">Neural Machine Translation of Rare Words with Subword Units</a> (Rico Sennrich, ACL 2016, <a href="https://github.com/rsennrich/subword-nmt">code</a>, <a href="https://zhuanlan.zhihu.com/p/38574684">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2012.15671.pdf">Vocabulary Learning via Optimal Transport for Machine Translation</a> (Jingjing Xu, ACL 2021, <a href="https://zhuanlan.zhihu.com/p/387561592">note</a>)</p>

<h5 id="word-alignment">Word Alignment</h5>

<p><a href="https://aclanthology.org/J93-2003.pdf">The Mathematics of Statistical Machine Translation: Parameter Estimation</a> (Peter F. Brown, CL 1993, <a href="https://zhuanlan.zhihu.com/p/72640549">code</a>)</p>

<p><a href="https://aclanthology.org/C96-2141.pdf">HMM-Based Word Alignment in Statistical Translation</a> (Stephan Vogel, COLING 1996)</p>

<p><a href="https://aclanthology.org/J00-2004.pdf">Models of Translational Equivalence among Words</a> (I. Dan Melamed, CL 2000)</p>

<p><a href="https://aclanthology.org/P00-1056.pdf">Improved Statistical Alignment Models</a> (Franz Josef Och, ACL 2000)</p>

<p><a href="https://aclanthology.org/J03-1002.pdf">A Systematic Comparison of Various Statistical Alignment Models</a> (Franz Josef Och, CL 2003)</p>

<p><a href="https://www.aclweb.org/anthology/P05-1057.pdf">Log-Linear Models for Word Alignment</a> (Yang Liu, ACL 2005)</p>

<p><a href="https://aclanthology.org/H05-1009.pdf">NeurAlign: Combining Word Alignments Using Neural Networks</a> (Necip Fazil Ayan, 2005)</p>

<p><a href="https://cs.stanford.edu/~pliang/papers/alignment-naacl2006.pdf">Alignment by Agreement</a> (Percy Liang, NAACL 2006)</p>

<p><a href="https://direct.mit.edu/coli/article/33/3/293/1949/Measuring-Word-Alignment-Quality-for-Statistical">Measuring Word Alignment Quality for Statistical Machine Translation</a> (Alexander Fraser, CL 2007)</p>

<p><a href="http://www.inesc-id.pt/pt/indicadores/Ficheiros/4735.pdf">Building a golden collection of parallel Multi-Language Word Alignment</a> (João de Almeida Varelas Graça, LREC 2008, <a href="https://www.hlt.inesc-id.pt/w/Word_Alignments#">code</a>)</p>

<p><a href="https://aclanthology.org/W08-0509.pdf">Parallel Implementations of Word Alignment Tool</a> (Qin Gao, 2008)</p>

<p><a href="https://aclanthology.org/P11-1042.pdf">Unsupervised Word Alignment with Arbitrary Features</a> (Chris Dyer, ACL 2011)</p>

<p><a href="https://aclanthology.org/P11-2032.pdf">Bayesian Word Alignment for Statistical Machine Translation</a> (Coşkun Mermer, ACL 2011)</p>

<p><a href="https://aclanthology.org/www.mt-archive.info/10/ACL-2012-Riley.pdf">Improving the IBM Alignment Models Using Variational Bayes</a> (Darcey Riley, ACL 2012)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2013/05/FinalPrinted.pdf">Improving Statistical Machine Translation Using Bayesian Word Alignment and Gibbs Sampling</a> (Coşkun Mermer, TASLP 2013)</p>

<p><a href="https://aclanthology.org/N13-1073.pdf">A Simple, Fast, and Effective Reparameterization of IBM Model 2</a> (Chris Dyer, NAACL 2013)</p>

<p><a href="https://aclanthology.org/N13-1117.pdf">A Systematic Bayesian Treatment of the IBM Alignment Models</a> (Yarin Gal, NAACL 2013)</p>

<p><a href="https://aclanthology.org/P13-1017.pdf">Word Alignment Modeling with Context Dependent Deep Neural Network</a> (Nan Yang, ACL 2013)</p>

<p><a href="https://aclanthology.org/P14-1138.pdf">Recurrent Neural Networks for Word Alignment Model</a> (Akihiro Tamura, ACL 2014)</p>

<p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9570/9556">Contrastive Unsupervised Word Alignment with Non-Local Features</a> (Yang Liu, AAAI 2015)</p>

<p><a href="https://helda.helsinki.fi/bitstream/handle/10138/232817/Efficient_Word_Alignment_with_Markov_Chain_Monte_Carlo.pdf?sequence=1">Efficient Word Alignment with Markov Chain Monte Carlo</a> (Robert Östling, 2016, <a href="https://github.com/robertostling/efmaral">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1606.09560.pdf">Neural Network-based Word Alignment through Score Aggregation</a> (Joel Legrand, 2016)</p>

<p><a href="https://arxiv.org/pdf/1809.03985.pdf">On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation</a> (Tamer Alkhouli, 2018)</p>

<p><a href="https://arxiv.org/pdf/1909.02074.pdf">Jointly Learning to Align and Translate with Transformer Models</a> (Sarthak Garg, EMNLP 2019)</p>

<p><a href="https://aclanthology.org/2020.acl-main.146.pdf">End-to-end Neural Word Alignment Outperforms GIZA++</a> (Thomas Zenkel, ACL 2020)</p>

<p><a href="https://aclanthology.org/2020.emnlp-main.41.pdf">A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT</a> (Masaaki Nagata, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.14837.pdf">Accurate Word Alignment Induction from Neural Machine Translation</a> (Yun Chen, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.08728.pdf">SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings</a> (Masoud Jalili Sabet, EMNLP 2020 Findings)</p>

<p><a href="https://arxiv.org/pdf/2009.13116.pdf">Neural Baselines for Word Alignment</a> (Anh Khoa Ngo Ho, 2020)</p>

<p><a href="https://arxiv.org/pdf/2101.08231.pdf">Word Alignment by Fine-tuning Embeddings on Parallel Corpora</a> (Zi-Yi Dou, EACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2012.07162.pdf">MASK-ALIGN: Self-Supervised Neural Word Alignment</a> (Chi Chen, ACL 2021, <a href="https://github.com/THUNLP-MT/Mask-Align">code</a>)</p>

<p><a href="https://aclanthology.org/2021.acl-long.24.pdf">A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment</a> (Jingyi Zhang, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2107.06632.pdf">ParCourE: A Parallel Corpus Explorer for a Massively Multilingual Corpus</a> (Ayyoob Imani, ACL 2021 demo)</p>

<p><a href="https://arxiv.org/pdf/2109.06283.pdf">Graph Algorithms for Multiparallel Word Alignment</a> (Ayyoob Imani, EMNLP 2021)</p>

<p><a href="https://arxiv.org/pdf/2104.08721.pdf">Embedding-Enhanced Giza++: Improving Alignment in Low- and High-Resource Scenarios Using Embedding Space Geometry</a> (Kelly Marchisio, 2021)</p>

<p><a href="https://arxiv.org/pdf/2102.04009.pdf">SLUA: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning</a> (Di Wu, 2021)</p>

<h5 id="sentence-alignment">Sentence Alignment</h5>

<p><a href="https://aclanthology.org/P91-1022.pdf">Aligning Sentences in Parallel Corpora</a> (Peter F. Brown, ACL 1991)</p>

<p><a href="https://aclanthology.org/J93-1004.pdf">A Program for Aligning Sentences in Bilingual Corpora</a> (William A. Gale, CL 1993)</p>

<p><a href="https://dl.acm.org/doi/abs/10.5555/972450.972457">Text-Translation Alignment</a> (Martin Kay, 1993)</p>

<p><a href="https://dl.acm.org/doi/abs/10.5555/962367.962411">Using Cognates to Align Sentences in Bilingual Corpora</a> (Michel Simard, 1993)</p>

<p><a href="https://link.springer.com/article/10.1023/A:1008010319408">Bilingual Sentence Alignment: Balancing Robustness and Accuracy</a> (Michel Simard, 1998)</p>

<p><a href="https://link.springer.com/chapter/10.1007/3-540-45820-4_14">Fast and Accurate Sentence Alignment of Bilingual Corpora</a> (Robert C. Moore, 2002)</p>

<p><a href="https://aclanthology.org/O05-2005.pdf">Aligning Parallel Bilingual Corpora Statistically with Punctuation Criteria</a> (Thomas C. Chuang, 2005)</p>

<p><a href="https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/segmentation-and-alignment-of-parallel-text-for-statistical-machine-translation/697D66DD385601AA57ECE2E848899BF7">Segmentation and Alignment of Parallel Text for Statistical Machine Translation</a> (Yonggang Deng, 2007)</p>

<p><a href="http://lml.bas.bg/ranlp2007/DOCS/RANLP2007.pdf#page=595">Improved Sentence Alignment for Movie Subtitles</a> (Jörg Tiedemann, 2007)</p>

<p><a href="https://biblio.ugent.be/publication/434669">Linguistically-Based Sub-Sentential Alignment for Terminology Extraction from a Bilingual Automotive Corpus</a> (Lieve Macken, COLING 2008)</p>

<p><a href="https://aclanthology.org/C10-2010.pdf">Improved Unsupervised Sentence Alignment for Symmetrical and Asymmetrical Parallel Corpora</a> (Fabienne Braune, COLING 2010)</p>

<p><a href="https://aclanthology.org/C10-2081.pdf">Fast-Champollion: A Fast and Robust Sentence Alignment Algorithm</a> (Peng Li, COLING 2010)</p>

<p><a href="https://aclanthology.org/W11-4624.pdf">Iterative, MT-based Sentence Alignment of Parallel Texts</a> (Rico Sennrich, 2011)</p>

<p><a href="http://atour.iro.umontreal.ca/rali/sites/default/files/publis/MTSummit-2013-Fethi.pdf">Yet Another Fast, Robust and Open Source Sentence Aligner. Time to Reconsider Sentence Alignment?</a> (Fethi Lamraoui, 2013)</p>

<p><a href="https://arxiv.org/pdf/1608.05426.pdf">A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments</a> (Omer Levy, EACL 2017)</p>

<p><a href="https://aclanthology.org/W17-2512.pdf">Overview of the Second BUCC Shared Task: Spotting Parallel Sentences in Comparable Corpora</a> (Pierre Zweigenbaum, 2017)</p>

<p><a href="https://aclanthology.org/C18-1122.pdf">Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation</a> (Francis Gregoire, COLING 2018)</p>

<p><a href="https://aclanthology.org/P19-1309.pdf">Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings</a> (Mikel Artetxe, ACL 2019)</p>

<p><a href="https://aclanthology.org/D19-1136.pdf">Vecalign: Improved Sentence Alignment in Linear Time and Space</a> (Brian Thompson, EMNLP 2019, <a href="https://github.com/thompsonb/vecalign">code</a>)</p>

<p><a href="https://aclanthology.org/2020.acl-main.152.pdf">Parallel Sentence Mining by Constrained Decoding</a> (Pinzhen Chen, ACL 2020)</p>

<p><a href="https://aclanthology.org/2020.coling-main.418.pdf">SpanAlign: Sentence Alignment Method based on Cross-Language Span Prediction and ILP</a> (Katsuki Chousa, COLING 2020)</p>

<h5 id="document-alignment">Document Alignment</h5>

<p><a href="https://aclanthology.org/W16-2347.pdf">Findings of the WMT 2016 Bilingual Document Alignment Shared Task</a> (Christian Buck, WMT 2016)</p>

<h5 id="unsupervised-machine-translation">Unsupervised Machine Translation</h5>

<p><a href="https://arxiv.org/pdf/1710.11041.pdf">Unsupervised Neural Machine Translation</a> (Mikel Artetxe, ICLR 2018, <a href="https://github.com/artetxem/undreamt">code</a>, <a href="https://zhuanlan.zhihu.com/p/30649985">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1711.00043.pdf">Unsupervised Machine Translation Using Monolingual Corpora Only</a> (Guillaume Lample, ICLR 2018)</p>

<p><a href="https://arxiv.org/pdf/1804.07755.pdf">Phrase-Based &amp; Neural Unsupervised Machine Translation</a> (Guillaume Lample, EMNLP 2018, <a href="https://github.com/facebookresearch/UnsupervisedMT">code</a>, <a href="https://blog.csdn.net/ljp1919/article/details/103074097">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1902.01313.pdf">An Effective Approach to Unsupervised Machine Translation</a> (Mikel Artetxe, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/2004.10581.pdf">When and Why is Unsupervised Neural Machine Translation Useless?</a> (Yunsu Kim, EAMT 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.05516.pdf">When Does Unsupervised Machine Translation Work?</a> (Kelly Marchisio, WMT 2020)</p>

<h5 id="non-autoregressive-machine-translation">Non-Autoregressive Machine Translation</h5>

<p><a href="https://arxiv.org/pdf/1711.02281.pdf">Non-Autoregressive Neural Machine Translation</a> (Jiatao Gu, ICLR 2018, <a href="https://github.com/salesforce/nonauto-nmt">code</a>, <a href="https://zhuanlan.zhihu.com/p/35866317">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2011.00770.pdf">Context-Aware Cross-Attention for Non-Autoregressive Translation</a> (Liang Ding, COLING 2020)</p>

<p><a href="https://arxiv.org/pdf/2106.05093.pdf">Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation</a> (Cunxiao Du, ICML 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.00903.pdf">Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation</a> (Liang Ding, ACL 2021)</p>

<h5 id="low-resource-machine-translation">Low-Resource Machine Translation</h5>

<p><a href="https://arxiv.org/pdf/1611.04558.pdf">Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a> (Melvin Johnson, TACL 2017)</p>

<p><a href="https://arxiv.org/pdf/1808.08437.pdf">Meta-Learning for Low-Resource Neural Machine Translation</a> (Jiatao Gu, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1808.04189.pdf">Rapid Adaptation of Neural Machine Translation to New Languages</a> (Graham Neubig, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1906.03785.pdf">Generalized Data Augmentation for Low-Resource Translation</a> (Mengzhou Xia, ACL 2019, <a href="https://github.com/xiamengzhou/DataAugForLRL">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.11901.pdf">Revisiting Low-Resource Neural Machine Translation: A Case Study</a> (Rico Sennrich, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.00040.pdf">Handling Syntactic Divergence in Low-resource Machine Translation</a> (Chunting Zhou, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1902.01382.pdf">The FLoRes Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English</a> (Francisco Guzman, EMNLP 2019, <a href="https://github.com/facebookresearch/flores">code</a>)</p>

<p><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5341">Cross-Lingual Pre-Training Based Transfer for Zero-Shot Neural Machine Translation</a> (Baijun Ji, AAAI 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.11867.pdf">Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</a> (Biao Zhang, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2009.11201.pdf">Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages</a> (Xavier Garcia, NAACL 2021)</p>

<h5 id="multilingual-machine-translation">Multilingual Machine Translation</h5>

<p><a href="https://arxiv.org/pdf/1902.10461.pdf">Multilingual Neural Machine Translation with Knowledge Distillation</a> (Xu Tan, ICLR 2019, <a href="https://github.com/RayeRen/multilingual-kd-pytorch">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1902.03499.pdf">Multilingual Neural Machine Translation With Soft Decoupled Encoding</a> (Xinyi Wang, ICLR 2019, <a href="https://github.com/cindyxinyiwang/SDE">code</a>, <a href="https://zhuanlan.zhihu.com/p/60845246">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1903.00089.pdf">Massively Multilingual Neural Machine Translation</a> (Roee Aharoni, NAACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1905.08212.pdf">Target Conditioned Sampling: Optimizing Data Selection for Multilingual Neural Machine Translation</a> (Xinyi Wang, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1905.05475.pdf">Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies</a> (Yunsu Kim, ACL 2019, <a href="https://github.com/yunsukim86/sockeye-transfer">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.09324.pdf">Multilingual Neural Machine Translation with Language Clustering</a> (Xu Tan, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.09524.pdf">Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages</a> (Yunsu Kim, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1907.05019.pdf">Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges</a> (Naveen Arivazhagan, 2019)</p>

<p><a href="https://arxiv.org/pdf/2001.08210.pdf">Multilingual Denoising Pre-training for Neural Machine Translation</a> (Yinhan Liu, TACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.06748.pdf">Balancing Training for Multilingual Neural Machine Translation</a> (Xinyi Wang, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.10171.pdf">Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation</a> (Haipeng Sun, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.04816.pdf">Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation</a> (Aditya Siddhant, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.11125.pdf">Beyond English-Centric Multilingual Machine Translation</a> (Angela Fan, 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.07972.pdf">Explicit Alignment Objectives for Multilingual Bidirectional Encoders</a> (Junjie Hu, NAACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.01463.pdf">Lightweight Adapter Tuning for Multilingual Speech Translation</a> (Hang Le, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2105.09501.pdf">Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</a> (Xiao Pan, ACL 2021)</p>

<h5 id="multi-domain-machine-translation">Multi-Domain Machine Translation</h5>

<p><a href="https://aclanthology.org/D18-1041.pdf">Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination</a> (Jiali Zeng, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1806.00258.pdf">A Survey of Domain Adaptation for Neural Machine Translation</a> (Chenhui Chu, COLING 2018)</p>

<p><a href="https://arxiv.org/pdf/1904.03879.pdf">Improving Domain Adaptation Translation with Domain Invariant and Specific Information</a> (Shuhao Gu, NAACL 2019)</p>

<p><a href="https://aclanthology.org/N19-1209.pdf">Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation</a> (Brian Thompson, NAACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.00376.pdf">Domain Adaptation of Neural Machine Translation by Lexicon Induction</a> (Junjie Hu, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1912.07239.pdf">Iterative Dual Domain Adaptation for Neural Machine Translation</a> (EMNLP 2019)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6461">Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks</a> (Yong Wang, AAAI 2020)</p>

<p><a href="https://arxiv.org/pdf/1908.10940.pdf">Learning a Multi-Domain Curriculum for Neural Machine Translation</a> (Wei Wang, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/1911.02692.pdf">Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing</a> (Haoming Jiang, ACL 2020)</p>

<p><a href="https://aclanthology.org/2020.emnlp-main.364.pdf">Distilling Multiple Domains for Neural Machine Translation</a> (Anna Currey, EMNLP 2020)</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/8907409">Exploring Discriminative Word-Level Domain Contexts for Multi-Domain Neural Machine Translation</a> (Jinsong Su, TPAMI 2021)</p>

<p><a href="https://arxiv.org/pdf/2012.10586.pdf">Finding Sparse Structures for Domain Specific Neural Machine Translation</a> (Jianze Liang, AAAI 2021)</p>

<p><a href="https://arxiv.org/pdf/2103.13678.pdf">Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation</a> (Shuhao Gu, NAACL 2021)</p>

<h5 id="multi-modal-translation">Multi-Modal Translation</h5>

<p><a href="https://www.aclweb.org/anthology/P19-1653.pdf">Distilling Translations with Visual Awareness</a> (Julia Ive, ACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/P19-1642.pdf">Latent Variable Model for Multi-modal Translation</a> (Iacer Calixto, ACL 2019)</p>

<p><a href="https://www.aclweb.org/anthology/2020.acl-main.400.pdf">Multimodal Transformer for Multimodal Machine Translation</a> (Shaowei Yao, ACL 2020)</p>

<h5 id="tree-based-machine-translation">Tree-Based Machine Translation</h5>

<p><a href="http://www.aclweb.org/anthology/P17-1065">Sequence-to-Dependency Neural Machine Translation</a> (Shuangzhi Wu, ACL 2017, <a href="https://www.jianshu.com/p/2eb3c89234cb">note</a>)</p>

<h5 id="context-aware-machine-translation">Context-Aware Machine Translation</h5>

<p><a href="https://arxiv.org/pdf/1708.05943.pdf">Neural Machine Translation with Extended Context</a> (Jorg Tiedemann, EMNLP 2017 Workshop)</p>

<p><a href="https://arxiv.org/pdf/1805.10163.pdf">Context-Aware Neural Machine Translation Learns Anaphora Resolution</a> (Elena Voita, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1905.05979.pdf">When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion</a> (Elena Voita, ACL 2019)</p>

<p><a href="https://www.ijcai.org/proceedings/2021/0522.pdf">Improving Context-Aware Neural Machine Translation with Source-side Monolingual Documents</a> (Linqing Chen, IJCAI 2021)</p>

<p><a href="https://arxiv.org/pdf/2105.03482.pdf">Measuring and Increasing Context Usage in Context-Aware Machine Translation</a> (Patrick Fernandes, ACL 2021)</p>

<h5 id="simultaneous-machine-translation">Simultaneous Machine Translation</h5>

<p><a href="https://arxiv.org/pdf/1909.12406.pdf">Monotonic Multihead Attention</a> (Xutai Ma, 2019)</p>

<p><a href="https://aclanthology.org/2020.emnlp-main.178.pdf">Learning Adaptive Segmentation Policy for Simultaneous Translation</a> (Ruiqing Zhang, EMNLP 2020)</p>

<h5 id="interpretability">Interpretability</h5>

<p><a href="https://aclanthology.org/D16-1159.pdf">Does String-Based Neural MT Learn Source Syntax?</a> (Xing Shi, EMNLP 2016)</p>

<h5 id="robustness">Robustness</h5>

<p><a href="https://arxiv.org/pdf/1711.02173.pdf">Synthetic and Natural Noise Both Break Neural Machine Translation</a> (Yonatan Belinkov, ICLR 2018)</p>

<p><a href="https://arxiv.org/pdf/1805.06130.pdf">Towards Robust Neural Machine Translation</a> (Yong Cheng, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1906.02443.pdf">Robust Neural Machine Translation with Doubly Adversarial Inputs</a> (Yong Cheng, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/2005.00580.pdf">Evaluating Robustness to Input Perturbations for Neural Machine Translation</a> (Xing Liu, ACL 2020)</p>

<h4 id="multilinguality">Multilinguality</h4>

<h5 id="language-identification">Language Identification</h5>

<p><a href="https://aclanthology.org/D13-1084.pdf">Word Level Language Identification in Online Multilingual Communication</a> (Dong Nguyen, EMNLP 2013)</p>

<p><a href="https://arxiv.org/pdf/1804.08186.pdf">Automatic Language Identification in Texts: A Survey</a> (Tommi Jauhiainen, JAIR 2019)</p>

<h5 id="code-switching">Code Switching</h5>

<p><a href="https://aclanthology.org/W16-5801.pdf">Challenges of Computational Processing of Code-Switching</a> (Özlem Çetinoglu, 2016)</p>

<p><a href="https://arxiv.org/pdf/1809.01962.pdf">Code-switched Language Models Using Dual RNNs and Same-Source Pretraining</a> (Saurabh Garg, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1810.11895.pdf">Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training</a> (Hila Gonen, EMNLP 2019)</p>

<p><a href="https://aclanthology.org/2020.acl-main.80.pdf">Modeling Code-Switch Languages Using Bilingual Parallel Corpus</a> (Grandee Lee, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/1904.00784.pdf">A Survey of Code-switched Speech and Language Processing</a> (Sunayana Sitaram, 2020)</p>

<h5 id="language-diversity-and-evolution">Language Diversity and Evolution</h5>

<p><a href="https://dspace.mit.edu/handle/1721.1/62802">A Statistical Model for Lost Language Deciphermen</a> (Benjamin Snyder, ACL 2010)</p>

<p><a href="https://aclanthology.org/C10-1044.pdf">Comparing Language Similarity across Genetic and Typologically-Based Groupings</a> (Ryan Georgi, COLING 2010)</p>

<p><a href="https://aclanthology.org/P13-1021.pdf">Unsupervised Transcription of Historical Documents</a> (Taylor Berg-Kirkpatrick, ACL 2013)</p>

<p><a href="https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_1554621">The World Atlas of Language Structures Online</a> (Matthew S. Dryer, 2013, <a href="https://wals.info/">code</a>)</p>

<p><a href="https://aclanthology.org/N15-1109.pdf">Unsupervised Code-Switching for Multilingual Historical Document Transcription</a> (Dan Garrette, NAACL 2015)</p>

<p><a href="https://aclanthology.org/N16-1055.pdf">An Unsupervised Model of Orthographic Variation for Historical Document Transcription</a> (Dan Garrette, NAACL 2016)</p>

<p><a href="https://arxiv.org/pdf/1605.09096.pdf">Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change</a> (William L. Hamilton, ACL 2016, <a href="https://nlp.stanford.edu/projects/histwords/">code</a>)</p>

<p><a href="http://proceedings.mlr.press/v70/bamler17a/bamler17a.pdf">Dynamic Word Embeddings</a> (Robert Bamler, ICML 2017)</p>

<p><a href="https://arxiv.org/pdf/1707.09569.pdf">Learning Language Representations for Typology Prediction</a> (Chaitanya Malaviya, EMNLP 2017)</p>

<p><a href="https://ifarm.nl/clin2017st/">The CLIN27 Shared Task: Translating Historical Text to Contemporary Language for Improving Automatic Linguistic Annotation</a> (Erik Tjong Kim Sang, 2017)</p>

<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5452980/">Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change</a> (William L. Hamilton, 2017)</p>

<p><a href="https://arxiv.org/pdf/1806.03537.pdf">Diachronic Word Embeddings and Semantic Shifts: A Survey</a> (Andrey Kutuzov, COLING 2018)</p>

<p><a href="https://www.pnas.org/content/115/16/E3635.short">Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes</a> (Nikhil Garg, 2018)</p>

<p><a href="https://direct.mit.edu/coli/article/45/3/559/93372/Modeling-Language-Variation-and-Universals-A">Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing</a> (Edoardo Maria Ponti, CL 2019)</p>

<p><a href="https://arxiv.org/pdf/1904.02036.pdf">A Large-Scale Comparison of Historical Text Normalization Systems</a> (Marcel Bollmann, NAACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.06718.pdf">Neural Decipherment via Minimum-Cost Flow: from Ugaritic to Linear B</a> (Jiaming Luo, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1910.06262.pdf">Restoring Ancient Text Using Deep Learning: A Case Study on Greek Epigraphy</a> (Yannis Assael, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/2004.09095.pdf">The State and Fate of Linguistic Diversity and Inclusion in the NLP World</a> (Pratik Joshi, ACL 2020)</p>

<p><a href="https://www.cambridge.org/core/journals/natural-language-engineering/article/natural-language-processing-for-similar-languages-varieties-and-dialects-a-survey/229652C86E329F83346BB6C66B9521A6">Natural Language Processing for Similar Languages, Varieties, and Dialects: A Survey</a> (Marcos Zampieri, 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.12707.pdf">Learning to Recognize Dialect Features</a> (Dorottya Demszky, NAACL 2021)</p>

<p><a href="https://aclanthology.org/2021.acl-demo.3.pdf">The Classical Language Toolkit: An NLP Framework for Pre-Modern Languages</a> (Kyle P. Johnson, ACL 2021 Demo)</p>

<h5 id="multilingual-trainingcross-lingual-transfer">Multilingual Training/Cross-lingual Transfer</h5>

<p><a href="https://arxiv.org/pdf/1309.4168.pdf">Exploiting Similarities among Languages for Machine Translation</a> (Tomas Mikolov, 2013)</p>

<p><a href="https://aclanthology.org/P14-1066.pdf">Learning Continuous Phrase Representations for Translation Modeling</a> (Jianfeng Gao, ACL 2014)</p>

<p><a href="https://aclanthology.org/N15-1104.pdf">Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation</a> (Chao Xing, NAACL 2015)</p>

<p><a href="https://arxiv.org/pdf/1710.04087.pdf">Word Translation Without Parallel Data</a> (Alexis Conneau, ICLR 2018)</p>

<p><a href="https://arxiv.org/pdf/1805.03620.pdf">On the Limitations of Unsupervised Bilingual Dictionary Induction</a> (Anders Søgaard, ACL 2018)</p>

<p><a href="https://aclanthology.org/D18-1330.pdf">Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion</a> (Armand Joulin, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/1801.06126.pdf">Non-Adversarial Unsupervised Word Translation</a> (Yedid Hoshen, EMNLP 2018, <a href="https://github.com/facebookresearch/Non-adversarialTranslation">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1706.04902.pdf">A Survey of Cross-lingual Word Embedding Models</a> (Sebastian Ruder, JAIR 2019)</p>

<p><a href="https://arxiv.org/pdf/1902.00508.pdf">How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions</a> (Goran Glavaš, ACL 2019)</p>

<p><a href="https://aclanthology.org/P19-1301.pdf">Choosing Transfer Languages for Cross-Lingual Learning</a> (Yu-Hsiang Lin, ACL 2019, <a href="https://github.com/neulab/langrank">code</a>)</p>

<p><a href="https://papers.nips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf">Cross-lingual Language Model Pretraining</a> (Alexis Conneau, NeurIPS 2019, <a href="https://github.com/facebookresearch/XLM">code</a>)</p>

<p><a href="http://proceedings.mlr.press/v119/hu20b/hu20b.pdf">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation</a> (Junjie Hu, ICML 2020)</p>

<p><a href="https://aclanthology.org/2020.acl-main.747.pdf">Unsupervised Cross-lingual Representation Learning at Scale</a> (Alexis Conneau, ACL 2020, <a href="https://github.com/pytorch/fairseq">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2005.00052.pdf">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</a> (Jonas Pfeiffer, EMNLP 2020)</p>

<p><a href="https://aclanthology.org/2020.coling-main.531.pdf">Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction</a> (COLING 2020)</p>

<p><a href="https://arxiv.org/abs/2106.05469">Variational Information Bottleneck for Effective Low-Resource Fine-Tuning</a> (Rabeeh Karimi Mahabadi, ICLR 2021)</p>

<p><a href="https://arxiv.org/pdf/2012.15613.pdf">How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models</a> (Phillip Rust, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2012.15682.pdf">A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters</a> (Mengjie Zhao, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2103.10730.pdf">MuRIL: Multilingual Representations for Indian Languages</a> (Simran Khanuja, 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.16138.pdf">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA</a> (Zewen Chi, 2021)</p>

<p><a href="https://arxiv.org/pdf/2103.13275.pdf">When Word Embeddings Become Endangered</a> (Khalid Alnajjar, 2021)</p>

<h5 id="multilingualcross-lingual-application">Multilingual/Cross-lingual Application</h5>

<p><a href="https://aclanthology.org/W11-3217.pdf">Named Entity Transliteration Generation Leveraging Statistical Machine Translation Technology</a> (Pradeep Dasigi, 2011)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/2766462.2767752">Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings</a> (Ivan Vulić, SIGIR 2015)</p>

<p><a href="https://aclanthology.org/L18-1263.pdf">Creating a Translation Matrix of the Bible’s Names Across 591 Languages</a> (Winston Wu, LREC 2018)</p>

<p><a href="https://arxiv.org/pdf/2007.07683.pdf">UniTrans : Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data</a> (Qianhui Wu, IJCAI 2020)</p>

<p><a href="https://arxiv.org/pdf/2101.11112.pdf">Cross-Lingual Named Entity Recognition Using Parallel Corpus: A New Approach Using XLM-RoBERTa Alignment</a> (Bing Li, 2021)</p>

<h4 id="interpretability-in-natural-language-processing">Interpretability in Natural Language Processing</h4>

<p><a href="https://arxiv.org/pdf/1805.01070.pdf">What you can cram into a single vector: Probing sentence embeddings for linguistic properties</a> (Alexis Conneau, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1903.08855.pdf">Linguistic Knowledge and Transferability of Contextual Representations</a> (Nelson F. Liu, NAACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.04341.pdf">What Does BERT Look At? An Analysis of BERT’s Attention</a> (Kevin Clark, ACL 2019 Workshop)</p>

<p><a href="https://arxiv.org/pdf/1908.08593.pdf">Revealing the Dark Secrets of BERT</a> (Olga Kovaleva, EMNLP 2019)</p>

<p><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00324/96460/How-Can-We-Know-What-Language-Models-Know">How Can We Know What Language Models Know?</a> (Zhengbao Jiang, TACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.02015.pdf">Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection</a> (Hanjie Chen, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.15980.pdf">AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a> (Taylor Shin, 2020)</p>

<p><a href="https://arxiv.org/pdf/2011.04946.pdf">When Do You Need Billions of Words of Pretraining Data?</a> (Yian Zhang, 2020)</p>

<p><a href="https://arxiv.org/pdf/2102.12871.pdf">SparseBERT: Rethinking the Importance Analysis in Self-attention</a> (Han Shi, ICML 2021)</p>

<p><a href="https://arxiv.org/pdf/2101.00403.pdf">Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words</a> (Valentin Hofmann, ACL 2021)</p>

<h4 id="fairness-in-natural-language-processing">Fairness in Natural Language Processing</h4>

<p><a href="https://arxiv.org/pdf/1809.01496.pdf">Learning Gender-Neutral Word Embeddings</a> (Jieyu Zhao, EMNLP 2018)</p>

<p><a href="https://arxiv.org/pdf/2004.09456.pdf">StereoSet: Measuring Stereotypical Bias in Pretrained Language Models</a> (Moin Nadeem, 2020)</p>

<h3 id="computer-vision">Computer Vision</h3>

<h4 id="feature-detector-and-descriptor">Feature Detector and Descriptor</h4>

<p><a href="https://philpapers.org/rec/MORTTA-3">Towards Automatic Visual Obstacle Avoidance</a> (Hans P. Moravec, 1977)</p>

<p><a href="https://ieeexplore.ieee.org/document/4767851">A Computational Approach to Edge Detection</a> (John Canny, TPAMI 1986)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&amp;rep=rep1&amp;type=pdf">A Combined Corner and Edge Detector</a> (Chris Harris, 1988)</p>

<p><a href="https://ieeexplore.ieee.org/document/56205">Scale-Space and Edge Detection Using Anisotropic Diffusion</a> (Pietro Perona, TPAMI 1990)</p>

<p><a href="https://link.springer.com/article/10.1023/A:1007963824710">SUSAN—A New Approach to Low Level Image Processing</a> (Stephen M. Smith, 1997)</p>

<p><a href="https://www.cs.ubc.ca/~lowe/papers/iccv99.pdf">Object Recognition from Local Scale-Invariant Features</a> (David G. Lowe, ICCV 1999)</p>

<p><a href="http://vision.stanford.edu/teaching/cs231b_spring1415/papers/lbp.pdf">Multiresolution Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns</a> (Timo Ojala, TPAMI 2002)</p>

<p><a href="https://www.sciencedirect.com/science/article/pii/S0262885604000435">Robust Wide Baseline Stereo from Maximally Stable Extremal Regions</a> (Jiri Matas, BMVC 2002)</p>

<p><a href="https://www.sciencedirect.com/science/article/pii/S0262885603001379">Image Registration Methods: A Survey</a> (Barbara Zitova, 2003)</p>

<p><a href="https://people.eecs.berkeley.edu/~malik/cs294/lowe-ijcv04.pdf">Distinctive Image Features from Scale-Invariant Keypoints</a> (David G. Lowe, IJCV 2004)</p>

<p><a href="https://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/vibes_ijcv2004.pdf">A Comparison of Affine Region Detectors</a> (Krystian Mikolajczyk, IJCV 2004)</p>

<p><a href="https://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/mikolajczyk_pami2004.pdf">A Performance Evaluation of Local Descriptors</a> (Krystian Mikolajczyk, TPAMI 2005)</p>

<p><a href="https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">Histograms of Oriented Gradients for Human Detection</a> (Navneet Dalal, CVPR 2005)</p>

<p><a href="https://link.springer.com/chapter/10.1007/11744023_34">Machine Learning for High-Speed Corner Detection</a> (Edward Rosten, ECCV 2006)</p>

<p><a href="https://link.springer.com/chapter/10.1007/11744023_32">SURF: Speeded Up Robust Features</a> (Herbert Bay, ECCV 2006)</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8">CenSurE: Center Surround Extremas for Realtime Feature Detection and Matching</a> (Motilal Agrawal, ECCV 2008)</p>

<p><a href="https://homes.esat.kuleuven.be/~tuytelaa/FT_survey_interestpoints08.pdf">Local Invariant Feature Detectors: A Survey</a> (Tinne Tuytelaars, 2008)</p>

<p><a href="http://matthewalunbrown.com/papers/pami2010.pdf">Discriminative Learning of Local Image Descriptors</a> (Matthew Brown, TPAMI 2010)</p>

<p><a href="https://ieeexplore.ieee.org/document/4815264">DAISY: An Efficient Dense Descriptor Applied to Wide-Baseline Stereo</a> (Engin Tola, TPAMI 2010)</p>

<p><a href="https://ieeexplore.ieee.org/document/6126544">ORB: An efficient alternative to SIFT or SURF</a> (Ethan Rublee, ICCV 2011)</p>

<p><a href="https://arxiv.org/pdf/1603.09114.pdf">LIFT: Learned Invariant Feature Transform</a> (Kwang Moo Yi, ECCV 2016)</p>

<h4 id="vision-representation">Vision Representation</h4>

<p><a href="https://arxiv.org/pdf/2106.08254.pdf">BEiT: BERT Pre-Training of Image Transformers</a> (Hanbo Bao, 2021)</p>

<p><a href="https://arxiv.org/pdf/2103.00020.pdf">Learning Transferable Visual Models From Natural Language Supervision</a> (Alec Radford, 2021)</p>

<h4 id="object-detection">Object Detection</h4>

<p><a href="https://ieeexplore.ieee.org/abstract/document/990517">Rapid object detection using a boosted cascade of simple features</a> (CVPR 2001, Paul Viola)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03500C970796A39BCC5A437E4AA79B10?doi=10.1.1.165.9750&amp;rep=rep1&amp;type=pdf">Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer</a> (Christoph H. Lampert, CVPR 2009, <a href="https://github.com/ahmedmazariML/Learning-To-Detect-Unseen-Object-Classes-by-Between-Class-Attribute-Transfer">code</a>, <a href="https://tongtianta.site/paper/23631">note</a>)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=CD1A6E4145A750B305F3512006D12FE9?doi=10.1.1.466.176&amp;rep=rep1&amp;type=pdf">DeViSE: A Deep Visual-Semantic Embedding Model</a> (Andrea Frome, NeurIPS 2013, <a href="https://zhuanlan.zhihu.com/p/52352455">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1311.2524.pdf">Rich feature hierarchies for accurate object detection and semantic segmentation</a> (Ross Girshick, CVPR 2014, <a href="https://github.com/rbgirshick/rcnn">code</a>, <a href="https://zhuanlan.zhihu.com/p/47579399">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1504.08083.pdf">Fast Region-based Convolutional Networks for object detection</a> (Ross Girshick, ICCV 2015, <a href="https://github.com/rbgirshick/fast-rcnn">code</a>, <a href="https://zhuanlan.zhihu.com/p/47579399">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a> (Shaoqing Ren, NeurIPS 2015, <a href="https://github.com/rbgirshick/py-faster-rcnn">code</a>, <a href="https://zhuanlan.zhihu.com/p/47579399">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1605.06409.pdf">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a> (Jifeng Dai, NeurIPS 2016, <a href="https://github.com/daijifeng001/R-FCN">code</a>, <a href="https://zhuanlan.zhihu.com/p/30867916">note</a>)</p>

<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf">You Only Look Once: Unified, Real-Time Object Detection</a> (Joseph Redmon, CVPR 2016, <a href="https://github.com/gliese581gg/YOLO_tensorflow">code</a>, <a href="https://zhuanlan.zhihu.com/p/32525231">note</a>)</p>

<p><a href="http://www.cs.unc.edu/~cyfu/pubs/ssd.pdf">SSD: Single Shot MultiBox Detector</a> (Wei Liu, ECCV 2016, <a href="https://github.com/balancap/SSD-Tensorflow">code</a>, <a href="https://zhuanlan.zhihu.com/p/33544892">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1612.08242.pdf">YOLO9000: Better, Faster, Stronger</a> (Joseph Redmon, CVPR 2017, <a href="https://github.com/experiencor/keras-yolo2">code</a>, <a href="https://zhuanlan.zhihu.com/p/25052190">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN</a> (Kaiming He, ICCV 2017, <a href="https://github.com/facebookresearch/Detectron">code</a>, <a href="https://zhuanlan.zhihu.com/p/47579399">note</a>)</p>

<p><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">YOLOv3: An Incremental Improvement</a> (Joseph Redmon, 2018, <a href="https://github.com/qqwweee/keras-yolo3">code</a>, <a href="https://zhuanlan.zhihu.com/p/76802514">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1903.12174.pdf">TensorMask: A Foundation for Dense Object Segmentation</a> (Xinlei Chen, 2019, <a href="https://zhuanlan.zhihu.com/p/60984659">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2004.10934.pdf">YOLOv4: Optimal Speed and Accuracy of Object Detection</a> (Alexey Bochkovskiy, 2020, <a href="https://github.com/AlexeyAB/darknet?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2010.15831.pdf">RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</a> (Cheng Chi, NeurIPS 2020)</p>

<p><a href="https://arxiv.org/pdf/1911.09070v3.pdf">EfficientDet: Scalable and Efficient Object Detection</a> (Mingxing Tan, CVPR 2020)</p>

<h4 id="semantic-segmentation">Semantic Segmentation</h4>

<p><a href="https://arxiv.org/pdf/1411.4038.pdf">Fully Convolutional Networks for Semantic Segmentation</a> (Jonathan Long, CVPR 2015, <a href="https://github.com/anoushkrit/Knowledge?utm_source=catalyzex.com">code</a>, <a href="https://blog.csdn.net/qq_36269513/article/details/80420363">note</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html">Dual Attention Network for Scene Segmentation</a> (Jun Fu, CVPR 2019)</p>

<h4 id="image-super-resolution">Image Super-Resolution</h4>

<p><a href="https://arxiv.org/pdf/1501.00092.pdf">Image Super-Resolution Using Deep Convolutional Networks</a> (Chao Dong, TPAMI 2015)</p>

<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a> (Wenzhe Shi, CVPR 2016, <a href="https://github.com/leftthomas/ESPCN">code</a>, <a href="https://zhuanlan.zhihu.com/p/76338220">note</a>)</p>

<p><a href="https://cv.snu.ac.kr/research/VDSR/VDSR_CVPR2016.pdf">Accurate Image Super-Resolution Using Very Deep Convolutional Networks</a> (Jiwon Kim, CVPR 2016, <a href="https://github.com/Jongchan/tensorflow-vdsr">code</a>, <a href="https://blog.csdn.net/shwan_ma/article/details/78193777">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1608.00367.pdf">Accelerating the Super-Resolution Convolutional Neural Network</a> (Chao Dong, ECCV 2016, <a href="https://github.com/Saafke/FSRCNN_Tensorflow">code</a>, <a href="https://blog.csdn.net/shwan_ma/article/details/78171649">note</a>)</p>

<p><a href="https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a> (Justin Johnson, ECCV 2016, <a href="https://github.com/yusuketomoto/chainer-fast-neuralstyle">code</a>, <a href="https://blog.csdn.net/kid_14_12/article/details/85871965">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1606.08921.pdf">Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections</a> (Xiao-Jiao Mao, 2016, <a href="https://github.com/ved27/RED-net">code</a>, <a href="https://blog.csdn.net/happyday_d/article/details/85239395">note</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf">Image Super-Resolution Using Dense Skip Connections</a> (Tong Tong, ICCV 2017, <a href="https://github.com/kweisamx/TensorFlow-SR-DenseNet">code</a>, <a href="https://blog.csdn.net/happyday_d/article/details/85461715">note</a>)</p>

<p><a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf">Image Super-Resolution via Deep Recursive Residual Network</a> (Tai Yang, CVPR 2017, <a href="https://github.com/tyshiwo/DRRN_CVPR17">code</a>, <a href="https://blog.csdn.net/wangkun1340378/article/details/74542166">note</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a> (Christian Ledig, CVPR 2017, <a href="https://github.com/tensorlayer/srgan">code</a>, <a href="https://www.cnblogs.com/wangxiaocvpr/p/5989802.html">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1702.00783.pdf">Pixel Recursive Super Resolution</a> (Ryan Dahl, 2017, <a href="https://github.com/nilboy/pixel-recursive-super-resolution">code</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Haris_Deep_Back-Projection_Networks_CVPR_2018_paper.pdf">Deep Back-Projection Networks for Super-Resolution</a> (Muhammad Haris, CVPR 2018, <a href="https://github.com/alterzero/DBPN-Pytorch">code</a>, <a href="https://zhuanlan.zhihu.com/p/34400207">note</a>)</p>

<h4 id="person-re-identification">Person Re-identification</h4>

<p><a href="https://arxiv.org/pdf/1904.07223.pdf">Joint Discriminative and Generative Learning for Person Re-identification</a> (Zhedong Zheng, CVPR 2019, <a href="https://github.com/NVlabs/DG-Net?utm_source=catalyzex.com">code</a>)</p>

<h3 id="machine-learning">Machine Learning</h3>

<h4 id="decision-tree">Decision Tree</h4>

<p><a href="http://xxpt.ynjgy.com/resource/data/110102/U/705/pdfs/L3ClassTrees.pdf">Classification and Regression Trees</a> (L. Breiman, 1984, <a href="https://github.com/bensadeghi/DecisionTree.jl">code1</a>, <a href="https://github.com/wreardan/cart">code2</a>, <a href="https://www.cnblogs.com/wxquare/p/5379970.html">note</a>)</p>

<p><a href="http://hunch.net/~coms-4771/quinlan.pdf">Induction of Decision Trees</a> (J. Ross Quinlan, 1986, <a href="https://github.com/igrigorik/decisiontree">code</a>, <a href="https://www.cnblogs.com/wxquare/p/5379970.html">note</a>)</p>

<p><a href="https://dl.acm.org/citation.cfm?id=152181">C4.5: Programs for Machine Learning</a> (J. Ross Quinlan, 1993, <a href="https://github.com/yandongliu/learningjs">code</a>, <a href="https://www.cnblogs.com/wxquare/p/5379970.html">note</a>)</p>

<h4 id="support-vector-machine">Support Vector Machine</h4>

<p><a href="http://www.svms.org/training/BOGV92.pdf">A Training Algorithm for Optimal Margin Classifiers</a> (Bernhard E Boser, 1992, <a href="https://github.com/cjlin1/libsvm">code</a>)</p>

<p><a href="http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf">Support-Vector Networks</a> (Corinna Cortes, 1995, <a href="https://github.com/cjlin1/libsvm">code</a>)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.9423&amp;rep=rep1&amp;type=pdf">Estimating the Support of a High-Dimensional Distribution</a> (Bernhard Schölkopf, 1999, <a href="https://github.com/cjlin1/libsvm">code</a>)</p>

<p><a href="https://www.researchgate.net/publication/12413257_New_Support_Vector_Algorithms">New Support Vector Algorithms</a> (Bernhard Schölkopf, 2000, <a href="https://github.com/cjlin1/libsvm">code</a>)</p>

<h4 id="conditional-random-field">Conditional Random Field</h4>

<p><a href="https://arxiv.org/pdf/1011.4088.pdf">An Introduction to Conditional Random Fields</a> (Charles Sutton, 2010, <a href="https://github.com/timvieira/crf">code</a>, <a href="https://zhuanlan.zhihu.com/p/70067113">note</a>)</p>

<h4 id="expectation-maximization">Expectation Maximization</h4>

<p><a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600.x">Maximum likelihood from incomplete data via the EM algorithm</a> (Arthur Dempster, 1977, <a href="https://zhuanlan.zhihu.com/p/40991784">note</a>)</p>

<p><a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470191613">The EM Algorithm and Extensions</a> (Geoff McLachlan, 1997)</p>

<h4 id="ensemble-method">Ensemble Method</h4>

<p><a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451">Greedy function approximation: a gradient boosting machine</a> (Jerome H. Friedman, 2001, <a href="https://github.com/dmlc/xgboost">code</a>, <a href="https://www.jianshu.com/p/005a4e6ac775">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1603.02754.pdf#page=10&amp;zoom=100,0,198">XGBoost: A Scalable Tree Boosting System</a> (Tianqi Chen, SIGKDD 2016, <a href="https://github.com/dmlc/xgboost">code</a>, <a href="http://djjowfy.com/2017/08/01/XGBoost%E7%9A%84%E5%8E%9F%E7%90%86/">note</a>)</p>

<h4 id="learning-theory">Learning Theory</h4>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-319-21852-6_3">On the Uniform Convergence of Relative Frequencies of Events to their Probabilities</a> (Vladimir Vapnik, 1971)</p>

<p><a href="https://dl.acm.org/doi/pdf/10.1145/1968.1972?casa_token=jkM2-kt89ncAAAAA:Eavindse5LyhN1lfVXUQHJej6wwPne4TV_AASPF6bkwDveiydsl6fpGD4prUdXEpYCoeF7kN4RfaElE">A Theory of the Learnable</a> (Leslie Valiant, 1984)</p>

<p><a href="https://users.soe.ucsc.edu/~manfred/pubs/J9.pdf">Occam’s Razor</a> (Anselm Blumer, 1987, <a href="https://zh.wikipedia.org/zh-hans/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80">note</a>)</p>

<h4 id="imbalanced-data">Imbalanced Data</h4>

<p><a href="https://arxiv.org/pdf/1106.1813.pdf">SMOTE: Synthetic Minority Over-sampling Technique</a> (Nitesh V. Chawla, 2002, <a href="https://github.com/scikit-learn-contrib/imbalanced-learn">code</a>, <a href="https://blog.csdn.net/shine19930820/article/details/54143241">note</a>)</p>

<p><a href="http://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf?attredirects=0">kNN approach to unbalanced data distributions: A case study involving information extraction</a> (Jianping Zhang, 2003, <a href="http://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf?attredirects=0">code</a>)</p>

<p><a href="https://pdfs.semanticscholar.org/c1a9/5197e15fa99f55cd0cb2ee14d2f02699a919.pdf">Balancing Training Data for Automated Annotation of Keywords: a Case Study</a> (Gustavo E. A. P. A. Batista, 2003, <a href="https://github.com/scikit-learn-contrib/imbalanced-learn">code</a>)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.7757&amp;rep=rep1&amp;type=pdf">A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data</a> (Ronaldo C. Prati, 2004, <a href="https://github.com/scikit-learn-contrib/imbalanced-learn">code</a>)</p>

<p><a href="https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf">Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning</a> (Hui Han, 2005, <a href="https://blog.csdn.net/shine19930820/article/details/54143241">note</a>)</p>

<p><a href="https://sci2s.ugr.es/keel/pdf/algorithm/congreso/2008-He-ieee.pdf">ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning</a> (Haibo He, 2008, <a href="https://github.com/scikit-learn-contrib/imbalanced-learn">code</a>, <a href="https://blog.csdn.net/weixin_40118768/article/details/80226423">note</a>)</p>

<p><a href="https://sci2s.ugr.es/keel/pdf/algorithm/congreso/2009-Bunkhumpornpat-LNCS.pdf">Safe-Level-SMOTE: Safe-Level-Synthetic Minority Over-Sampling Technique for Handling the Class Imbalanced Problem</a> (Chumphol Bunkhumpornpat, 2009)</p>

<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128907">Learning from Imbalanced Data</a> (Haibo He, 2009, <a href="https://blog.csdn.net/shine19930820/article/details/54143241">note</a>)</p>

<h4 id="multi-task-learning">Multi-Task Learning</h4>

<p><a href="https://www.di.ens.fr/~fbach/skm_icml.pdf">Multiple Kernel Learning, Conic Duality, and the SMO Algorithm</a> (Francis R. Bach, ICML 2004)</p>

<p><a href="http://www.jmlr.org/papers/volume7/sonnenburg06a/sonnenburg06a.pdf">Large Scale Multiple Kernel Learning</a> (Sören Sonnenburg, JMLR 2006)</p>

<p><a href="https://papers.nips.cc/paper/3953-factorized-latent-spaces-with-structured-sparsity.pdf">Factorized Latent Spaces with Structured Sparsity</a> (Yangqing Jia, NeurIPS 2010)</p>

<p><a href="https://ttic.uchicago.edu/~rurtasun/publications/SalzmannEkUrtasunDarrell10.pdf">Factorized Orthogonal Latent Spaces</a> (Mathieu Salzmann, 2010)</p>

<p><a href="https://papers.nips.cc/paper/6254-domain-separation-networks.pdf">Domain Separation Networks</a> (Konstantinos Bousmalis, NeurIPS 2016, <a href="https://zhuanlan.zhihu.com/p/49479734">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1901.11504.pdf">Multi-Task Deep Neural Networks for Natural Language Understanding</a> (Xiaodong Liu, ACL 2019, <a href="https://github.com/namisan/mt-dnn">code</a>, <a href="https://zhuanlan.zhihu.com/p/60282783">note</a>)</p>

<h3 id="deep-learning">Deep Learning</h3>

<h4 id="artificial-neural-network">Artificial Neural Network</h4>

<p><a href="https://link.springer.com/article/10.1007%252FBF02478259">A logical Calculus of Ideas Immanent in Nervous Activity</a> (Warren McCulloch, 1943)</p>

<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a> (Frank Rosenblatt, 1958)</p>

<p><a href="https://babel.hathitrust.org/cgi/pt?id=mdp.39015039846566&amp;view=1up&amp;seq=8">Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</a> (Frank Rosenblatt, 1961)</p>

<p><a href="https://isl.anthropomatik.kit.edu/downloads/PhonemeRecognitionUsingTimeDelayNeuralNetworks_NEU(1).pdf">Phoneme Recognition Using Time-Delay Neural Networks</a> (Alexander Waibel, 1989)</p>

<h4 id="convolutional-neural-network">Convolutional Neural Network</h4>

<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/">Receptive Fields, Binocular Interaction and Functional Architecture in the Cat’s Visual Cortex</a> (David Hunter Hubel, 1962)</p>

<p><a href="https://www.ics.uci.edu/~welling/teaching/273ASpring09/lecun-89e.pdf">Backpropagation Applied to Handwritten Zip Code Recognition</a> (Yann LeCun, 1989, <a href="https://blog.csdn.net/u012679707/article/details/80738633">note</a>)</p>

<p><a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">Gradient-Based Learning Applied to Document Recognition</a> (Yann LeCun, 1998, <a href="https://blog.csdn.net/sunshine_010/article/details/79876255">note</a>)</p>

<p><a href="http://202.116.81.74/cache/7/03/cogprints.org/036a03bc6027afc65c14907d0a1fae73/cnn_tutorial.pdf">Notes on Convolutional Neural Networks</a> (Jake Bouvrie, 2006, <a href="https://blog.csdn.net/langb2014/article/details/48470181">note</a>)</p>

<p><a href="https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a> (Alex Krizhevsky, NeurIPS 2012, <a href="https://zhuanlan.zhihu.com/p/20324656">note</a>)</p>

<p><a href="https://liris.cnrs.fr/Documents/Liris-5659.pdf">Simplifying ConvNets for Fast Learning</a> (Franck Mamalet, 2012)</p>

<p><a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">Visualizing and Understanding Convolutional Networks</a> (Matthew D. Zeiler, ECCV 2014, <a href="https://www.zybuluo.com/lutingting/note/459569">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1403.1687.pdf">Rigid-Motion Scattering for Texture Classification</a> (Laurent Sifre, 2014)</p>

<p><a href="https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf">Going Deeper with Convolutions</a> (Christian Szegedy, CVPR 2015, <a href="https://blog.csdn.net/lhanchao/article/details/55804968">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for Large-Scale Image Recognition</a> (Karen Simonyan, ICLR 2015, <a href="https://zhuanlan.zhihu.com/p/32853559">note</a>)</p>

<p><a href="http://de.arxiv.org/pdf/1505.00387">Highway Networks</a> (Rupesh Kumar Srivastava, 2015, <a href="https://zhuanlan.zhihu.com/p/38130339">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1511.07122.pdf">Multi-Scale Context Aggregation by Dilated Convolutions</a> (Fisher Yu, ICLR 2016, <a href="https://github.com/iesl/dilated-cnn-ner">code</a>, <a href="https://www.cnblogs.com/fourmi/p/10049998.html">note</a>)</p>

<p><a href="https://x-algo.cn/wp-content/uploads/2016/12/residual.pdf">Deep Residual Learning for Image Recognition</a> (Kaiming He, CVPR 2016, <a href="https://zhuanlan.zhihu.com/p/47199669">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1512.00567.pdf">Rethinking the Inception Architecture for Computer Vision</a> (Christian Szegedy, CVPR 2016, <a href="https://github.com/pytorch/vision?utm_source=catalyzex.com">code</a>, <a href="https://zhuanlan.zhihu.com/p/50751422">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1603.09382.pdf">Deep Networks with Stochastic Depth</a> (Gao Huang, ECCV 2016)</p>

<p><a href="https://arxiv.org/pdf/1603.08029.pdf">Resnet in Resnet: Generalizing Residual Architectures</a> (Sasha Targ, ICLR 2016 Workshop)</p>

<p><a href="https://arxiv.org/pdf/1605.07146.pdf">Wide Residual Networks</a> (Sergey Zagoruyko, BMVC 2016)</p>

<p><a href="https://arxiv.org/pdf/1602.07261.pdf">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a> (Christian Szegedy, AAAI 2017)</p>

<p><a href="http://www.cs.cmu.edu/~jeanoh/16-785/papers/huang-cvpr2017-densenet.pdf">Densely Connected Convolutional Networks</a> (Gao Huang, CVPR 2017, <a href="https://github.com/liuzhuang13/DenseNet">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1611.05431.pdf">Aggregated Residual Transformations for Deep Neural Networks</a> (Saining Xie, CVPR 2017)</p>

<p><a href="https://arxiv.org/pdf/1610.02357.pdf">Xception: Deep Learning with Depthwise Separable Convolutions</a> (Francois Chollet, CVPR 2017)</p>

<p><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Ioannou_Deep_Roots_Improving_CVPR_2017_paper.pdf">Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups</a> (Yani Ioannou, CVPR 2017)</p>

<p><a href="https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w10/Wang_Factorized_Convolutional_Neural_ICCV_2017_paper.pdf">Factorized Convolutional Neural Networks</a> (Min Wang, ICCV 2017)</p>

<p><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.pdf">Deformable Convolutional Networks</a> (Jifeng Dai, ICCV 2017)</p>

<p><a href="https://arxiv.org/pdf/1707.09855.pdf">Convolution with Logarithmic Filter Groups for Efficient Shallow CNN</a> (Tae Kwan Lee, 2017)</p>

<p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">Squeeze-and-Excitation Networks</a> (Jie Hu, CVPR 2018)</p>

<p><a href="https://arxiv.org/pdf/1802.05800.pdf">Tree-CNN: A Deep Convolutional Neural Network for Lifelong Learning</a> (Deboleena Roy, 2018, <a href="https://github.com/magical2world/tensorflow-Tree-CNN">code</a>, <a href="https://blog.csdn.net/qq_24305433/article/details/79856672">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a> (Mingxing Tan, ICML 2019, <a href="https://github.com/qubvel/efficientnet">code</a>, <a href="https://zhuanlan.zhihu.com/p/70369784">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1908.03888.pdf">HBONet: Harmonious Bottleneck on Two Orthogonal Dimensions</a> (Duo Li, ICCV 2019, <a href="https://github.com/d-li14/HBONet">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2104.00298.pdf">EfficientNetV2: Smaller Models and Faster Training</a> (Mingxing Tan, 2021)</p>

<h4 id="recurrent-neural-network">Recurrent Neural Network</h4>

<p><a href="http://xueshu.baidu.com/s?wd=paperuri%3A%28051bcc198724a1da0b831afe39380852%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D1">Long Short-Term memory</a> (Sepp Hochreiter, 1997)</p>

<p><a href="https://arxiv.org/abs/1409.2329">Recurrent Neural Network Regularization</a> (Wojciech Zaremba, ICLR 2015)</p>

<p><a href="http://pdfs.semanticscholar.org/0651/b333c2669227b0cc42de403268a4546ece70.pdf">A Critical Review of Recurrent Neural Networks for Sequence Learning</a> (Zachary C. Lipton, 2015, <a href="https://blog.csdn.net/xizero00/article/details/51225065">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1603.08983.pdf">Adaptive Computation Time for Recurrent Neural Networks</a> (Alex Graves, 2016, <a href="https://blog.csdn.net/liuyuemaicha/article/details/53999091">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1708.02182.pdf">Regularizing and Optimizing LSTM Language Models</a> (Stephen Merity, 2017, <a href="https://ldzhangyx.github.io/2019/07/31/awd-lstm/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1709.02755.pdf">Simple Recurrent Units for Highly Parallelizable Recurrence</a> (Tao Lei, EMNLP 2018, <a href="https://github.com/taolei87/sru">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1708.06834.pdf">Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks</a> (Victor Campos, ICLR 2018, <a href="https://www.jianshu.com/p/5c4dd629b1ec">note</a>)</p>

<h4 id="generative-adversarial-networks">Generative Adversarial Networks</h4>

<p><a href="https://arxiv.org/pdf/1406.2661.pdf">Generative Adversarial Nets</a> (Ian Goodfellow, 2014)</p>

<p><a href="https://arxiv.org/pdf/1411.1784.pdf">Conditional Generative Adversarial Nets</a> (Mehdi Mirza, 2014, <a href="https://zhuanlan.zhihu.com/p/23648795">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1511.06434.pdf">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a> (Alec Radford, ICLR 2016, <a href="https://www.cnblogs.com/wangxiaocvpr/p/5965434.html">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1701.00160.pdf">NeurIPS 2016 Tutorial: Generative Adversarial Networks</a> (Ian Goodfellow, NeurIPS 2016, <a href="https://blog.csdn.net/cskywit/article/details/86612142">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1610.09585.pdf">Conditional Image Synthesis with Auxiliary Classifier GANs</a> (Augustus Odena, 2016, <a href="https://blog.csdn.net/qq_24477135/article/details/85758496">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1606.03498.pdf">Improved Techniques for Training GANs</a> (Tim Salimans, 2016, <a href="https://blog.csdn.net/zijin0802034/article/details/58643889">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1710.07035.pdf">Generative Adversarial Networks: An Overview</a> (Antonia Creswell, 2017)</p>

<p><a href="https://arxiv.org/pdf/1711.05914.pdf">How Generative Adversarial Nets and its variants Work: An Overview of GAN</a> (Yongjun Hong, 2017)</p>

<p><a href="https://arxiv.org/pdf/1611.07004.pdf">Image-to-Image Translation with Conditional Adversarial Networks</a> (Phillip Isola, CVPR 2017, <a href="https://github.com/phillipi/pix2pix">code</a>, <a href="https://blog.csdn.net/Teeyohuang/article/details/82699781">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1612.03242.pdf">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a> (Han Zhang, ICCV 2017, <a href="https://github.com/hanzhanggit/StackGAN">code</a>, <a href="https://blog.csdn.net/a312863063/article/details/83574422">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1703.10593.pdf">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a> (Jun-Yan Zhu, ICCV 2017, <a href="https://github.com/junyanz/CycleGAN">code</a>, <a href="https://blog.csdn.net/hhy_csdn/article/details/82913776">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1711.11585.pdf">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a>  (Ting-Chun Wang, CVPR 2018, <a href="https://github.com/NVIDIA/pix2pixHD">code</a>, <a href="https://zhuanlan.zhihu.com/p/35955531">note</a>)</p>

<p><a href="https://www.arxiv-vanity.com/papers/1612.07828/">Learning from Simulated and Unsupervised Images through Adversarial Training</a> (Ashish Shrivastava, ICCV 2017, <a href="https://github.com/carpedm20/simulated-unsupervised-tensorflow">code</a>, <a href="https://blog.csdn.net/daydayjump/article/details/81977479">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1701.07875.pdf">Wasserstein GAN</a> (Martin Arjovsky, 2017, <a href="https://github.com/martinarjovsky/WassersteinGAN">code</a>, <a href="https://zhuanlan.zhihu.com/p/25071913">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1710.10196.pdf">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a> (Tero Karras, ICLR 2018, <a href="https://github.com/tkarras/progressive_growing_of_gans">code</a>, <a href="https://zhuanlan.zhihu.com/p/30637133">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.01677.pdf">Transferring GANs: Generating Images from Limited Data</a> (Yaxing Wang, ECCV 2018, <a href="https://github.com/yaxingwang/Transferring-GANs">code</a>, <a href="https://medium.com/@xiaosean5408/transferring-gans%E7%B0%A1%E4%BB%8B-transferring-gans-generating-images-from-limited-data-90bcf6be7fd2">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1811.11212.pdf">Self-Supervised GANs via Auxiliary Rotation Loss</a> (Ting Chen, CVPR 2019, <a href="https://github.com/vandit15/Self-Supervised-Gans-Pytorch">code</a>, <a href="https://blog.csdn.net/weixin_44363205/article/details/104918734">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1812.04948.pdf">A Style-Based Generator Architecture for Generative Adversarial Networks</a> (Tero Karras, CVPR 2019, <a href="https://github.com/NVlabs/stylegan">code</a>, <a href="https://blog.csdn.net/weixin_42360095/article/details/89522153">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.11096.pdf">Large Scale GAN Training for High Fidelity Natural Image Synthesis</a> (Andrew Brock, ICLR 2019, <a href="https://zhuanlan.zhihu.com/p/46581611">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1805.08318.pdf">Self-Attention Generative Adversarial Networks</a> (Han Zhang, ICML 2019, <a href="https://github.com/heykeetae/Self-Attention-GAN">code</a>, <a href="https://www.paperweekly.site/papers/notes/414">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1912.04958.pdf">Analyzing and Improving the Image Quality of StyleGAN</a> (Tero Karras, CVPR 2020, <a href="https://blog.csdn.net/lynlindasy/article/details/104495583">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2103.01209.pdf">Generative Adversarial Transformers</a> (Drew A. Hudson, ICML 2021)</p>

<h4 id="autoencoder">Autoencoder</h4>

<p><a href="https://link.springer.com/article/10.1007/BF00332918">Auto-Association by Multilayer Perceptrons and Singular Value Decomposition</a> (Herve Bourland, 1988)</p>

<p><a href="https://www.cs.toronto.edu/~hinton/science.pdf">Reducing the Dimensionality of Data with Neural Networks</a> (Geoffrey Hinton, 2006)</p>

<p><a href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf">Extracting and Composing Robust Features with Denoising Autoencoders</a> (Pascal Vincent, ICML 2008)</p>

<p><a href="https://icml.cc/2011/papers/455_icmlpaper.pdf">Contractive Auto-Encoders: Explicit Invariance During Feature Extraction</a> (Salah Rifai, ICML 2011)</p>

<p><a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf">Sparse Autoencoder</a> (Andrew Ng, 2011)</p>

<h5 id="variational-autoencoder">Variational Autoencoder</h5>

<p><a href="https://arxiv.org/pdf/1312.6114.pdf">Auto-Encoding Variational Bayes</a> (Diederik P. Kingma, ICLR 2014)</p>

<p><a href="http://proceedings.mlr.press/v37/rezende15.pdf">Variational Inference with Normalizing Flows</a> (Danilo Jimenez Rezende, ICML 2015, <a href="https://bingning.wang/research/Article/?id=100">note</a>)</p>

<p><a href="https://papers.nips.cc/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf">Learning Structured Output Representation using Deep Conditional Generative Models</a> (Kihyuk Sohn, NeurIPS 2015, <a href="https://zhuanlan.zhihu.com/p/25518643">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1606.04934.pdf">Improved Variational Inference with Inverse Autoregressive Flow</a> (Diederik P. Kingma, NeurIPS 2016)</p>

<p><a href="https://arxiv.org/pdf/1611.07308.pdf">Variational Graph AutoEncoders</a> (Thomas N. Kipf, NeurIPS 2016 Workshop)</p>

<p><a href="https://openreview.net/pdf?id=Sy2fzU9gl">beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</a> (Irina Higgins, ICLR 2017)</p>

<p><a href="https://arxiv.org/abs/1612.00410">Deep Variational Information Bottleneck</a> (Alexander A. Alemi, ICLR 2017)</p>

<p><a href="https://arxiv.org/pdf/1611.02731.pdf">Variational Lossy Autoencoder</a> (Xi Chen, ICLR 2017)</p>

<p><a href="https://arxiv.org/pdf/1711.00937.pdf">Neural Discrete Representation Learning</a> (Aaron van den Oord, NeurIPS 2017)</p>

<p><a href="http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf">Adversarially Regularized Autoencoders</a> (Junbo Zhao, ICML 2018)</p>

<p><a href="http://proceedings.mlr.press/v80/kim18b/kim18b.pdf">Disentangling by Factorising</a> (Hyunjik Kim, ICML 2018)</p>

<p><a href="https://www.ijcai.org/proceedings/2018/0362.pdf">Adversarially Regularized Graph Autoencoder for Graph Embedding</a> (Shirui Pan, IJCAI 2018)</p>

<p><a href="https://arxiv.org/pdf/1802.04942.pdf">Isolating Sources of Disentanglement in VAEs</a> (Ricky T. Q. Chen, NeurIPS 2018)</p>

<p><a href="https://arxiv.org/pdf/1804.00104.pdf">Learning Disentangled Joint Continuous and Discrete Representations</a> (Emilien Dupont, NeurIPS 2018)</p>

<p><a href="https://arxiv.org/pdf/1802.03480.pdf">GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders</a> (Martin Simonovsky, 2018)</p>

<p><a href="http://proceedings.mlr.press/v89/esmaeili19a/esmaeili19a.pdf">Structured Disentangled Representations</a> (Babak Esmaeili, AISTATS 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.00446.pdf">Generating Diverse High-Fidelity Images with VQ-VAE-2</a> (Ali Razavi, NeurIPS 2019)</p>

<p><a href="http://proceedings.mlr.press/v108/khemakhem20a/khemakhem20a.pdf">Variational Autoencoders and Nonlinear ICA: A Unifying Framework</a> (Ilyes Khemakhem, AISTATS 2020)</p>

<p><a href="https://arxiv.org/abs/1903.12436">From Variational to Deterministic Autoencoders</a> (Partha Ghosh, ICLR 2020)</p>

<h4 id="graph-neural-network">Graph Neural Network</h4>

<p><a href="http://persagen.com/files/misc/scarselli2009graph.pdf">The Graph Neural Network Model</a> (Franco Scarselli, 2009, <a href="https://zhuanlan.zhihu.com/p/76290138">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-Supervised Classification with Graph Convolutional Networks</a> (Thomas N. Kipf, ICLR 2017, <a href="https://github.com/tkipf/gcn">code</a>, <a href="https://zhuanlan.zhihu.com/p/35630785">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1710.10903.pdf">Graph Attention Networks</a> (Petar Veličković, ICLR 2018, <a href="https://github.com/PetarV-/GAT">code</a>, <a href="https://zhuanlan.zhihu.com/p/34232818">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1901.00596.pdf">A Comprehensive Survey on Graph Neural Networks</a> (Zonghan Wu, TNNLS 2019)</p>

<p><a href="https://arxiv.org/pdf/2106.06090.pdf">Graph Neural Networks for Natural Language Processing: A Survey</a> (Lingfei Wu, 2021)</p>

<h4 id="capsule-network">Capsule Network</h4>

<p><a href="https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf">Dynamic Routing Between Capsules</a> (Geoffrey Hinton, NeurIPS 2017, <a href="https://github.com/naturomics/CapsNet-Tensorflow">code</a>, <a href="https://zhuanlan.zhihu.com/p/32156167">note</a>)</p>

<p><a href="https://openreview.net/pdf?id=HJWLfGWRb">Matrix Capsules with EM Routing</a> (Geoffrey Hinton, ICLR 2018)</p>

<h4 id="attention-mechanism">Attention Mechanism</h4>

<p><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a> (Dzmitry Bahdanau, ICLR 2015, <a href="https://blog.csdn.net/WUTab/article/details/73657905">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1709.04696.pdf">DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</a> (Tao Shen, 2017, <a href="https://github.com/taoshen58/DiSAN">code</a>, <a href="https://zhuanlan.zhihu.com/p/36349043">note</a>)</p>

<p><a href="https://www.aclweb.org/anthology/D17-1036.pdf">Learning What’s Easy: Fully Differentiable Neural Easy-First Taggers</a> (Andre F. T. Martins, EMNLP 2017)</p>

<p><a href="https://arxiv.org/pdf/1702.00887.pdf">Structured Attention Networks</a> (Yoon Kim, ICLR 2017)</p>

<p><a href="https://arxiv.org/pdf/1810.13409.pdf">You May Not Need Attention</a> (Ofir Press, 2018, <a href="https://github.com/ofirpress/YouMayNotNeedAttention">code</a>, <a href="https://zhuanlan.zhihu.com/p/48374997">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1811.05544.pdf">An Introductory Survey on Attention Mechanisms in NLP Problems</a> (Dichao Hu, 2018, <a href="https://blog.csdn.net/cskywit/article/details/84753293">note</a>)</p>

<h5 id="memory-network">Memory Network</h5>

<p><a href="https://arxiv.org/pdf/1410.5401.pdf">Neural Turing Machines</a> (Alex Graves, 2014, <a href="https://github.com/carpedm20/NTM-tensorflow">code</a>, <a href="https://zhuanlan.zhihu.com/p/30383994">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1410.3916v11.pdf">Memory Networks</a> (Jason Weston, ICLR 2015, <a href="https://zhuanlan.zhihu.com/p/32257642">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1503.08895v5.pdf">End-To-End Memory Networks</a> (Sainbayar Sukhbaatar, 2015, <a href="https://zhuanlan.zhihu.com/p/32257642">note</a>)</p>

<p><a href="http://www.thespermwhale.com/jaseweston/ram/papers/paper_21.pdf">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a> (Ankit Kumar, 2015, <a href="https://zhuanlan.zhihu.com/p/32257642">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1603.01417.pdf">Dynamic Memory Networks for Visual and Textual Question Answering</a> (Caiming Xiong, 2016, <a href="https://github.com/ethancaballero/Improved-Dynamic-Memory-Networks-DMN-plus">code</a>, <a href="https://zhuanlan.zhihu.com/p/32257642">note</a>)</p>

<p><a href="http://www.aclweb.org/anthology/E/E17/E17-1001.pdf">Gated End-to-End Memory Networks</a> (Fei Liu, 2016, <a href="https://zhuanlan.zhihu.com/p/30722242">note</a>)</p>

<h5 id="transformer">Transformer</h5>

<p><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a> (Ashish Vaswani, 2017, <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">code1</a>, <a href="https://github.com/Kyubyong/transformer">code2</a>, <a href="https://github.com/bojone/attention">code3</a>, <a href="https://zhuanlan.zhihu.com/p/48508221">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1803.02155.pdf">Self-Attention with Relative Position Representations</a> (Peter Shaw, 2018, <a href="https://www.jianshu.com/p/cb5b2d967e90">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1811.04716.pdf">Input Combination Strategies for Multi-Source Transformer Decoder</a> (Jindrich Libovicky, WMT 2018)</p>

<p><a href="https://arxiv.org/pdf/1905.09418.pdf">Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a> (Elena Voita, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1807.03819.pdf">Universal Transformer</a> (Mostafa Dehghani, ICLR 2019, <a href="https://github.com/andreamad8/Universal-Transformer-Pytorch">code</a>, <a href="https://zhuanlan.zhihu.com/p/44655133">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1905.07799.pdf">Adaptive Attention Span in Transformers</a> (Sainbayar Sukhbaatar, ACL 2019)</p>

<p><a href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> (Zihang Dai, ACL 2019, <a href="https://zhuanlan.zhihu.com/p/70745925">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1909.00015.pdf">Adaptively Sparse Transformers</a> (Goncalo M. Correia, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.00383.pdf">Self-Attention with Structural Position Representations</a> (Xing Wang, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1909.06639.pdf">Tree Transformer: Integrating Tree Structures into Self-Attention</a> (Yau-Shian Wang, EMNLP 2019)</p>

<p><a href="https://arxiv.org/pdf/1902.09113.pdf">Star-Transformer</a> (Qipeng Guo, 2019, <a href="https://zhuanlan.zhihu.com/p/97888995">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2001.04451.pdf">Reformer: The Efficient Transformer</a> (Nikita Kitaev, ICLR 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.00979.pdf">How Does Selective Mechanism Improve Self-Attention Networks?</a> (Xinwei Geng, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.06420.pdf">The Unstoppable Rise of Computational Linguistics in Deep Learning</a> (James Henderson, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2004.13310.pdf">Self-Attention with Cross-Lingual Position Representation</a> (Liang Ding, ACL 2020)</p>

<p><a href="https://arxiv.org/pdf/2006.04862.pdf">O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers</a> (Chulhee Yun, NeurIPS 2020)</p>

<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf">ETC: Encoding Long and Structured Data in Transformers</a> (Joshua Ainslie, EMNLP 2020)</p>

<p><a href="https://arxiv.org/pdf/2010.02648.pdf">On the Sub-layer Functionalities of Transformer Decoder</a> (Yilin Yang, EMNLP 2020 Findings)</p>

<p><a href="https://arxiv.org/pdf/2004.05150.pdf">Longformer: The Long-Document Transformer</a> (Iz Beltagy, 2020, <a href="https://zhuanlan.zhihu.com/p/134748587">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2006.04768.pdf">Linformer: Self-Attention with Linear Complexity</a> (Sinong Wang, 2020)</p>

<p><a href="https://openreview.net/forum?id=bK-rJMKrOsm">Multi-Head Attention: Collaborate Instead of Concatenate</a> (Jean-Baptiste Cordonnier, 2020, <a href="https://github.com/epfml/collaborative-attention">code</a>, <a href="https://medium.com/im%E6%97%A5%E8%A8%98/%E8%AB%96%E6%96%87%E5%88%86%E4%BA%AB-multi-head-attention-collaborate-instead-of-concatenate-196dccff6118">note</a>)</p>

<p><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00353/97776/Efficient-Content-Based-Sparse-Attention-with">Efficient Content-Based Sparse Attention with Routing Transformers</a> (Aurko Roy, TACL 2021)</p>

<p><a href="http://proceedings.mlr.press/v139/tay21a/tay21a.pdf">Synthesizer: Rethinking Self-Attention for Transformer Models</a> (Yi Tay, ICML 2021, <a href="https://zhuanlan.zhihu.com/p/148054019">note</a>)</p>

<p><a href="http://proceedings.mlr.press/v139/liutkus21a/liutkus21a.pdf">Relative Positional Encoding for Transformers with Linear Complexity</a> (Antoine Liutkus, ICML 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.01950.pdf">The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models</a> (Ulme Wennberg, ACL 2021)</p>

<p><a href="https://arxiv.org/pdf/2104.04692.pdf">Not All Attention Is All You Need</a> (Hongqiu Wu, 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.06295.pdf">Going Beyond Linear Transformers with Recurrent Fast Weight Programmers</a> (Kazuki Irie, 2021)</p>

<h5 id="sparse-attention">Sparse Attention</h5>

<p><a href="http://proceedings.mlr.press/v48/martins16.pdf">From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification</a> (Andre F. T. Martins, ICML 2016, <a href="https://github.com/KrisKorrel/sparsemax-pytorch">code</a>, <a href="https://www.cs.utah.edu/~tli/posts/2019/01/blog-post-1/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1705.07704.pdf">A Regularized Framework for Sparse and Structured Neural Attention</a> (Vlad Niculae, NeurIPS 2017)</p>

<p><a href="https://arxiv.org/pdf/1805.08241.pdf">Sparse and Constrained Attention for Neural Machine Translation</a> (Chaitanya Malaviya, ACL 2018)</p>

<p><a href="https://arxiv.org/pdf/1810.11975.pdf">On Controllable Sparse Alternatives to Softmax</a> (Anirban Laha, NeurIPS 2018)</p>

<p><a href="https://arxiv.org/pdf/1905.05702.pdf">Sparse Sequence-to-Sequence Models</a> (Ben Peters, ACL 2019, <a href="https://zhuanlan.zhihu.com/p/76607614">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2106.01087.pdf">Is Sparse Attention more Interpretable?</a> (Clara Meister, ACL 2021)</p>

<p><a href="https://arxiv.org/abs/2104.07012">Sparse Attention with Linear Units</a> (Biao Zhang, 2021)</p>

<h4 id="optimization">Optimization</h4>

<p><a href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf">Learning representations by back-propagating errors</a> (David E. Rumelhart, 1986)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.5612&amp;rep=rep1&amp;type=pdf">On the Momentum Term in Gradient Descent Learning</a> (Ning Qian, 1999)</p>

<p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a> (John Duchi, JMLR 2011)</p>

<p><a href="https://ml.informatik.uni-freiburg.de/papers/11-LION5-SMAC.pdf">Sequential Model-Based Optimization for General Algorithm Configuration</a> (Frank Hutter, LION 2011)</p>

<p><a href="https://arxiv.org/pdf/1212.5701.pdf">ADADELTA: An Adaptive Learning Rate Method</a> (Matthew D. Zeiler, 2012, <a href="https://zh.d2l.ai/chapter_optimization/adadelta.html">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1412.6980.pdf">ADAM: A Method for Stochastic Optimization</a> (Diederik P. Kingma, 2015, <a href="https://www.jianshu.com/p/aebcaf8af76e">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1609.04747.pdf">An overview of gradient descent optimization algorithms</a> (Sebastian Ruder, 2017, <a href="https://zhuanlan.zhihu.com/p/21539419">note</a>)</p>

<h4 id="weight-initialization">Weight Initialization</h4>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.2059&amp;rep=rep1&amp;type=pdf">Understanding the difficulty of training deep feedforward neural networks</a> (Xavier Glorot, JMLR 2010, <a href="https://zhuanlan.zhihu.com/p/43840797">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1502.01852.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a> (Kaiming He, ICCV 2015, <a href="https://www.cnblogs.com/everyday-haoguo/p/Note-PRelu.html">note</a>)</p>

<h4 id="loss-function">Loss Function</h4>

<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf">FaceNet: A Unified Embedding for Face Recognition and Clustering</a> (Florian Schroff, CVPR 2015, <a href="https://github.com/davidsandberg/facenet">code</a>, <a href="https://blog.csdn.net/chenriwei2/article/details/45031677">note</a>)</p>

<p><a href="http://www.eccv2016.org/files/posters/P-3B-20.pdf">A Discriminative Feature Learning Approach for Deep Face Recognition</a> (Yandong Wen, ECCV 2016, <a href="https://github.com/pangyupo/mxnet_center_loss">code</a>, <a href="https://blog.csdn.net/oJiMoDeYe12345/article/details/78548663">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1511.05042.pdf">An exploration of softmax alternatives belonging to the spherical loss family</a> (Alexandre de Brebisson, ICLR 2016)</p>

<p><a href="https://arxiv.org/pdf/1612.02295.pdf">Large-Margin Softmax Loss for Convolutional Neural Networks</a> (Weiyang Liu, ICML 2016, <a href="https://github.com/wy1iu/LargeMargin_Softmax_Loss">code</a>, <a href="https://zhuanlan.zhihu.com/p/45448909">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1708.02002.pdf">Focal Loss for Dense Object Detection</a> (Tsung-Yi Lin, ICCV 2017, <a href="https://github.com/unsky/focal-loss">code</a>, <a href="https://zhuanlan.zhihu.com/p/49981234">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1704.08063.pdf">SphereFace: Deep Hypersphere Embedding for Face Recognition</a> (Weiyang Liu, ICML 2017, <a href="https://github.com/wy1iu/sphereface">code</a>, <a href="https://zhuanlan.zhihu.com/p/45448909">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1801.09414.pdf">CosFace: Large Margin Cosine Loss for Deep Face Recognition</a> (Hao Wang, 2018, <a href="https://github.com/yule-li/CosFace">code</a>, <a href="https://zhuanlan.zhihu.com/p/59736735">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1801.07698.pdf">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a> (Jiankang Deng, 2018, <a href="https://github.com/deepinsight/insightface">code</a>, <a href="https://zhuanlan.zhihu.com/p/76541084">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1801.05599.pdf">Additive Margin Softmax for Face Verification</a> (Feng Wang, 2018, <a href="https://github.com/Joker316701882/Additive-Margin-Softmax">code</a>, <a href="https://blog.csdn.net/shaoxiaohu1/article/details/79139039">note</a>)</p>

<p><a href="http://papers.nips.cc/paper/7371-dropmax-adaptive-variational-softmax.pdf">DropMax: Adaptive Variational Softmax</a> (Hae Beom Lee, NeurIPS 2018)</p>

<h4 id="activation-function">Activation Function</h4>

<p><a href="https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf">Rectified Linear Units Improve Restricted Boltzmann Machines</a> (Vinod Nair, ICML 2010)</p>

<p><a href="https://arxiv.org/pdf/1505.00853.pdf">Empirical Evaluation of Rectified Activations in Convolution Network</a> (Bing Xu, 2015)</p>

<p><a href="https://arxiv.org/pdf/1603.05201v2.pdf">Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units</a> (Wenling Shang, ICML 2016, <a href="https://blog.csdn.net/cv_family_z/article/details/52399921">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1606.08415.pdf">Gaussian Error Linear Units (GELUs)</a> (Dan Hendrycks, 2016, <a href="https://github.com/hendrycks/GELUs">code</a>, <a href="https://zhuanlan.zhihu.com/p/100175788">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1611.01144">Categorical Reparameterization with Gumbel-Softmax</a> (Eric Jang, ICLR 2017, <a href="https://zhuanlan.zhihu.com/p/50065712">note</a>)</p>

<h4 id="normalization">Normalization</h4>

<p><a href="http://de.arxiv.org/pdf/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> (Sergey Ioffe, ICML 2015, <a href="https://zhuanlan.zhihu.com/p/50444499">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a> (Jimmy Lei Ba, 2016, <a href="https://zhuanlan.zhihu.com/p/54530247">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1602.07868.pdf">Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</a> (Tim Salimans, 2016)</p>

<p><a href="https://arxiv.org/pdf/1607.08022.pdf">Instance Normalization: The Missing Ingredient for Fast Stylization</a> (Dmitry Ulyanov, 2017)</p>

<p><a href="https://arxiv.org/pdf/1803.08494.pdf">Group Normalization</a> (Yuxin Wu, 2018, <a href="https://zhuanlan.zhihu.com/p/35005794">note</a>)</p>

<h4 id="regularization">Regularization</h4>

<p><a href="http://proceedings.mlr.press/v139/liu21f/liu21f.pdf">Just Train Twice: Improving Group Robustness without Training Group Information</a> (Evan Zheran Liu, ICML 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.14448.pdf">R-Drop: Regularized Dropout for Neural Networks</a> (Xiaobo Liang, 2021)</p>

<h4 id="visualization">Visualization</h4>

<p><a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbclid=IwA">Visualizing Data using t-SNE</a> (Laurens van der Maaten, JMLR 2008)</p>

<h4 id="reinforcement-learning">Reinforcement Learning</h4>

<p><a href="https://arxiv.org/pdf/1708.05866.pdf">A Brief Survey of Deep Reinforcement Learning</a> (Kai Arulkumaran, 2017, <a href="https://blog.csdn.net/KyrieHe/article/details/79504481">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1701.07274.pdf">Deep Reinforcement Learning: An Overview</a> (Yuxi Li, 2017, <a href="https://zhuanlan.zhihu.com/p/31595581">note</a>)</p>

<h5 id="reinforcement-learning-application">Reinforcement Learning Application</h5>

<p><a href="https://arxiv.org/pdf/1312.5602.pdf">Playing Atari with Deep Reinforcement Learning</a> (Volodymyr Mnih, 2013, <a href="https://zhuanlan.zhihu.com/p/33962867">note</a>)</p>

<p><a href="http://www.worlduc.com/FileSystem/1/134755/1585588/ac5b78a1934c49bb93a1f3ad09d96e46.pdf">Mastering the Game of Go with Deep Neural Networks and Tree Search</a> (David Silver, Nature 2016)</p>

<p><a href="https://skatgame.net/mburo/ps/alphago-zero.pdf">Mastering the Game of Go without Human Knowledge</a> (David Silver, Nature 2017, <a href="https://github.com/gcp/leela-zero">code</a>, <a href="https://zhuanlan.zhihu.com/p/101669366">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1712.01815.pdf">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a> (David Silver, 2017)</p>

<p><a href="https://arxiv.org/pdf/2106.06135.pdf">DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning</a> (Daochen Zha, ICML 2021)</p>

<h4 id="self-supervised-learning">Self-Supervised Learning</h4>

<p><a href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">Dimensionality Reduction by Learning an Invariant Mapping</a> (Raia Hadsell, CVPR 2006)</p>

<p><a href="http://proceedings.mlr.press/v48/oord16.pdf">Pixel Recurrent Neural Networks</a> (Aaron van den Oord, ICML 2016)</p>

<p><a href="https://arxiv.org/pdf/1606.05328.pdf">Conditional Image Generation with PixelCNN Decoders</a> (Aaron van den Oord, NeurIPS 2016)</p>

<p><a href="https://papers.nips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</a> (Kihyuk Sohn, NeurIPS 2016)</p>

<p><a href="https://arxiv.org/pdf/1807.03748.pdf">Representation Learning with Contrastive Predictive Coding</a> (Aaron van den Oord, 2018)</p>

<p><a href="https://arxiv.org/pdf/1808.06670.pdf">Learning Deep Representations by Mutual Information Estimation and Maximization</a> (R Devon Hjelm, ICLR 2019)</p>

<p><a href="https://arxiv.org/pdf/1906.00910.pdf">Learning Representations by Maximizing Mutual Information Across Views</a> (Philip Bachman, 2019, <a href="https://github.com/Philip-Bachman/amdim-public">code</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.05849.pdf">Contrastive Multiview Coding</a> (Yonglong Tian, 2019)</p>

<p><a href="https://arxiv.org/pdf/1911.05722.pdf">Momentum Contrast for Unsupervised Visual Representation Learning</a> (Kaiming He, CVPR 2020)</p>

<p><a href="https://arxiv.org/pdf/2002.05709.pdf">A Simple Framework for Contrastive Learning of Visual Representations</a> (Ting Chen, ICML 2020)</p>

<p><a href="https://arxiv.org/pdf/2006.09882.pdf">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</a> (Mathilde Caron, NeurIPS 2020)</p>

<p><a href="https://arxiv.org/pdf/2003.04297.pdf">Improved Baselines with Momentum Contrastive Learning</a> (Xinlei Chen, 2020)</p>

<p><a href="https://arxiv.org/pdf/2011.10566.pdf">Exploring Simple Siamese Representation Learning</a> (Xinlei Chen, 2020, <a href="https://zhuanlan.zhihu.com/p/331678807">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2006.07733.pdf">Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</a> (Jean-Bastien Gril, 2020, <a href="https://github.com/lucidrains/byol-pytorch">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2006.08218.pdf">Self-supervised Learning: Generative or Contrastive</a> (Xiao Liu, TKDE 2021)</p>

<p><a href="https://arxiv.org/pdf/2103.03230.pdf">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</a> (Jure Zbontar, ICML 2021, <a href="https://github.com/facebookresearch/barlowtwins">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2102.06810.pdf">Understanding self-supervised Learning Dynamics without Contrastive Pairs</a> (Yuandong Tian, ICML 2021)</p>

<p><a href="https://arxiv.org/pdf/2104.02057.pdf">An Empirical Study of Training Self-Supervised Vision Transformers</a> (Xinlei Chen, 2021)</p>

<h4 id="incrementalcontinuallifelong-learning">Incremental/Continual/Lifelong Learning</h4>

<p><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf">iCaRL: Incremental Classifier and Representation Learning</a> (Sylvestre-Alvise Rebuffi, CVPR 2017)</p>

<p><a href="https://arxiv.org/abs/1706.08840">Gradient Episodic Memory for Continual Learning</a> (David Lopez-Paz, NeurIPS 2017)</p>

<p><a href="https://arxiv.org/pdf/1705.08690.pdf">Continual Learning with Deep Generative Replay</a> (Hanul Shin, NeurIPS 2017)</p>

<p><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Francisco_M._Castro_End-to-End_Incremental_Learning_ECCV_2018_paper.pdf">End-to-End Incremental Learning</a> (Francisco M. Castro, ECCV 2018)</p>

<p><a href="https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf">Lifelong Machine Learning</a> (Zhiyuan Chen, 2018)</p>

<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Large_Scale_Incremental_Learning_CVPR_2019_paper.html">Large Scale Incremental Learning</a> (Yue Wu, CVPR 2019)</p>

<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Hou_Learning_a_Unified_Classifier_Incrementally_via_Rebalancing_CVPR_2019_paper.html">Learning a Unified Classifier Incrementally via Rebalancing</a> (Saihui Hou, CVPR 2019)</p>

<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019300231">Continual Lifelong Learning with Neural Networks: A Review</a> (German I. Parisi, 2019)</p>

<p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Mnemonics_Training_Multi-Class_Incremental_Learning_Without_Forgetting_CVPR_2020_paper.pdf">Mnemonics Training: Multi-Class Incremental Learning without Forgetting</a> (Yaoyao Liu, CVPR 2020)</p>

<h4 id="zero-shot-learning">Zero-Shot Learning</h4>

<p><a href="http://www.cs.cmu.edu/afs/cs/project/theo-73/www/papers/zero-shot-learning.pdf">Zero-Shot Learning with Semantic Output Codes</a> (Mark Palatucci, NeurIPS 2009, <a href="https://zhuanlan.zhihu.com/p/34076480">note</a>)</p>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=94AE59377DBC233D6EE1808F16B505CB?doi=10.1.1.371.9746&amp;rep=rep1&amp;type=pdf">Label-Embedding for Attribute-Based Classification</a> (Zeynep Akata, CVPR 2013, <a href="https://blog.csdn.net/hanss2/article/details/80537356">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1703.05002.pdf">Zero-Shot Recognition using Dual Visual-Semantic Mapping Paths</a> (Yanan Li, CVPR 2017, <a href="https://zhuanlan.zhihu.com/p/29392845">note</a>)</p>

<h4 id="few-shot-learning">Few-Shot Learning</h4>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.3.9021&amp;rep=rep1&amp;type=pdf">Learning from one example through shared densities on transform</a> (Erik G Miller, CVPR 2000)</p>

<p><a href="http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf">One-Shot Learning of Object Categories</a> (Li Fei-Fei, PAMI 2006, <a href="https://blog.csdn.net/sinat_36594453/article/details/89817314">note</a>)</p>

<p><a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf">Siamese neural networks for one-shot image recognition</a> (Gregory Koch, ICML 2015 Workshop, <a href="https://zhuanlan.zhihu.com/p/86283037">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1606.04080.pdf">Matching Networks for One Shot Learning</a> (Oriol Vinyals, NeurIPS 2016, <a href="https://zhuanlan.zhihu.com/p/32101204">note</a>)</p>

<h4 id="meta-learning">Meta Learning</h4>

<p><a href="https://arxiv.org/pdf/1703.03400.pdf">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> (Chelsea Finn, ICML 2017, <a href="https://github.com/cbfinn/maml?utm_source=catalyzex.com">code</a>, <a href="https://zhuanlan.zhihu.com/p/57864886">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1810.03548.pdf">Meta-Learning: A Survey</a> (Joaquin Vanschoren, 2018)</p>

<p><a href="https://arxiv.org/pdf/1905.12588.pdf">Meta-Learning Representations for Continual Learning</a> (Khurram Javed, NeurIPS 2019, <a href="https://github.com/khurramjaved96/mrcl?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://arxiv.org/pdf/2002.09571.pdf">Learning to Continually Learn</a> (Shawn Beaulieu, ECAI 2020, <a href="https://github.com/uvm-neurobotics-lab/ANML?utm_source=catalyzex.com">code</a>)</p>

<h4 id="curriculum-learning">Curriculum Learning</h4>

<p><a href="https://mila.quebec/wp-content/uploads/2019/08/2009_curriculum_icml.pdf">Curriculum Learning</a> (Yoshua Bengio, ICML 2009, <a href="https://zhuanlan.zhihu.com/p/114825029">note</a>)</p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/9608">Self-Paced Curriculum Learning</a> (Lu Jiang, AAAI 2015)</p>

<p><a href="http://proceedings.mlr.press/v70/graves17a/graves17a.pdf">Automated Curriculum Learning for Neural Networks</a> (Alex Graves, ICML 2017)</p>

<h4 id="federated-learning">Federated Learning</h4>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3298981">Federated Machine Learning: Concept and Applications</a> (Qiang Yang, 2019)</p>

<p><a href="https://arxiv.org/abs/1912.04977">Advances and Open Problems in Federated Learning</a> (Peter Kairouz, 2019)</p>

<p><a href="https://arxiv.org/pdf/1902.01046.pdf">Towards Federated Learning at Scale: System Design</a> (Keith Bonawitz, 2019)</p>

<p><a href="https://arxiv.org/pdf/1901.08755.pdf">SecureBoost: A Lossless Federated Learning Framework</a> (Kewei Cheng, 2019)</p>

<p><a href="https://proceedings.neurips.cc/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf">Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach</a> (Alireza Fallah, NeurIPS 2020)</p>

<p><a href="https://arxiv.org/pdf/2104.14628.pdf">Cluster-driven Graph Federated Learning over Multiple Domains</a> (Debora Caldarola, CVPR 2021)</p>

<h4 id="model-compression-and-acceleration">Model Compression and Acceleration</h4>

<p><a href="https://arxiv.org/pdf/1710.09282.pdf">A Survey of Model Compression and Acceleration for Deep Neural Networks</a> (Yu Cheng, 2017)</p>

<h5 id="parameter-pruning-and-quantization">Parameter Pruning and Quantization</h5>

<p><a href="https://arxiv.org/pdf/1810.05270.pdf">Rethinking the Value of Network Pruning</a> (Zhuang Liu, ICLR 2019, <a href="https://github.com/Eric-mingjie/rethinking-network-pruning">code</a>, <a href="https://xmfbit.github.io/2018/10/22/paper-rethinking-the-value-of-network-pruning/">note</a>)</p>

<h5 id="human-designed-model">Human-Designed Model</h5>

<p><a href="https://arxiv.org/pdf/1704.04861.pdf">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> (Andrew Howard, 2017)</p>

<p><a href="https://arxiv.org/pdf/1801.04381.pdf">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a> (Mark Sandler, CVPR 2018, <a href="https://github.com/pytorch/vision?utm_source=catalyzex.com">code</a>)</p>

<p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile</a> (Xiangyu Zhang, CVPR 2018)</p>

<p><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Ningning_Light-weight_CNN_Architecture_ECCV_2018_paper.pdf">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a> (Ningning Ma, ECCV 2018)</p>

<p><a href="https://arxiv.org/pdf/1911.11907.pdf">GhostNet: More Features from Cheap Operations</a> (Kai Han, CVPR 2020)</p>

<h5 id="knowledge-distillation">Knowledge Distillation</h5>

<p><a href="https://arxiv.org/pdf/1503.02531.pdf">Distilling the Knowledge in a Neural Network</a> (Geoffrey Hinton, NeurIPS 2015, <a href="https://xmfbit.github.io/2018/06/07/knowledge-distilling/">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2003.13678.pdf">Designing Network Design Spaces</a> (Ilija Radosavovic, CVPR 2020)</p>

<p><a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01453-z.pdf">Knowledge Distillation: A Survey</a> (Jianping Gou, IJCV 2021)</p>

<p><a href="https://arxiv.org/pdf/2106.05945.pdf">Does Knowledge Distillation Really Work?</a> (Samuel Stanton, 2021)</p>

<h4 id="neural-architecture-search">Neural Architecture Search</h4>

<p><a href="https://arxiv.org/pdf/1611.01578.pdf">Neural Architecture Search with Reinforcement Learning</a> (Barret Zoph, ICLR 2017, <a href="https://zhuanlan.zhihu.com/p/47221948">note</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html">Learning Transferable Architectures for Scalable Image Recognition</a> (Barret Zoph, CVPR 2018, <a href="https://zhuanlan.zhihu.com/p/31655995">note</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper.html">Progressive Neural Architecture Search</a> (Chenxi Liu, ECCV 2018, <a href="https://blog.csdn.net/dhaiuda/article/details/102599427">note</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Tien-Ju_Yang_NetAdapt_Platform-Aware_Neural_ECCV_2018_paper.html">NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications</a> (Tien-Ju Yang, ECCV 2018, <a href="https://github.com/denru01/netadapt">code</a>, <a href="https://blog.csdn.net/thisiszdy/article/details/90515075">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1802.03268.pdf">Efficient Neural Architecture Search via Parameter Sharing</a> (Hieu Pham, ICML 2018, <a href="https://cloud.tencent.com/developer/article/1182704">note</a>)</p>

<p><a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4405">Regularized Evolution for Image Classifier Architecture Search</a> (Esteban Real, AAAI 2019, <a href="https://blog.csdn.net/dhaiuda/article/details/93337707">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1806.09055.pdf">DARTS: Differentiable Architecture Search</a> (Hanxiao Liu, ICLR 2019, <a href="https://github.com/quark0/darts">code</a>, <a href="https://www.cnblogs.com/wangxiaocvpr/p/10556789.html">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1812.00332.pdf">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</a> (Han Cai, ICLR 2019, <a href="https://github.com/mit-han-lab/proxylessnas">code</a>, <a href="https://zhuanlan.zhihu.com/p/55220311">note</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Tan_MnasNet_Platform-Aware_Neural_Architecture_Search_for_Mobile_CVPR_2019_paper.html">MnasNet: Platform-Aware Neural Architecture Search for Mobile</a> (Mingxing Tan, CVPR 2019, <a href="https://github.com/mingxingtan/mnasnet">code</a>, <a href="https://zhuanlan.zhihu.com/p/42474017">note</a>)</p>

<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.html">Searching for MobileNetV3</a> (Andrew Howard, ICCV 2019, <a href="https://blog.nex3z.com/2020/07/27/reading-searching-for-mobilenetv3/">note</a>)</p>

<p><a href="http://www.jmlr.org/papers/volume20/18-598/18-598.pdf">Neural Architecture Search: A Survey</a> (Thomas Elsken, JMLR 2019, <a href="https://zhuanlan.zhihu.com/p/123144164">note</a>)</p>

<p><a href="https://arxiv.org/pdf/2006.14090.pdf">Neural Architecture Design for GPU-Efficient Networks</a> (Ming Lin, 2020, <a href="https://github.com/idstcv/GPU-Efficient-Networks">code</a>, <a href="https://zhuanlan.zhihu.com/p/151042330">note</a>)</p>

<h4 id="interpretability-1">Interpretability</h4>

<p><a href="https://arxiv.org/pdf/2012.14261.pdf">A Survey on Neural Network Interpretability</a> (Yu Zhang, 2020)</p>

<h4 id="evidential-deep-learning-and-uncertainty">Evidential Deep Learning and Uncertainty</h4>

<p><a href="https://arxiv.org/pdf/1506.02142.pdf">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a> (Yarin Gal, ICML 2016, <a href="https://zhuanlan.zhihu.com/p/82108924">note</a>)</p>

<p><a href="https://papers.nips.cc/paper/2016/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a> (Yarin Gal, NeurIPS 2016)</p>

<p><a href="https://arxiv.org/pdf/1703.04977.pdf">What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</a> (Alex Kendall, NeurIPS 2017, <a href="https://zhuanlan.zhihu.com/p/98756147">note</a>)</p>

<p><a href="https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a> (Balaji Lakshminarayanan, NeurIPS 2017)</p>

<p><a href="https://papers.nips.cc/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf">Evidential Deep Learning to Quantify Classification Uncertainty</a> (Murat Sensoy, NeurIPS 2018)</p>

<p><a href="https://arxiv.org/pdf/1906.02530.pdf">Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift</a> (Yaniv Ovadia, NeurIPS 2019)</p>

<h3 id="recommendation-system">Recommendation System</h3>

<p><a href="http://de.arxiv.org/pdf/1707.07435v5">Deep Learning based Recommender System: A Survey and New Perspectives</a> (Shuai Zhang, 2017, <a href="https://github.com/cheungdaven/DeepRec">code</a>, <a href="https://www.cnblogs.com/z1141000271/p/11399916.html">note</a>)</p>

<p><a href="https://link.springer.com/article/10.1007/s10462-018-9654-y">A review on deep learning for recommender systems: challenges and remedies</a> (Zeynep Batmaz, 2018, <a href="https://blog.csdn.net/qq_35771020/article/details/88759986">note</a>)</p>

<h4 id="news-recommendation">News Recommendation</h4>

<p><a href="http://wwwconference.org/www2007/papers/paper570.pdf">Google News Personalization: Scalable Online Collaborative Filtering</a> (Abhinandan Das, WWW 2007, <a href="https://blog.csdn.net/jj12345jj198999/article/details/12654531">note</a>)</p>

<p><a href="http://www.cs.northwestern.edu/~jli156/IUI224-liu.pdf">Personalized News Recommendation Based on Click Behavior</a> (Jiahui Liu, 2010, <a href="https://www.jianshu.com/p/f3d147fbce3f">note</a>)</p>

<p><a href="http://wwwconference.org/www2009/proceedings/pdf/p691.pdf">Personalized Recommendation on Dynamic Content Using Predictive Bilinear Models</a> (Wei Chu, WWW 2009, <a href="https://zhuanlan.zhihu.com/p/75484786">note</a>)</p>

<p><a href="http://wwwconference.org/proceedings/www2010/www/p661.pdf">A Contextual-Bandit Approach to Personalized News Article Recommendation</a> (Lihong Li, WWW 2010, <a href="https://zhuanlan.zhihu.com/p/34940176">note</a>)</p>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/frp1159-songA.pdf">A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems</a> (Ali Elkahky, WWW 2015, <a href="https://zhuanlan.zhihu.com/p/56384078">note</a>)</p>

<h4 id="click-through-rate">Click-Through Rate</h4>

<p><a href="https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf">Factorization Machines</a> (Steffen Rendle, ICDM 2010, <a href="https://zhuanlan.zhihu.com/p/50426292">note</a>)</p>

<p><a href="https://papers.nips.cc/paper/6144-higher-order-factorization-machines.pdf">Higher-Order Factorization Machine</a> (Mathieu Blondel, NeurIPS 2016)</p>

<p><a href="https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf">Field-aware Factorization Machines for CTR Prediction</a> (Yu-Chin Juan, RecSys 2016, <a href="https://blog.csdn.net/Dby_freedom/article/details/84899120">note</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/2017/0239.pdf">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</a> (Huifeng Guo, IJCAI 2017, <a href="https://blog.csdn.net/Dby_freedom/article/details/85263694">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1706.06978.pdf">Deep Interest Network for Click-Through Rate Prediction</a> (Guorui Zhou, KDD 2018, <a href="https://www.jianshu.com/p/7af364dcea12">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1809.03672.pdf">Deep Interest Evolution Network for Click-Through Rate Prediction</a> (Guorui Zhou, AAAI 2019, <a href="https://github.com/mouna99/dien">code</a>, <a href="https://zhuanlan.zhihu.com/p/50758485">note</a>)</p>

<p><a href="https://www.ijcai.org/Proceedings/2019/0634.pdf">Representation Learning-Assisted Click-Through Rate Prediction</a> (Wentao Ouyang, IJCAI 2019, <a href="https://zhuanlan.zhihu.com/p/102075293">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1906.03776.pdf">Deep Spatio-Temporal Neural Networks for Click-Through Rate Prediction</a> (Wentao Ouyang, KDD 2019, <a href="https://github.com/oywtece/dstn">code</a>, <a href="https://www.sohu.com/a/326460892_99979179">note</a>)</p>

<p><a href="https://arxiv.org/pdf/1904.04447.pdf">Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction</a> (Bin Liu, WWW 2019, <a href="https://blog.csdn.net/w55100/article/details/90601310">note</a>)</p>

<p><a href="https://dl.acm.org/doi/pdf/10.1145/3336191.3371785">Interpretable Click-Through Rate Prediction through Hierarchical Attention</a> (Zeyu Li, WSDM 2020)</p>

<p><a href="https://arxiv.org/pdf/2005.14171.pdf">User Behavior Retrieval for Click-Through Rate Prediction</a> (Jiarui Qin, SIGIR 2020)</p>

<p><a href="https://arxiv.org/pdf/2001.03025.pdf">Deep Time-Stream Framework for Click-through Rate Prediction by Tracking Interest Evolution</a> (Shu-Ting Shi, AAAI 2020)</p>

<p><a href="https://aaai.org/ojs/index.php/AAAI/article/view/5346">Deep Match to Rank Model for Personalized Click-Through Rate Prediction</a> (Zequn Lyu, AAAI 2020, <a href="https://github.com/lvze92/DMR">code</a>, <a href="https://www.jianshu.com/p/60eed27e06d4">note</a>)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3340531.3412092?casa_token=nH4IJveJv8IAAAAA:DGfLy48IytEIldltBbIJCiY6dG6qzoZac3M_rXJBxJArwTQ8KhqcV3Qh_FCP7gUp_tDz9RnxZ55yeqo">Deep Multi-Interest Network for Click-through Rate Prediction</a> (Zhibo Xiao, CIKM 2020)</p>

<h3 id="big-data">Big Data</h3>

<p><a href="http://index-of.es/Misc/pdf/google_file_system.pdf">The Google File System</a> (Sanjay Ghemawat, SOSP 2003, <a href="https://zhuanlan.zhihu.com/p/28155582">note</a>)</p>

<p><a href="https://homeostasis.scs.carleton.ca/~soma/distos/2008-03-24/mapreduce-osdi04.pdf">MapReduce: Simplified Data Processing on Large Clusters</a> (Jeffrey Dean, OSDI 2004, <a href="https://juejin.im/post/6844903812784717831">note</a>)</p>

<p><a href="https://fenix.tecnico.ulisboa.pt/downloadFile/845043405442710/10.g-bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> (Fay Chang, OSDI 2006, <a href="https://zh.wikipedia.org/wiki/Bigtable">note</a>)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/1323293.1294281">Dynamo: Amazon’s Highly Available Key-Value Store</a> (Giuseppe DeCandia, SIGOPS 2007)</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/1773912.1773922">Cassandra: A Decentralized Structured Storage System</a> (Avinash Lakshman, SIGOPS 2010)</p>

<h3 id="tool">Tool</h3>

<p><a href="https://www.aclweb.org/anthology/P13-4009">FudanNLP: A Toolkit for Chinese Natural Language Processing</a> (Xipeng Qiu, ACL 2013, <a href="https://github.com/FudanNLP/fnlp">code</a>)</p>

<p><a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">LIBSVM: A library for support vector machines</a> (Chih-Jen Lin, 2011, <a href="https://github.com/cjlin1/libsvm">code</a>, <a href="https://blog.csdn.net/s9434/article/details/75091602">note</a>)</p>

<p><a href="http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0111988&amp;type=printable">HemI: A Toolkit for Illustrating Heatmaps</a> (Wankun Deng, 2014, <a href="http://hemi.biocuckoo.org/">code</a>)</p>

    </article>
    <div class="share">
      <div class="share-component"></div>
    </div>
    <div class="comment">
      
  
        <div id="container"></div>
        <link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
        <script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
        <script>
        var gitment = new Gitment({
            id: '/wiki/computer-science-papers/',
            owner: 'lpq29743',
            repo: 'blog-comments',
            oauth: {
                client_id: '5818ceff784afe05d31c',
                client_secret: 'dc7e8efc044c7194cf453740f82c646aa998f206',
            },
        })
        gitment.render('container')
        </script>
  


    </div>
  </div>
  <div class="column one-fourth">
    
<h3>Search</h3>
<div id="site_search">
    <input type="text" id="search_box" placeholder="Search">
</div>

<ul id="search_results"></ul>

<link rel="stylesheet" type="text/css" href="http://localhost:4000/assets/css/modules/sidebar-search.css">
<script src="http://localhost:4000/assets/js/jekyll-search.min.js"></script>
<script src="http://localhost:4000/assets/js/search.js"></script>

<script type="text/javascript">
SimpleJekyllSearch({
    searchInput: document.getElementById('search_box'),
    resultsContainer: document.getElementById('search_results'),
    json: 'http://localhost:4000/assets/search_data.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a></li>',
    noResultsText: 'No results found',
    limit: 10,
    fuzzy: false,
    exclude: ['Welcome']
})
</script>

    
<h3 class="post-directory-title mobile-hidden">Table of Contents</h3>
<div id="post-directory-module" class="mobile-hidden">
  <section class="post-directory">
  <!-- Links that trigger the jumping -->
  <!-- Added by javascript below -->
  <dl></dl>
  </section>
</div>

<script src="http://localhost:4000/assets/js/jquery.toc.js"></script>

  </div>
</div>
</section>
<!-- /section.content -->

    <footer class="container">
        <div class="site-footer" role="contentinfo">
            <div class="copyright left mobile-block">
                <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
            </div>

            <ul class="site-footer-links right mobile-hidden">
                <li>
                    <a href="javascript:window.scrollTo(0,0)" >TOP</a>
                </li>
            </ul>
            <a href="https://github.com/lpq29743/lpq29743.github.io" target="_blank" aria-label="view source code">
                <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
            </a>
            <ul class="site-footer-links mobile-hidden">
                
            </ul>

        </div>
    </footer>
    <!-- / footer -->
    <script src="http://localhost:4000/assets/vendor/share.js/dist/js/share.min.js"></script>
    <script src="http://localhost:4000/assets/js/geopattern.js"></script>
    <script src="http://localhost:4000/assets/js/prism.js"></script>
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/prism.css">
    <script>
      jQuery(document).ready(function($) {
        // geopattern
        $('.geopattern').each(function(){
          $(this).geopattern($(this).data('pattern-id'));
        });
       // hljs.initHighlightingOnLoad();
      });
    </script>
    
</body>
</html>
